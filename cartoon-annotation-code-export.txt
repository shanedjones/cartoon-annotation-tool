# Cartoon Annotation Tool - Complete Source Code Export
# Generated on Wed Mar 26 23:40:46 PDT 2025
# This file contains all source code from the project

|| START ./app/serialization-test/page.tsx ||

'use client';

import React, { useState, useRef, useEffect } from 'react';
import Link from 'next/link';

// Define minimal audio chunk interface
interface AudioChunk {
  blob: Blob | string;
  mimeType?: string;
}

// Client-only component for browser info
const BrowserInfo = () => {
  const [userAgent, setUserAgent] = useState('Loading...');
  
  useEffect(() => {
    setUserAgent(navigator.userAgent);
  }, []);
  
  return <span>Browser: {userAgent}</span>;
};

// Minimal serialization test page
export default function SerializationTestPage() {
  // Recording state
  const [isRecording, setIsRecording] = useState(false);
  const [recordedBlob, setRecordedBlob] = useState<Blob | null>(null);
  const [recordingFormat, setRecordingFormat] = useState('');
  
  // Serialization state
  const [serializedString, setSerializedString] = useState<string | null>(null);
  const [deserializedBlob, setDeserializedBlob] = useState<Blob | null>(null);
  const [deserializedAudioUrl, setDeserializedAudioUrl] = useState<string | null>(null);
  
  // Original audio URL (direct from recorder)
  const [directAudioUrl, setDirectAudioUrl] = useState<string | null>(null);
  
  // Debug information
  const [debugInfo, setDebugInfo] = useState<any>({});
  const [error, setError] = useState<string | null>(null);
  
  // Refs
  const mediaRecorderRef = useRef<MediaRecorder | null>(null);
  const chunksRef = useRef<Blob[]>([]);
  const directAudioRef = useRef<HTMLAudioElement | null>(null);
  const deserializedAudioRef = useRef<HTMLAudioElement | null>(null);
  
  // Helper function to convert blob to base64
  const blobToBase64 = (blob: Blob): Promise<string> => {
    return new Promise((resolve, reject) => {
      const reader = new FileReader();
      reader.onload = () => {
        const dataUrl = reader.result as string;
        resolve(dataUrl);
      };
      reader.onerror = reject;
      reader.readAsDataURL(blob);
    });
  };
  
  // Helper function to convert base64 back to blob
  const base64ToBlob = (base64: string, mimeType: string): Blob => {
    try {
      const byteString = atob(base64.split(',')[1]);
      const ab = new ArrayBuffer(byteString.length);
      const ia = new Uint8Array(ab);
      
      for (let i = 0; i < byteString.length; i++) {
        ia[i] = byteString.charCodeAt(i);
      }
      
      return new Blob([ab], { type: mimeType });
    } catch (error) {
      console.error('Error converting base64 to Blob:', error);
      throw error;
    }
  };
  
  // Start recording audio
  const startRecording = async () => {
    try {
      // Reset state
      chunksRef.current = [];
      setRecordedBlob(null);
      setDirectAudioUrl(null);
      setSerializedString(null);
      setDeserializedBlob(null);
      setDeserializedAudioUrl(null);
      setDebugInfo({});
      setError(null);
      
      // Request microphone access
      const stream = await navigator.mediaDevices.getUserMedia({
        audio: {
          echoCancellation: true,
          noiseSuppression: true,
          autoGainControl: true,
          channelCount: 1
        }
      });
      
      // Find the best supported audio format
      let mimeType = '';
      const formats = [
        'audio/webm;codecs=opus',
        'audio/webm',
        'audio/mp4;codecs=opus',
        'audio/mp4',
        'audio/ogg;codecs=opus',
        'audio/ogg',
        'audio/wav'
      ];
      
      for (const format of formats) {
        if (MediaRecorder.isTypeSupported(format)) {
          mimeType = format;
          break;
        }
      }
      
      setRecordingFormat(mimeType || 'default format');
      console.log('Using audio format:', mimeType || 'default');
      
      // Create recorder
      const recorderOptions = {
        mimeType: mimeType || undefined,
        audioBitsPerSecond: 128000
      };
      
      const recorder = new MediaRecorder(stream, recorderOptions);
      mediaRecorderRef.current = recorder;
      
      // Handle data available event
      recorder.ondataavailable = (e) => {
        if (e.data.size > 0) {
          chunksRef.current.push(e.data);
        }
      };
      
      // Handle recording stop
      recorder.onstop = async () => {
        try {
          // Create audio blob from chunks
          const audioBlob = new Blob(chunksRef.current, { type: mimeType || 'audio/webm' });
          setRecordedBlob(audioBlob);
          
          // Create direct URL for immediate playback
          const directUrl = URL.createObjectURL(audioBlob);
          setDirectAudioUrl(directUrl);
          
          // Log original blob info
          console.log('Original audio blob:', {
            type: audioBlob.type,
            size: audioBlob.size,
            chunks: chunksRef.current.length
          });
          
          // Start serialization process
          await serializeAndDeserialize(audioBlob, mimeType || 'audio/webm');
          
          // Stop all tracks to release the microphone
          stream.getTracks().forEach(track => track.stop());
        } catch (err) {
          setError(`Error processing recording: ${err instanceof Error ? err.message : String(err)}`);
          console.error('Recording processing error:', err);
        }
      };
      
      // Start the recorder
      recorder.start();
      setIsRecording(true);
    } catch (error) {
      setError(`Could not start recording: ${error instanceof Error ? error.message : String(error)}`);
      console.error('Error starting recording:', error);
    }
  };
  
  // Stop recording
  const stopRecording = () => {
    if (mediaRecorderRef.current && mediaRecorderRef.current.state !== 'inactive') {
      mediaRecorderRef.current.stop();
      setIsRecording(false);
    }
  };
  
  // Serialize and deserialize audio blob
  const serializeAndDeserialize = async (blob: Blob, mimeType: string) => {
    try {
      // Step 1: Convert blob to base64 string
      console.log('Step 1: Converting blob to base64...');
      const startTime1 = performance.now();
      const base64String = await blobToBase64(blob);
      const endTime1 = performance.now();
      
      // Set serialized string state
      setSerializedString(base64String);
      console.log('Base64 conversion complete:', {
        originalSize: blob.size,
        base64Length: base64String.length,
        timeMs: (endTime1 - startTime1).toFixed(2)
      });
      
      // Step 2: Convert base64 back to blob
      console.log('Step 2: Converting base64 back to blob...');
      const startTime2 = performance.now();
      const newBlob = base64ToBlob(base64String, mimeType);
      const endTime2 = performance.now();
      
      // Set deserialized blob state
      setDeserializedBlob(newBlob);
      console.log('Blob conversion complete:', {
        newSize: newBlob.size, 
        newType: newBlob.type,
        timeMs: (endTime2 - startTime2).toFixed(2)
      });
      
      // Step 3: Create audio URL from new blob
      const deserializedUrl = URL.createObjectURL(newBlob);
      setDeserializedAudioUrl(deserializedUrl);
      
      // Set debug info for display
      setDebugInfo({
        originalBlob: {
          size: blob.size,
          type: blob.type,
          mimeType: mimeType
        },
        base64String: {
          length: base64String.length,
          preview: base64String.substring(0, 50) + '...',
          conversionTimeMs: (endTime1 - startTime1).toFixed(2)
        },
        deserializedBlob: {
          size: newBlob.size,
          type: newBlob.type,
          conversionTimeMs: (endTime2 - startTime2).toFixed(2)
        },
        comparison: {
          sizeMatch: blob.size === newBlob.size,
          typeMatch: blob.type === newBlob.type
        }
      });
    } catch (error) {
      setError(`Serialization error: ${error instanceof Error ? error.message : String(error)}`);
      console.error('Serialization process failed:', error);
    }
  };
  
  // Clean up on unmount
  useEffect(() => {
    return () => {
      if (directAudioUrl) URL.revokeObjectURL(directAudioUrl);
      if (deserializedAudioUrl) URL.revokeObjectURL(deserializedAudioUrl);
    };
  }, [directAudioUrl, deserializedAudioUrl]);
  
  return (
    <div className="p-8 max-w-4xl mx-auto">
      <div className="mb-6">
        <Link href="/" className="text-blue-500 hover:underline mb-4 inline-block">
          &larr; Back to main app
        </Link>
        <h1 className="text-3xl font-bold mb-4">Audio Serialization Test</h1>
        <p className="text-gray-600 mb-4">
          This minimal test focuses only on recording audio and testing the serialization/deserialization process.
        </p>
      </div>
      
      <div className="bg-white p-6 rounded-lg shadow-md mb-6">
        {/* Recording controls */}
        <div className="mb-6">
          <h2 className="text-xl font-semibold mb-2">1. Record Audio</h2>
          <p className="text-sm text-gray-500 mb-4">
            Recording format: {recordingFormat || 'Not determined yet'}
          </p>
          
          <div className="flex items-center space-x-4 mb-4">
            {!isRecording ? (
              <button
                onClick={startRecording}
                className="flex items-center px-4 py-2 bg-red-500 text-white rounded-md hover:bg-red-600"
              >
                <span className="h-3 w-3 bg-white rounded-full mr-2"></span>
                Start Recording
              </button>
            ) : (
              <button
                onClick={stopRecording}
                className="flex items-center px-4 py-2 bg-gray-700 text-white rounded-md hover:bg-gray-800"
              >
                <span className="h-3 w-3 bg-white mr-2"></span>
                Stop Recording
              </button>
            )}
          </div>
          
          {isRecording && (
            <div className="flex items-center mb-4">
              <span className="h-3 w-3 bg-red-500 rounded-full animate-pulse mr-2"></span>
              <span className="text-red-500">Microphone active</span>
            </div>
          )}
        </div>
        
        {/* Error message */}
        {error && (
          <div className="mb-6 p-3 bg-red-100 border border-red-300 text-red-700 rounded-md">
            <strong>Error:</strong> {error}
          </div>
        )}
        
        {/* Audio playback comparison */}
        {directAudioUrl && (
          <div className="mb-6">
            <h2 className="text-xl font-semibold mb-4">2. Playback Comparison</h2>
            
            <div className="grid grid-cols-1 md:grid-cols-2 gap-6">
              {/* Direct playback */}
              <div className="bg-gray-100 p-4 rounded-md">
                <h3 className="font-semibold text-lg mb-2 text-blue-700">Direct Playback</h3>
                <p className="text-sm text-gray-500 mb-2">
                  Original recording (no serialization)
                </p>
                
                <audio 
                  ref={directAudioRef}
                  src={directAudioUrl} 
                  controls 
                  className="w-full mb-3"
                />
                
                <div className="text-xs text-gray-600 mb-2">
                  Size: {recordedBlob ? `${(recordedBlob.size / 1024).toFixed(2)} KB` : 'Unknown'}
                </div>
                
                <button
                  onClick={() => {
                    if (directAudioRef.current) {
                      directAudioRef.current.currentTime = 0;
                      directAudioRef.current.play();
                    }
                  }}
                  className="px-3 py-1 bg-blue-500 text-white text-sm rounded hover:bg-blue-600"
                >
                  Replay
                </button>
              </div>
              
              {/* Serialized playback */}
              <div className="bg-gray-100 p-4 rounded-md">
                <h3 className="font-semibold text-lg mb-2 text-purple-700">Deserialized Playback</h3>
                <p className="text-sm text-gray-500 mb-2">
                  After base64 conversion and back
                </p>
                
                {deserializedAudioUrl ? (
                  <audio 
                    ref={deserializedAudioRef}
                    src={deserializedAudioUrl} 
                    controls 
                    className="w-full mb-3"
                  />
                ) : (
                  <div className="w-full h-12 bg-gray-200 flex items-center justify-center text-gray-500 mb-3">
                    Waiting for deserialization...
                  </div>
                )}
                
                <div className="text-xs text-gray-600 mb-2">
                  Size: {deserializedBlob ? `${(deserializedBlob.size / 1024).toFixed(2)} KB` : 'Unknown'}
                </div>
                
                <button
                  onClick={() => {
                    if (deserializedAudioRef.current) {
                      deserializedAudioRef.current.currentTime = 0;
                      deserializedAudioRef.current.play();
                    }
                  }}
                  disabled={!deserializedAudioUrl}
                  className={`px-3 py-1 text-white text-sm rounded ${deserializedAudioUrl ? 'bg-purple-500 hover:bg-purple-600' : 'bg-gray-400'}`}
                >
                  Replay
                </button>
              </div>
            </div>
          </div>
        )}
        
        {/* Serialization details */}
        {Object.keys(debugInfo).length > 0 && (
          <div className="mt-6 border-t pt-4">
            <h3 className="font-semibold text-lg mb-2">3. Serialization Details</h3>
            
            <div className="bg-gray-800 text-green-300 p-4 rounded font-mono text-xs overflow-x-auto">
              <pre>
{`Serialization Process:
------------------------
Original Blob:
  Size: ${debugInfo.originalBlob?.size} bytes (${(debugInfo.originalBlob?.size / 1024).toFixed(2)} KB)
  Type: ${debugInfo.originalBlob?.type}
  MIME: ${debugInfo.originalBlob?.mimeType}

Base64 Conversion:
  Length: ${debugInfo.base64String?.length} characters
  Time: ${debugInfo.base64String?.conversionTimeMs}ms
  Preview: ${debugInfo.base64String?.preview}

Deserialized Blob:
  Size: ${debugInfo.deserializedBlob?.size} bytes (${(debugInfo.deserializedBlob?.size / 1024).toFixed(2)} KB)
  Type: ${debugInfo.deserializedBlob?.type}
  Time: ${debugInfo.deserializedBlob?.conversionTimeMs}ms

Comparison:
  Size Match: ${debugInfo.comparison?.sizeMatch ? '✅ Yes' : '❌ No'}
  Type Match: ${debugInfo.comparison?.typeMatch ? '✅ Yes' : '❌ No'}
`}
              </pre>
            </div>
            
            {/* Base64 string preview */}
            {serializedString && (
              <div className="mt-4">
                <h4 className="font-semibold mb-2">Base64 String (first 100 chars)</h4>
                <div className="bg-gray-100 p-3 rounded overflow-x-auto">
                  <code className="text-xs break-all">
                    {serializedString.substring(0, 100)}...
                  </code>
                </div>
                <button
                  onClick={() => {
                    if (serializedString) {
                      const a = document.createElement('a');
                      const blob = new Blob([serializedString], { type: 'text/plain' });
                      a.href = URL.createObjectURL(blob);
                      a.download = `audio-base64-${new Date().toISOString()}.txt`;
                      document.body.appendChild(a);
                      a.click();
                      document.body.removeChild(a);
                      URL.revokeObjectURL(a.href);
                    }
                  }}
                  className="mt-2 px-3 py-1 bg-gray-500 text-white text-sm rounded hover:bg-gray-600"
                >
                  Download Base64 String
                </button>
              </div>
            )}
          </div>
        )}
      </div>
      
      <div className="text-xs text-gray-500 mt-4">
        <BrowserInfo />
      </div>
    </div>
  );
}
|| END ||


|| START ./app/audio-test/page.tsx ||

'use client';

import React, { useState, useRef, useEffect } from 'react';
import Link from 'next/link';
import dynamic from 'next/dynamic';

// Import the proper VideoPlayer component from the main app
// Use dynamic import to avoid SSR issues
const VideoPlayer = dynamic(() => import('../../src/components/VideoPlayer'), { ssr: false });

// Define AudioChunk interface identical to the one in AudioRecorder.tsx
interface AudioChunk {
  blob: Blob | string;      // The audio data as Blob or string (for serialization)
  startTime: number;        // Relative to recording start
  duration: number;         // Length of audio chunk in ms
  videoTime: number;        // Video timestamp when this audio was recorded
  url?: string;             // URL for playback (created during replay)
  mimeType?: string;        // MIME type for proper playback
}

// Client-side component for browser info to prevent hydration mismatch
const BrowserInfo = () => {
  const [userAgent, setUserAgent] = useState('Loading...');
  
  useEffect(() => {
    setUserAgent(navigator.userAgent);
  }, []);
  
  return <span>Browser: {userAgent}</span>;
};

// Client-side component for MIME type support to prevent hydration mismatch
const MimeTypesInfo = () => {
  const [supportedTypes, setSupportedTypes] = useState<Record<string, boolean>>({});
  const mimeTypes = ['audio/webm', 'audio/webm;codecs=opus', 'audio/mp4', 'audio/ogg'];
  
  useEffect(() => {
    const support: Record<string, boolean> = {};
    
    mimeTypes.forEach(type => {
      if (typeof MediaRecorder !== 'undefined') {
        support[type] = MediaRecorder.isTypeSupported(type);
      } else {
        support[type] = false;
      }
    });
    
    setSupportedTypes(support);
  }, []);
  
  return (
    <>
      <span>Supported MIME types:</span>
      <ul className="pl-4 mt-1">
        {mimeTypes.map(type => (
          <li 
            key={type} 
            className={supportedTypes[type] ? 'text-green-600' : 'text-red-600'}
          >
            {type}: {Object.keys(supportedTypes).length > 0 ? 
              (supportedTypes[type] ? 'Supported' : 'Not supported') : 
              'Checking...'}
          </li>
        ))}
      </ul>
    </>
  );
};

// Simple audio recorder and player for testing - enhanced with serialization testing
export default function AudioTestPage() {
  const [isRecording, setIsRecording] = useState(false);
  const [recordedAudio, setRecordedAudio] = useState<string | null>(null);
  const [recordingDuration, setRecordingDuration] = useState(0);
  const [elapsedTime, setElapsedTime] = useState(0);
  const [recordingFormat, setRecordingFormat] = useState('');
  const [recordingStartTime, setRecordingStartTime] = useState<number | null>(null);
  
  // State for storing the recorded audio blob for serialization testing
  const [recordedBlob, setRecordedBlob] = useState<Blob | null>(null);
  const [serializedAudio, setSerializedAudio] = useState<string | null>(null);
  const [deserializedAudio, setDeserializedAudio] = useState<string | null>(null);
  const [audioChunk, setAudioChunk] = useState<AudioChunk | null>(null);
  const [debugInfo, setDebugInfo] = useState<Record<string, any>>({});
  
  // State for the programmatic playback test
  const [playbackTestResult, setPlaybackTestResult] = useState<{success: boolean; details?: string} | null>(null);
  const [playbackTestRunning, setPlaybackTestRunning] = useState(false);
  const [syncTestResult, setSyncTestResult] = useState<{success: boolean; details?: string} | null>(null);
  
  const mediaRecorderRef = useRef<MediaRecorder | null>(null);
  const chunksRef = useRef<Blob[]>([]);
  const timerRef = useRef<NodeJS.Timeout | null>(null);
  const audioRef = useRef<HTMLAudioElement | null>(null);
  const audioSerializedRef = useRef<HTMLAudioElement | null>(null);
  const videoPlayerRef = useRef<HTMLVideoElement | null>(null);
  
  // State for tracking video synchronization
  const [syncWithVideo, setSyncWithVideo] = useState(true);
  const [videoPlaying, setVideoPlaying] = useState(false);
  
  // Helper function to convert Blob to base64 for storage (copied from VideoPlayerWrapper)
  const blobToBase64 = (blob: Blob): Promise<string> => {
    return new Promise((resolve, reject) => {
      const reader = new FileReader();
      reader.onload = () => {
        const dataUrl = reader.result as string;
        resolve(dataUrl);
      };
      reader.onerror = reject;
      reader.readAsDataURL(blob);
    });
  };

  // Helper function to convert base64 back to Blob for playback (copied from VideoPlayerWrapper)
  const base64ToBlob = (base64: string, mimeType: string): Blob => {
    try {
      // First ensure we have a proper data URL with the correct format
      if (!base64.includes(',')) {
        throw new Error('Invalid base64 string format - missing comma separator');
      }
      
      // Extract the base64 part after the comma
      const base64Data = base64.split(',')[1];
      if (!base64Data) {
        throw new Error('Invalid base64 string - no data after comma');
      }
      
      // Decode the base64 string to binary
      const byteString = atob(base64Data);
      
      // Create an ArrayBuffer to hold the decoded data
      const ab = new ArrayBuffer(byteString.length);
      const ia = new Uint8Array(ab);
      
      // Copy the decoded binary data to the array buffer
      for (let i = 0; i < byteString.length; i++) {
        ia[i] = byteString.charCodeAt(i);
      }
      
      // Create and return a new Blob from the array buffer
      return new Blob([ab], { type: mimeType });
    } catch (error) {
      if (typeof window !== 'undefined') {
        console.error('Error converting base64 to Blob:', error);
      }
      throw error;
    }
  };
  
  // Start recording audio
  const startRecording = async () => {
    try {
      // Reset state
      chunksRef.current = [];
      setRecordedAudio(null);
      setElapsedTime(0);
      setRecordedBlob(null);
      setSerializedAudio(null);
      setDeserializedAudio(null);
      setAudioChunk(null);
      setDebugInfo({});
      
      // Request microphone access with high quality settings
      const stream = await navigator.mediaDevices.getUserMedia({
        audio: {
          echoCancellation: true,
          noiseSuppression: true,
          autoGainControl: true,
          channelCount: 1, // Mono for voice clarity
          sampleRate: 48000 // Higher sample rate for better quality
        }
      });
      
      // Find the best supported audio format
      let mimeType = '';
      const formats = [
        'audio/webm;codecs=opus',
        'audio/webm',
        'audio/mp4;codecs=opus',
        'audio/mp4',
        'audio/ogg;codecs=opus',
        'audio/ogg',
        'audio/wav'
      ];
      
      for (const format of formats) {
        if (MediaRecorder.isTypeSupported(format)) {
          mimeType = format;
          break;
        }
      }
      
      setRecordingFormat(mimeType || 'default format');
      if (typeof window !== 'undefined') {
        console.log('Using audio format:', mimeType || 'default');
      }
      
      // Create recorder with high quality settings
      const recorderOptions = {
        mimeType: mimeType || undefined,
        audioBitsPerSecond: 128000
      };
      
      const recorder = new MediaRecorder(stream, recorderOptions);
      mediaRecorderRef.current = recorder;
      
      // Handle data available event
      recorder.ondataavailable = (e) => {
        if (e.data.size > 0) {
          chunksRef.current.push(e.data);
        }
      };
      
      // Handle recording stop
      recorder.onstop = async () => {
        // Always clean up resources in finally block
        try {
          // Create audio blob from chunks
          const audioBlob = new Blob(chunksRef.current, { type: mimeType || 'audio/webm' });
          const audioUrl = URL.createObjectURL(audioBlob);
          setRecordedAudio(audioUrl);
          setRecordingDuration(elapsedTime);
          setRecordedBlob(audioBlob);
          
          if (typeof window !== 'undefined') {
            console.log('Recording stopped. Blob created:', {
              size: audioBlob.size,
              type: audioBlob.type,
              chunks: chunksRef.current.length
            });
          }
          
          // Use either the recordingStartTime if available, or fallback to current time
          const startTime = recordingStartTime || Date.now();
          if (typeof window !== 'undefined' && !recordingStartTime) {
            console.log('Warning: Recording start time was not set, using fallback time');
          }
          
          // Create an AudioChunk object similar to the one in the main app
          const chunk: AudioChunk = {
            blob: audioBlob,
            startTime: startTime,
            duration: elapsedTime * 1000, // Convert to ms
            videoTime: 0, // No video in this test, so set to 0
            mimeType: mimeType || 'audio/webm',
          };
          setAudioChunk(chunk);
          
          // Directly call our standalone serialization method
          try {
            if (typeof window !== 'undefined') {
              console.log('Calling direct serialization method for audio blob:', {
                size: audioBlob.size,
                type: audioBlob.type
              });
            }
            
            // Call the serializeAndDeserialize method to handle the entire process
            await serializeAndDeserialize(audioBlob, mimeType || 'audio/webm');
          } catch (serializationError) {
            if (typeof window !== 'undefined') {
              console.error('Error in serialization process:', serializationError);
            }
            setDebugInfo({
              serializationError: serializationError instanceof Error ? 
                serializationError.message : String(serializationError),
              stage: 'serialization_process'
            });
          }
        } catch (recordingError) {
          // Handle any errors in the main try block
          if (typeof window !== 'undefined') {
            console.error('Error processing recording:', recordingError);
          }
          setDebugInfo({
            error: recordingError instanceof Error ? 
              recordingError.message : String(recordingError),
            stage: 'recording_processing'
          });
        } finally {
          // Clean up resources
          stream.getTracks().forEach(track => track.stop());
          
          if (timerRef.current) {
            clearInterval(timerRef.current);
            timerRef.current = null;
          }
        }
      };
      
      // Start the recorder and timer
      const startTime = Date.now();
      setRecordingStartTime(startTime);
      
      // Log that we're setting the recording start time
      if (typeof window !== 'undefined') {
        console.log('Setting recording start time:', startTime);
      }
      
      recorder.start();
      setIsRecording(true);
      
      // Update elapsed time every second
      timerRef.current = setInterval(() => {
        setElapsedTime(prev => prev + 1);
      }, 1000);
    } catch (error) {
      if (typeof window !== 'undefined') {
        console.error('Error starting recording:', error);
        alert(`Could not start recording: ${error instanceof Error ? error.message : String(error)}`);
      }
    }
  };
  
  // Stop recording
  const stopRecording = () => {
    if (mediaRecorderRef.current && mediaRecorderRef.current.state !== 'inactive') {
      mediaRecorderRef.current.stop();
      setIsRecording(false);
    }
  };
  
  // Format seconds as MM:SS
  const formatTime = (seconds: number) => {
    const mins = Math.floor(seconds / 60);
    const secs = seconds % 60;
    return `${mins}:${secs < 10 ? '0' : ''}${secs}`;
  };
  
  // Direct serialization method (similar to serialization-test page)
  const serializeAndDeserialize = async (blob: Blob, mimeType: string) => {
    if (typeof window !== 'undefined') {
      console.log('Starting direct serializeAndDeserialize method with blob:', {
        size: blob.size,
        type: blob.type,
        mimeType: mimeType
      });
    }
    
    try {
      // Step 1: Convert blob to base64 string
      const startTime1 = performance.now();
      const base64String = await blobToBase64(blob);
      const endTime1 = performance.now();
      
      // Set serialized string state
      setSerializedAudio(base64String);
      
      if (typeof window !== 'undefined') {
        console.log('Base64 conversion complete:', {
          originalSize: blob.size,
          base64Length: base64String.length,
          timeMs: (endTime1 - startTime1).toFixed(2)
        });
      }
      
      // Step 2: Convert base64 back to blob
      if (typeof window !== 'undefined') {
        console.log('Converting base64 back to blob using MIME type:', mimeType || 'audio/webm');
      }
      
      const startTime2 = performance.now();
      const newBlob = base64ToBlob(base64String, mimeType || 'audio/webm');
      const endTime2 = performance.now();
      
      if (typeof window !== 'undefined') {
        console.log('Blob conversion complete:', {
          newSize: newBlob.size, 
          newType: newBlob.type,
          timeMs: (endTime2 - startTime2).toFixed(2)
        });
      }
      
      // Step 3: Create audio URL from new blob
      const deserializedUrl = URL.createObjectURL(newBlob);
      setDeserializedAudio(deserializedUrl);
      
      // Set debug info for display
      setDebugInfo({
        originalBlob: {
          size: blob.size,
          type: blob.type,
          mimeType: mimeType || 'audio/webm'
        },
        base64String: {
          length: base64String.length,
          preview: base64String.substring(0, 50) + '...',
          conversionTimeMs: (endTime1 - startTime1).toFixed(2)
        },
        deserializedBlob: {
          size: newBlob.size,
          type: newBlob.type,
          conversionTimeMs: (endTime2 - startTime2).toFixed(2)
        },
        comparison: {
          sizeMatch: blob.size === newBlob.size,
          typeMatch: blob.type === newBlob.type,
          diffBytes: Math.abs(blob.size - newBlob.size)
        }
      });
      
      return {
        base64String,
        deserializedBlob: newBlob,
        deserializedUrl
      };
    } catch (error) {
      if (typeof window !== 'undefined') {
        console.error('Serialization process failed:', error);
      }
      setDebugInfo(prev => ({
        ...prev,
        serializationError: error instanceof Error ? error.message : String(error)
      }));
      throw error;
    }
  };

  // Create JSON representation of the audio chunk
  const createJsonRepresentation = async () => {
    if (!audioChunk) return;
    
    try {
      // Create a deep copy and convert blob to base64 if needed
      const chunkCopy = {...audioChunk};
      
      if (chunkCopy.blob instanceof Blob) {
        // Convert blob to base64
        chunkCopy.blob = await blobToBase64(chunkCopy.blob);
      }
      
      // Remove url property
      delete chunkCopy.url;
      
      // Return JSON representation
      return JSON.stringify(chunkCopy, null, 2);
    } catch (error) {
      if (typeof window !== 'undefined') {
        console.error('Error creating JSON representation:', error);
      }
      return null;
    }
  };
  
  // Attempt to play an audio chunk programmatically
  const testAudioChunkPlayback = async (chunk: AudioChunk | null) => {
    if (!chunk) {
      console.error('No audio chunk to test');
      return false;
    }
    
    try {
      console.log('Testing audio chunk playback:', {
        blobType: chunk.blob instanceof Blob ? 'Blob' : typeof chunk.blob,
        startTime: chunk.startTime,
        duration: chunk.duration,
        videoTime: chunk.videoTime,
        mimeType: chunk.mimeType || 'unknown'
      });
      
      // Create audio element
      const audio = new Audio();
      
      // Set up event listeners
      return new Promise<boolean>((resolve) => {
        audio.onloadedmetadata = () => {
          console.log('Audio metadata loaded:', {
            duration: audio.duration,
            readyState: audio.readyState
          });
        };
        
        audio.oncanplay = () => {
          console.log('Audio can play now');
          audio.play().catch(error => {
            console.error('Failed to play audio:', error);
            resolve(false);
          });
        };
        
        audio.onplay = () => {
          console.log('Audio playback started');
        };
        
        audio.onended = () => {
          console.log('Audio playback completed successfully');
          resolve(true);
        };
        
        audio.onerror = (e) => {
          console.error('Audio playback error:', {
            error: audio.error?.code,
            message: audio.error?.message
          });
          resolve(false);
        };
        
        // Handle different blob types
        let audioUrl: string;
        if (chunk.url) {
          audioUrl = chunk.url;
        } else if (chunk.blob instanceof Blob) {
          audioUrl = URL.createObjectURL(chunk.blob);
        } else if (typeof chunk.blob === 'string' && chunk.blob.startsWith('data:')) {
          // Option 1: Convert data URL to blob first
          try {
            // Split the data URL into parts
            const parts = chunk.blob.split(',');
            if (parts.length !== 2) {
              throw new Error('Invalid data URL format');
            }
            
            // Extract MIME type
            const mimeMatch = parts[0].match(/:(.*?);/);
            const mime = mimeMatch ? mimeMatch[1] : chunk.mimeType || 'audio/webm';
            
            // Convert base64 to binary
            const binary = atob(parts[1]);
            
            // Create array buffer
            const arrayBuffer = new ArrayBuffer(binary.length);
            const uint8Array = new Uint8Array(arrayBuffer);
            
            for (let i = 0; i < binary.length; i++) {
              uint8Array[i] = binary.charCodeAt(i);
            }
            
            // Create blob and URL
            const newBlob = new Blob([uint8Array], { type: mime });
            audioUrl = URL.createObjectURL(newBlob);
            console.log('Converted data URL to blob for playback:', { 
              size: newBlob.size, 
              type: newBlob.type 
            });
          } catch (error) {
            console.error('Error processing data URL:', error);
            resolve(false);
            return;
          }
        } else {
          console.error('Invalid audio data format:', typeof chunk.blob);
          resolve(false);
          return;
        }
        
        // Set source and start loading
        audio.src = audioUrl;
        audio.load();
        
        // Set timeout for overall operation
        setTimeout(() => {
          console.warn('Audio test timed out after 10 seconds');
          resolve(false);
        }, 10000);
      });
    } catch (error) {
      console.error('Error testing audio chunk:', error);
      return false;
    }
  };
  
  // Test audio and video synchronization
  const testAudioVideoSync = async () => {
    if (!videoPlayerRef.current || !audioRef.current) {
      console.error('Video or audio element not available for sync test');
      return false;
    }
    
    if (!recordedAudio) {
      console.error('No recorded audio available to test');
      return false;
    }
    
    setSyncTestResult(null);
    
    try {
      console.log('Starting audio-video synchronization test');
      
      // Reset both players
      videoPlayerRef.current.currentTime = 0;
      audioRef.current.currentTime = 0;
      
      // Set up recording and measurement variables
      const videoStartTime = Date.now();
      let audioStartTime = 0;
      let syncDifference = 0;
      
      // Create promise for detecting synchronization
      return new Promise<{success: boolean; details: string}>((resolve) => {
        // Set up audio event handlers
        const audioPlayHandler = () => {
          audioStartTime = Date.now();
          syncDifference = audioStartTime - videoStartTime;
          
          console.log('Audio playback started in sync test:', {
            videoStartTime,
            audioStartTime,
            syncDifference: `${syncDifference}ms`
          });
        };
        
        const audioEndHandler = () => {
          // Clean up event listeners
          audioRef.current?.removeEventListener('play', audioPlayHandler);
          audioRef.current?.removeEventListener('ended', audioEndHandler);
          
          // Calculate results
          const success = syncDifference < 500; // Less than 500ms difference is considered good
          
          console.log('Sync test completed:', {
            syncDifference: `${syncDifference}ms`,
            success,
            threshold: '500ms'
          });
          
          resolve({
            success,
            details: `Audio started ${syncDifference}ms after video. ${
              success ? 'Synchronization is good!' : 'Synchronization needs improvement.'
            }`
          });
        };
        
        // Add event listeners to the audio element
        audioRef.current.addEventListener('play', audioPlayHandler);
        audioRef.current.addEventListener('ended', audioEndHandler);
        
        // Start playing the video, which should trigger the audio
        const videoPlayPromise = videoPlayerRef.current.play();
        
        videoPlayPromise.catch(error => {
          console.error('Video playback failed during sync test:', error);
          audioRef.current?.removeEventListener('play', audioPlayHandler);
          audioRef.current?.removeEventListener('ended', audioEndHandler);
          
          resolve({
            success: false,
            details: `Failed to play video: ${error.message}. Synchronization test aborted.`
          });
        });
        
        // Set timeout for overall operation
        setTimeout(() => {
          audioRef.current?.removeEventListener('play', audioPlayHandler);
          audioRef.current?.removeEventListener('ended', audioEndHandler);
          
          console.warn('Sync test timed out after 20 seconds');
          resolve({
            success: false,
            details: 'Synchronization test timed out. Check console for details.'
          });
        }, 20000);
      });
    } catch (error) {
      console.error('Error during sync test:', error);
      return {
        success: false,
        details: `Error during synchronization test: ${error instanceof Error ? error.message : String(error)}`
      };
    }
  };
  
  // Clean up on unmount
  useEffect(() => {
    return () => {
      if (timerRef.current) {
        clearInterval(timerRef.current);
      }
      if (recordedAudio) {
        URL.revokeObjectURL(recordedAudio);
      }
      if (deserializedAudio) {
        URL.revokeObjectURL(deserializedAudio);
      }
    };
  }, [recordedAudio, deserializedAudio]);
  
  return (
    <div className="p-8 max-w-4xl mx-auto">
      <div className="mb-6">
        <Link href="/" className="text-blue-500 hover:underline mb-4 inline-block">
          &larr; Back to main app
        </Link>
        <h1 className="text-3xl font-bold mb-4">Audio Recording Test</h1>
        <p className="text-gray-600 mb-4">
          This page provides a standalone audio recorder to test recording and playback functionality.
        </p>
        <div className="bg-amber-50 border-l-4 border-amber-500 p-4 text-amber-700">
          <h3 className="font-bold">Dual Playback Test</h3>
          <p>This enhanced test compares direct audio playback with JSON-serialized playback (mimicking the main app).</p>
        </div>
      </div>
      
      {/* Video player section */}
      <div className="mb-6">
        <h2 className="text-xl font-semibold mb-4">Video Player</h2>
        <div className="flex items-center justify-between mb-3 bg-white p-4 rounded-lg">
          <p className="text-sm text-gray-600">
            This video will synchronize with audio playback when testing recordings.
          </p>
          <div className="flex items-center">
            <label className="flex items-center cursor-pointer">
              <span className="mr-2 text-sm">Sync with Audio:</span>
              <div 
                className={`relative inline-block w-10 h-6 transition-colors duration-200 ease-in-out rounded-full ${syncWithVideo ? 'bg-green-500' : 'bg-gray-300'}`}
                onClick={() => setSyncWithVideo(!syncWithVideo)}
              >
                <span 
                  className={`absolute left-1 top-1 bottom-1 w-4 h-4 transition-transform duration-200 ease-in-out bg-white rounded-full ${syncWithVideo ? 'transform translate-x-4' : ''}`}
                ></span>
              </div>
            </label>
          </div>
        </div>
        
        <VideoPlayer 
          setVideoRef={(ref) => {
            videoPlayerRef.current = ref;
            
            // Add our custom event listeners for sync
            if (ref) {
              ref.addEventListener('play', () => setVideoPlaying(true));
              ref.addEventListener('pause', () => setVideoPlaying(false));
            }
          }}
        />
        
        <div className="bg-white p-3 rounded-lg mt-2">
          <div className="flex justify-between items-center mb-2">
            <div className="text-xs text-gray-500">
              {videoPlaying ? (
                <span className="text-green-500 flex items-center">
                  <span className="h-2 w-2 bg-green-500 rounded-full mr-1 animate-pulse"></span> 
                  Video is playing
                </span>
              ) : (
                <span>Video is paused</span>
              )}
            </div>
            {syncWithVideo && (
              <div className="text-blue-500 font-medium text-xs">
                ✓ Audio synchronization is enabled
              </div>
            )}
          </div>
          
          {recordedAudio && (
            <div className="mt-2">
              <button
                onClick={async () => {
                  setSyncTestResult(null);
                  const result = await testAudioVideoSync();
                  if (result) {
                    setSyncTestResult(result);
                  }
                }}
                className="bg-indigo-500 hover:bg-indigo-600 text-white text-sm py-1 px-3 rounded"
              >
                Test Audio-Video Sync
              </button>
              
              {syncTestResult && (
                <div className={`mt-2 p-2 text-xs rounded ${
                  syncTestResult.success ? 'bg-green-100 text-green-700' : 'bg-amber-100 text-amber-700'
                }`}>
                  {syncTestResult.success ? '✓ ' : '⚠ '}
                  {syncTestResult.details}
                </div>
              )}
            </div>
          )}
        </div>
      </div>
      
      <div className="bg-white p-6 rounded-lg shadow-md">
        <div className="mb-6">
          <h2 className="text-xl font-semibold mb-2">Audio Recorder</h2>
          <p className="text-sm text-gray-500 mb-4">
            Recording format: {recordingFormat || 'Not determined yet'}
          </p>
          
          <div className="flex items-center space-x-4 mb-4">
            {!isRecording ? (
              <button
                onClick={startRecording}
                className="flex items-center px-4 py-2 bg-red-500 text-white rounded-md hover:bg-red-600"
              >
                <span className="h-3 w-3 bg-white rounded-full mr-2"></span>
                Start Recording
              </button>
            ) : (
              <button
                onClick={stopRecording}
                className="flex items-center px-4 py-2 bg-gray-700 text-white rounded-md hover:bg-gray-800"
              >
                <span className="h-3 w-3 bg-white mr-2"></span>
                Stop Recording
              </button>
            )}
            <span className="text-gray-500">
              {isRecording ? `Recording: ${formatTime(elapsedTime)}` : ''}
            </span>
          </div>
          
          {isRecording && (
            <div className="flex items-center mb-4">
              <span className="h-3 w-3 bg-red-500 rounded-full animate-pulse mr-2"></span>
              <span className="text-red-500">Microphone active</span>
            </div>
          )}
        </div>
        
        {recordedAudio && (
          <div className="border-t pt-4">
            <h2 className="text-xl font-semibold mb-4">Audio Playback Comparison</h2>
            
            <div className="grid grid-cols-1 md:grid-cols-2 gap-6">
              {/* Direct Playback */}
              <div className="bg-gray-100 p-4 rounded-md">
                <h3 className="font-semibold text-lg mb-2 text-blue-700">1. Direct Playback</h3>
                <p className="text-sm text-gray-500 mb-2">
                  Direct blob URL playback (known to work)
                </p>
                
                <audio 
                  ref={audioRef}
                  src={recordedAudio} 
                  controls 
                  className="w-full mb-3"
                />
                
                <div className="text-xs text-gray-600 mb-2">
                  Recording length: {formatTime(recordingDuration)}
                </div>
                
                <div className="mt-2 flex flex-wrap gap-2">
                  <button
                    onClick={() => {
                      if (audioRef.current) {
                        audioRef.current.currentTime = 0;
                        
                        // If sync is enabled, start the video first
                        if (syncWithVideo && videoPlayerRef.current) {
                          // Reset the video to the beginning
                          videoPlayerRef.current.currentTime = 0;
                          
                          // Start playing the video
                          const playPromise = videoPlayerRef.current.play();
                          
                          // Video play is a promise that may be rejected (e.g., if user hasn't interacted with page)
                          playPromise.then(() => {
                            // Once video is playing, play the audio
                            audioRef.current?.play();
                            setVideoPlaying(true);
                          }).catch(err => {
                            console.warn('Video play failed, playing audio only:', err);
                            // Fall back to audio-only if video fails
                            audioRef.current?.play();
                          });
                        } else {
                          // Just play audio if sync is disabled
                          audioRef.current.play();
                        }
                      }
                    }}
                    className="px-3 py-1 bg-blue-500 text-white text-sm rounded hover:bg-blue-600"
                  >
                    Replay
                  </button>
                  
                  <button
                    onClick={() => {
                      const a = document.createElement('a');
                      a.href = recordedAudio;
                      a.download = `audio-recording-${new Date().toISOString()}.webm`;
                      document.body.appendChild(a);
                      a.click();
                      document.body.removeChild(a);
                    }}
                    className="px-3 py-1 bg-green-500 text-white text-sm rounded hover:bg-green-600"
                  >
                    Download
                  </button>
                </div>
              </div>
              
              {/* Serialized Playback */}
              <div className="bg-gray-100 p-4 rounded-md">
                <h3 className="font-semibold text-lg mb-2 text-purple-700">2. Serialized Playback</h3>
                <p className="text-sm text-gray-500 mb-2">
                  JSON serialized and deserialized (mimics main app)
                </p>
                
                {deserializedAudio ? (
                  <audio 
                    ref={audioSerializedRef}
                    src={deserializedAudio} 
                    controls 
                    className="w-full mb-3"
                  />
                ) : (
                  <div className="w-full h-12 bg-gray-200 flex items-center justify-center text-gray-500 mb-3">
                    Waiting for deserialization...
                  </div>
                )}
                
                <div className="text-xs text-gray-600 mb-2">
                  Conversion: Blob → base64 → Blob → URL
                </div>
                
                <div className="mt-2 flex flex-wrap gap-2">
                  <button
                    onClick={() => {
                      if (audioSerializedRef.current) {
                        audioSerializedRef.current.currentTime = 0;
                        
                        // If sync is enabled, start the video first
                        if (syncWithVideo && videoPlayerRef.current) {
                          // Reset the video to the beginning
                          videoPlayerRef.current.currentTime = 0;
                          
                          // Start playing the video
                          const playPromise = videoPlayerRef.current.play();
                          
                          // Video play is a promise that may be rejected (e.g., if user hasn't interacted with page)
                          playPromise.then(() => {
                            // Once video is playing, play the audio
                            audioSerializedRef.current?.play();
                            setVideoPlaying(true);
                          }).catch(err => {
                            console.warn('Video play failed, playing audio only:', err);
                            // Fall back to audio-only if video fails
                            audioSerializedRef.current?.play();
                          });
                        } else {
                          // Just play audio if sync is disabled
                          audioSerializedRef.current.play();
                        }
                      }
                    }}
                    disabled={!deserializedAudio}
                    className={`px-3 py-1 text-white text-sm rounded ${deserializedAudio ? 'bg-purple-500 hover:bg-purple-600' : 'bg-gray-400'}`}
                  >
                    Replay
                  </button>
                  
                  <button
                    onClick={async () => {
                      const jsonRepresentation = await createJsonRepresentation();
                      if (jsonRepresentation) {
                        const a = document.createElement('a');
                        const blob = new Blob([jsonRepresentation], { type: 'application/json' });
                        a.href = URL.createObjectURL(blob);
                        a.download = `audio-chunk-${new Date().toISOString()}.json`;
                        document.body.appendChild(a);
                        a.click();
                        document.body.removeChild(a);
                        URL.revokeObjectURL(a.href);
                      }
                    }}
                    disabled={!audioChunk}
                    className={`px-3 py-1 text-white text-sm rounded ${audioChunk ? 'bg-indigo-500 hover:bg-indigo-600' : 'bg-gray-400'}`}
                  >
                    Export JSON
                  </button>
                  
                  <button
                    onClick={async () => {
                      if (!audioChunk) return;
                      
                      setPlaybackTestRunning(true);
                      setPlaybackTestResult(null);
                      
                      try {
                        const success = await testAudioChunkPlayback(audioChunk);
                        setPlaybackTestResult({
                          success,
                          details: success 
                            ? "Playback test completed successfully!" 
                            : "Playback test failed. Check console for details."
                        });
                      } catch (error) {
                        setPlaybackTestResult({
                          success: false,
                          details: `Test error: ${error instanceof Error ? error.message : String(error)}`
                        });
                      } finally {
                        setPlaybackTestRunning(false);
                      }
                    }}
                    disabled={!audioChunk || playbackTestRunning}
                    className={`px-3 py-1 text-white text-sm rounded ${
                      !audioChunk || playbackTestRunning 
                        ? 'bg-gray-400' 
                        : 'bg-yellow-500 hover:bg-yellow-600'
                    }`}
                  >
                    {playbackTestRunning ? 'Testing...' : 'Test Playback'}
                  </button>
                </div>
                
                {playbackTestResult && (
                  <div className={`mt-3 p-2 text-xs rounded ${
                    playbackTestResult.success 
                      ? 'bg-green-100 text-green-700' 
                      : 'bg-red-100 text-red-700'
                  }`}>
                    <strong>{playbackTestResult.success ? 'Success:' : 'Error:'}</strong> {playbackTestResult.details}
                  </div>
                )}
                
                {debugInfo.serializationError && (
                  <div className="mt-3 p-2 bg-red-100 text-red-700 text-xs rounded">
                    <strong>Error:</strong> {debugInfo.serializationError}
                  </div>
                )}
              </div>
            </div>
            
            <div className="mt-4 flex justify-center">
              <button
                onClick={() => {
                  setRecordedAudio(null);
                  setRecordedBlob(null);
                  setSerializedAudio(null);
                  setDeserializedAudio(null);
                  setAudioChunk(null);
                  setDebugInfo({});
                  chunksRef.current = [];
                }}
                className="px-3 py-1 bg-gray-500 text-white text-sm rounded hover:bg-gray-600"
              >
                Clear All
              </button>
            </div>
          </div>
        )}
        
        {/* Serialization Debug Information */}
        {Object.keys(debugInfo).length > 0 && (
          <div className="mt-6 border-t pt-4">
            <h3 className="font-semibold text-lg mb-2">Debug Information</h3>
            
            <div className="bg-gray-800 text-green-300 p-4 rounded font-mono text-xs overflow-x-auto">
              <pre>
{`Audio Serialization Debug:
------------------------
Original Blob:
  Size: ${debugInfo.originalBlob?.size} bytes (${debugInfo.originalBlob?.size ? (debugInfo.originalBlob.size / 1024).toFixed(2) : '?'} KB)
  Type: ${debugInfo.originalBlob?.type}
  MIME: ${debugInfo.originalBlob?.mimeType}

Base64 Conversion:
  Length: ${debugInfo.base64String?.length} characters
  Time: ${debugInfo.base64String?.conversionTimeMs}ms
  Preview: ${debugInfo.base64String?.preview}

Deserialized Blob:
  Size: ${debugInfo.deserializedBlob?.size} bytes (${debugInfo.deserializedBlob?.size ? (debugInfo.deserializedBlob.size / 1024).toFixed(2) : '?'} KB)
  Type: ${debugInfo.deserializedBlob?.type}
  Time: ${debugInfo.deserializedBlob?.conversionTimeMs}ms

Comparison:
  Size Match: ${debugInfo.comparison?.sizeMatch ? '✅ Yes' : '❌ No'}
  Type Match: ${debugInfo.comparison?.typeMatch ? '✅ Yes' : '❌ No'}
  Size Difference: ${debugInfo.comparison?.diffBytes || 0} bytes
`}
              </pre>
            </div>
            
            {/* Add base64 string preview and download button */}
            {serializedAudio && (
              <div className="mt-4">
                <h4 className="font-semibold mb-2">Base64 String (first 100 chars)</h4>
                <div className="bg-gray-100 p-3 rounded overflow-x-auto">
                  <code className="text-xs break-all">
                    {serializedAudio.substring(0, 100)}...
                  </code>
                </div>
                <button
                  onClick={() => {
                    if (serializedAudio) {
                      const a = document.createElement('a');
                      const blob = new Blob([serializedAudio], { type: 'text/plain' });
                      a.href = URL.createObjectURL(blob);
                      a.download = `audio-base64-${new Date().toISOString()}.txt`;
                      document.body.appendChild(a);
                      a.click();
                      document.body.removeChild(a);
                      URL.revokeObjectURL(a.href);
                    }
                  }}
                  className="mt-2 px-3 py-1 bg-gray-500 text-white text-sm rounded hover:bg-gray-600"
                >
                  Download Base64 String
                </button>
              </div>
            )}
          </div>
        )}
        
        <div className="mt-8 border-t pt-4">
          <h3 className="font-semibold text-lg mb-2">Cross-Browser Compatibility</h3>
          
          <div className="bg-blue-50 p-4 rounded-md mb-4">
            <p className="text-sm text-blue-800 mb-2">
              This section helps diagnose audio recording and playback issues across different browsers.
              Below is a compatibility chart based on our testing.
            </p>
            
            <div className="overflow-x-auto">
              <table className="w-full text-xs border-collapse">
                <thead>
                  <tr className="bg-blue-100">
                    <th className="border border-blue-200 p-2">Browser</th>
                    <th className="border border-blue-200 p-2">Direct Recording</th>
                    <th className="border border-blue-200 p-2">Direct Playback</th>
                    <th className="border border-blue-200 p-2">Serialized Playback</th>
                    <th className="border border-blue-200 p-2">Preferred Format</th>
                    <th className="border border-blue-200 p-2">Notes</th>
                  </tr>
                </thead>
                <tbody>
                  <tr>
                    <td className="border border-blue-200 p-2">Chrome (Desktop)</td>
                    <td className="border border-blue-200 p-2 text-green-600">✓ Works</td>
                    <td className="border border-blue-200 p-2 text-green-600">✓ Works</td>
                    <td className="border border-blue-200 p-2 text-green-600">✓ Works</td>
                    <td className="border border-blue-200 p-2">audio/webm;codecs=opus</td>
                    <td className="border border-blue-200 p-2">Most reliable</td>
                  </tr>
                  <tr>
                    <td className="border border-blue-200 p-2">Firefox (Desktop)</td>
                    <td className="border border-blue-200 p-2 text-green-600">✓ Works</td>
                    <td className="border border-blue-200 p-2 text-green-600">✓ Works</td>
                    <td className="border border-blue-200 p-2 text-green-600">✓ Works</td>
                    <td className="border border-blue-200 p-2">audio/webm</td>
                    <td className="border border-blue-200 p-2">Works well</td>
                  </tr>
                  <tr>
                    <td className="border border-blue-200 p-2">Safari (Desktop)</td>
                    <td className="border border-blue-200 p-2 text-yellow-600">⚠ Variable</td>
                    <td className="border border-blue-200 p-2 text-green-600">✓ Works</td>
                    <td className="border border-blue-200 p-2 text-yellow-600">⚠ Variable</td>
                    <td className="border border-blue-200 p-2">audio/mp4</td>
                    <td className="border border-blue-200 p-2">May require explicit user interaction</td>
                  </tr>
                  <tr>
                    <td className="border border-blue-200 p-2">Chrome (Mobile)</td>
                    <td className="border border-blue-200 p-2 text-green-600">✓ Works</td>
                    <td className="border border-blue-200 p-2 text-green-600">✓ Works</td>
                    <td className="border border-blue-200 p-2 text-green-600">✓ Works</td>
                    <td className="border border-blue-200 p-2">audio/webm;codecs=opus</td>
                    <td className="border border-blue-200 p-2">Touch interaction required</td>
                  </tr>
                  <tr>
                    <td className="border border-blue-200 p-2">Safari (iOS)</td>
                    <td className="border border-blue-200 p-2 text-yellow-600">⚠ Variable</td>
                    <td className="border border-blue-200 p-2 text-green-600">✓ Works</td>
                    <td className="border border-blue-200 p-2 text-yellow-600">⚠ Variable</td>
                    <td className="border border-blue-200 p-2">audio/mp4</td>
                    <td className="border border-blue-200 p-2">Requires explicit user interaction</td>
                  </tr>
                </tbody>
              </table>
            </div>
          </div>
          
          <div className="text-xs text-gray-500">
            <h4 className="font-semibold mb-1">Your Browser Information:</h4>
            <ul className="list-disc pl-5 space-y-1">
              <li>
                <BrowserInfo />
              </li>
              <li>
                <MimeTypesInfo />
              </li>
            </ul>
          </div>
        </div>
      </div>
    </div>
  );
}
|| END ||


|| START ./app/layout.tsx ||

import type { Metadata } from "next";
import { Geist, Geist_Mono } from "next/font/google";
import "./globals.css";

const geistSans = Geist({
  variable: "--font-geist-sans",
  subsets: ["latin"],
});

const geistMono = Geist_Mono({
  variable: "--font-geist-mono",
  subsets: ["latin"],
});

export const metadata: Metadata = {
  title: "Create Next App",
  description: "Generated by create next app",
};

export default function RootLayout({
  children,
}: Readonly<{
  children: React.ReactNode;
}>) {
  return (
    <html lang="en">
      <body
        className={`${geistSans.variable} ${geistMono.variable} antialiased`}
      >
        {children}
      </body>
    </html>
  );
}

|| END ||


|| START ./app/page.tsx ||

"use client"

import Image from "next/image";
import VideoPlayerWrapper from "../src/components/VideoPlayerWrapper";
import { useState, useCallback, useEffect } from "react";

export default function Home() {
  // Interface for review content configuration
  interface DataLabelingProperty {
    id: string;
    label: string;
  }
  
  interface KeyMetric {
    name: string;
    value: string | number;
  }

  interface ReviewContent {
    videoUrl: string;
    videoTitle?: string;
    videoDescription?: string;
    dataLabelingTitle: string;
    labelProperties: DataLabelingProperty[];
    keyMetricsTitle?: string;
    keyMetrics?: KeyMetric[];
  }
  
  // Example content to be reviewed
  const contentToReview: ReviewContent = {
    videoUrl: "https://commondatastorage.googleapis.com/gtv-videos-bucket/sample/BigBuckBunny.mp4",
    videoTitle: "Big Buck Bunny",
    videoDescription: "A short animated film featuring a big rabbit dealing with three bullying rodents",
    dataLabelingTitle: "Animation Categories",
    labelProperties: [
      { id: "artisticStyle", label: "Artistic Style" },
      { id: "characterDesign", label: "Character Design" },
      { id: "backgroundSettings", label: "Background Settings" },
      { id: "motionDynamics", label: "Motion Dynamics" },
      { id: "colorPalette", label: "Color Palette" },
      { id: "soundEffects", label: "Sound Effects" },
      { id: "visualEffects", label: "Visual Effects" },
      { id: "narrativeTechniques", label: "Narrative Techniques" },
      { id: "perspectiveView", label: "Perspective View" },
      { id: "lightingShadows", label: "Lighting & Shadows" },
    ],
    keyMetricsTitle: "Production Metrics",
    keyMetrics: [
      { name: "Runtime", value: "10:34" },
      { name: "Release Year", value: 2008 },
      { name: "Production Budget", value: "$150,000" },
      { name: "Character Count", value: 4 },
      { name: "Animation Team Size", value: 12 },
      { name: "Frames Rendered", value: "15,240" },
      { name: "Software Used", value: "Blender" },
      { name: "Render Time (hours)", value: 687 }
    ]
  };
  
  // Generate initial categories state from labelProperties
  const initialCategories = contentToReview.labelProperties.reduce((acc, prop) => {
    acc[prop.id] = false;
    return acc;
  }, {} as Record<string, boolean>);
  
  // State for checkboxes during recording
  const [categories, setCategories] = useState(initialCategories);
  
  // State to track if we're in replay mode
  const [isReplayMode, setIsReplayMode] = useState(false);
  
  // State to track active recording
  const [isRecording, setIsRecording] = useState(false);
  
  // State for category list that will be shown during replay
  const [categoryList, setCategoryList] = useState<string[]>([]);
  
  // Define window type with our custom properties
  declare global {
    interface Window {
      __videoPlayerWrapper?: {
        recordCategoryChange: (category: string, checked: boolean) => void;
        isRecording: boolean;
      };
      __hasRecordedSession?: boolean;
    }
  }
  
  // State to track if we have a recorded session available
  const [hasRecordedSession, setHasRecordedSession] = useState(false);
  
  // Get formatted category label
  const getCategoryLabel = (category: string) => {
    return category.replace(/([A-Z])/g, ' $1').trim().replace(/^./, str => str.toUpperCase());
  };
  
  const handleCategoryChange = (category: string) => {
    const newValue = !categories[category as keyof typeof categories];
    console.log(`RECORDING: Changing category ${category} to ${newValue}`);
    
    const newCategories = {
      ...categories,
      [category]: newValue,
    };
    
    console.log('RECORDING: New categories state:', newCategories);
    setCategories(newCategories);
    
    // Record the category change event in the orchestrator if we're recording
    if (typeof window !== 'undefined' && window.__videoPlayerWrapper?.isRecording) {
      console.log(`RECORDING: Sending category event to orchestrator: ${category}=${newValue}`);
      
      try {
        window.__videoPlayerWrapper.recordCategoryChange(category, newValue);
        console.log('RECORDING: Category event sent successfully');
      } catch (error) {
        console.error('RECORDING: Error sending category event:', error);
      }
    } else {
      console.warn('RECORDING: Not recording category event - isRecording is false or wrapper not available');
      console.log('RECORDING: window.__videoPlayerWrapper?.isRecording =', 
        typeof window !== 'undefined' ? window.__videoPlayerWrapper?.isRecording : 'window undefined');
    }
  };
  
  // Function to clear all categories (called when recording stops)
  const clearCategories = useCallback(() => {
    // Reset all categories to false
    const resetCategories = Object.keys(categories).reduce((acc, key) => {
      acc[key] = false;
      return acc;
    }, {} as Record<string, boolean>);
    
    setCategories(resetCategories);
    
    // Also clear the category list for replay mode
    setCategoryList([]);
  }, []);
  
  // Function to handle categories during replay
  const handleCategoryAddedDuringReplay = useCallback((categoryChanges: Record<string, boolean>) => {
    console.log('PARENT: Received categories for replay:', categoryChanges);
    
    // Debug log all entries
    Object.entries(categoryChanges).forEach(([key, value]) => {
      console.log(`PARENT: Category ${key} = ${value}`);
    });
    
    // Convert all checked categories to formatted labels
    const checkedCategories = Object.entries(categoryChanges)
      .filter(([_, isChecked]) => isChecked)
      .map(([categoryName, _]) => {
        const label = getCategoryLabel(categoryName);
        console.log(`PARENT: Formatting category ${categoryName} to ${label}`);
        return label;
      });
    
    if (checkedCategories.length > 0) {
      console.log(`PARENT: Adding ${checkedCategories.length} categories to replay list:`, checkedCategories);
      
      // Force state update with a deep copy and force render with a callback
      const newList = [...checkedCategories];
      console.log('PARENT: Setting category list to:', newList);
      
      // Ensure we're in replay mode
      setIsReplayMode(true);
      
      // Replace the entire list at once with a forced update
      setCategoryList(newList);
      
      // Force UI update by using double setState in different ticks for React 18+ batching
      setTimeout(() => {
        setCategoryList(prevList => {
          console.log('PARENT: Forced update, category list is now:', prevList);
          return prevList; // Return same array but force an update
        });
        
        // Double-check in the next tick
        setTimeout(() => {
          console.log('PARENT: After update, category list should be:', newList);
        }, 100);
      }, 50);
    } else {
      console.log('PARENT: No checked categories found');
      setCategoryList([]);
    }
  }, []);
  
  // Function to handle replay mode change
  const handleReplayModeChange = useCallback((isReplay: boolean) => {
    console.log(`Setting replay mode to: ${isReplay}`);
    setIsReplayMode(isReplay);
    
    // Clear the category list when entering/exiting replay mode
    if (isReplay) {
      setCategoryList([]);
    }
  }, []);
  
  // Function to update recording state (attached to global window object for access)
  useEffect(() => {
    // Check window object for recording state and session availability
    const checkState = () => {
      if (typeof window !== 'undefined') {
        // Check recording state
        if (window.__videoPlayerWrapper) {
          setIsRecording(!!window.__videoPlayerWrapper.isRecording);
        }
        
        // Check session availability
        const hasSession = !!window.__hasRecordedSession;
        if (hasSession !== hasRecordedSession) {
          console.log(`Session availability changed: ${hasSession}`);
          setHasRecordedSession(hasSession);
        }
      }
    };
    
    // Initial check
    checkState();
    
    // Set up interval to periodically check state
    const interval = setInterval(checkState, 300);
    
    // Listen for a custom event that might be triggered when session is available
    const handleSessionChange = () => {
      console.log('Received session change event');
      checkState();
    };
    
    window.addEventListener('session-available', handleSessionChange);
    
    return () => {
      clearInterval(interval);
      window.removeEventListener('session-available', handleSessionChange);
    };
  }, [hasRecordedSession]);
  
  // Force client-side rendering for window access
  const [isClient, setIsClient] = useState(false);
  useEffect(() => {
    setIsClient(true);
  }, []);

  return (
    <div className="grid grid-rows-[auto_1fr_auto] items-center justify-items-center min-h-screen p-4 pb-8 gap-4 sm:p-8 font-[family-name:var(--font-geist-sans)]">
      <main className="flex flex-col gap-4 w-full max-w-4xl items-center">
        <div className="w-full flex flex-col sm:flex-row justify-between items-center gap-2 mb-4">
          <h1 className="text-2xl font-bold">Cartoon Annotation Tool</h1>
          <div className="action-buttons flex space-x-2">
            {/* Record Button - Always visible but disabled during replay */}
            <button
              onClick={() => document.getElementById(isClient && isRecording ? 'stopButton' : 'startRecordingButton')?.click()}
              disabled={isReplayMode}
              className={`flex items-center justify-center gap-1 w-36 py-2 px-4 rounded-md transition-colors whitespace-nowrap ${
                isReplayMode
                  ? 'bg-gray-300 text-gray-500 cursor-not-allowed'
                  : isClient && isRecording 
                    ? 'bg-gray-700 hover:bg-gray-800 text-white' 
                    : 'bg-red-500 hover:bg-red-600 text-white'
              }`}
            >
              {isClient && isRecording ? (
                <>
                  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="currentColor" className="w-5 h-5">
                    <path fillRule="evenodd" d="M6.75 5.25a.75.75 0 0 1 .75-.75H9a.75.75 0 0 1 .75.75v13.5a.75.75 0 0 1-.75.75H7.5a.75.75 0 0 1-.75-.75V5.25Zm7.5 0A.75.75 0 0 1 15 4.5h1.5a.75.75 0 0 1 .75.75v13.5a.75.75 0 0 1-.75.75H15a.75.75 0 0 1-.75-.75V5.25Z" clipRule="evenodd" />
                  </svg>
                  <span>Stop</span>
                </>
              ) : (
                <>
                  <span className="h-2 w-2 rounded-full bg-white"></span>
                  <span>Record</span>
                </>
              )}
            </button>
            
            {/* Replay Button - Toggles between Start/Stop Replay */}
            <button
              onClick={() => document.getElementById(isReplayMode ? 'stopButton' : 'startReplayButton')?.click()}
              disabled={isClient && (isRecording || (!hasRecordedSession && !isReplayMode))}
              className={`flex items-center gap-1 py-2 px-4 rounded-md transition-colors ${
                isClient && (isRecording || (!hasRecordedSession && !isReplayMode))
                  ? 'bg-gray-300 text-gray-500 cursor-not-allowed'
                  : isReplayMode
                    ? 'bg-yellow-500 hover:bg-yellow-600 text-white'
                    : 'bg-green-600 hover:bg-green-700 text-white'
              }`}
            >
              {isReplayMode ? (
                <>
                  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="currentColor" className="w-5 h-5">
                    <path fillRule="evenodd" d="M6.75 5.25a.75.75 0 0 1 .75-.75H9a.75.75 0 0 1 .75.75v13.5a.75.75 0 0 1-.75.75H7.5a.75.75 0 0 1-.75-.75V5.25Zm7.5 0A.75.75 0 0 1 15 4.5h1.5a.75.75 0 0 1 .75.75v13.5a.75.75 0 0 1-.75.75H15a.75.75 0 0 1-.75-.75V5.25Z" clipRule="evenodd" />
                  </svg>
                  <span>Stop Replay</span>
                </>
              ) : (
                <>
                  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="currentColor" className="w-5 h-5">
                    <path fillRule="evenodd" d="M4.5 5.653c0-1.427 1.529-2.33 2.779-1.643l11.54 6.347c1.295.712 1.295 2.573 0 3.286l-11.54 6.347c-1.25.687-2.779-.217-2.779-1.643V5.653Z" clipRule="evenodd" />
                  </svg>
                  <span>Replay Session</span>
                </>
              )}
            </button>
            
            {/* Download Button - Always visible */}
            <button
              onClick={() => document.getElementById('downloadDataButton')?.click()}
              disabled={isClient && (isRecording || !hasRecordedSession)}
              className={`py-2 px-4 rounded-md transition-colors ${
                isClient && (isRecording || !hasRecordedSession)
                  ? 'bg-gray-300 text-gray-500 cursor-not-allowed' 
                  : 'bg-blue-500 hover:bg-blue-600 text-white'
              }`}
            >
              Download Data
            </button>
            
            {/* Load Data Button - Always visible */}
            <label className="bg-purple-500 hover:bg-purple-600 text-white py-2 px-4 rounded-md transition-colors cursor-pointer">
              Load Data
              <input 
                type="file" 
                accept=".json" 
                onChange={(e) => {
                  const fileInput = document.getElementById('fileUploadInput') as HTMLInputElement;
                  if (fileInput && e.target.files && e.target.files.length > 0) {
                    // Create a new DataTransfer object and add the file
                    const dataTransfer = new DataTransfer();
                    dataTransfer.items.add(e.target.files[0]);
                    
                    // Set the files property to the new DataTransfer's files
                    fileInput.files = dataTransfer.files;
                    
                    // Trigger the change event
                    const event = new Event('change', { bubbles: true });
                    fileInput.dispatchEvent(event);
                  }
                }}
                className="hidden"
              />
            </label>
          </div>
        </div>
        
        <div className="flex flex-col lg:flex-row w-full gap-4">
          {/* Categories Section - 1/4 */}
          <div className="w-full lg:w-1/4">
            <div className="p-4 border rounded-lg bg-gray-50 h-full">
              <h2 className="text-xl font-semibold mb-3">{contentToReview.dataLabelingTitle}</h2>
              
              {/* Show checkboxes during recording mode */}
              {!isReplayMode ? (
                <div className="space-y-2">
                  {contentToReview.labelProperties.map((property) => (
                    <div key={property.id} className="flex items-center">
                      <input 
                        type="checkbox" 
                        id={property.id} 
                        checked={categories[property.id] || false}
                        onChange={() => handleCategoryChange(property.id)}
                        className="h-4 w-4 mr-2"
                      />
                      <label htmlFor={property.id}>{property.label}</label>
                    </div>
                  ))}
                </div>
              ) : (
                // Show a simple list during replay mode
                <div className="replay-categories">
                  {categoryList.length === 0 ? (
                    <p className="text-gray-500 italic">Categories will appear here when session loads.</p>
                  ) : (
                    <>
                      <p className="text-sm font-medium mb-2">Selected animation categories:</p>
                      <ul className="list-disc pl-5 space-y-1">
                        {categoryList.map((category, index) => (
                          <li key={index} className="text-green-600 font-medium">
                            {category}
                          </li>
                        ))}
                      </ul>
                      <p className="text-xs text-gray-500 mt-2">{categoryList.length} categories selected</p>
                    </>
                  )}
                </div>
              )}
            </div>
          </div>
          
          {/* Video Player - 1/2 */}
          <div className="w-full lg:w-1/2">
            <VideoPlayerWrapper 
              categories={categories}
              onCategoriesCleared={clearCategories}
              onCategoriesLoaded={handleCategoryAddedDuringReplay}
              onReplayModeChange={handleReplayModeChange}
              videoUrl={contentToReview.videoUrl}
              videoId={contentToReview.videoTitle?.replace(/\s+/g, '-').toLowerCase()}
              contentToReview={contentToReview}
            />
          </div>
          
          {/* Key Metrics Section - 1/4 */}
          {contentToReview.keyMetrics && contentToReview.keyMetrics.length > 0 && (
            <div className="w-full lg:w-1/4">
              <div className="p-4 border rounded-lg bg-gray-50 h-full">
                <h2 className="text-xl font-semibold mb-3">{contentToReview.keyMetricsTitle || "Key Metrics"}</h2>
                <div className="flex flex-col gap-3">
                  {contentToReview.keyMetrics.map((metric, index) => (
                    <div key={index} className="p-2 bg-white rounded shadow-sm">
                      <span className="block text-xs text-gray-500">{metric.name}</span>
                      <span className="text-lg font-semibold">{metric.value}</span>
                    </div>
                  ))}
                </div>
              </div>
            </div>
          )}
        </div>
      </main>
    </div>
  );
}

|| END ||


|| START ./app/globals.css ||

@import "tailwindcss";

:root {
  --background: #ffffff;
  --foreground: #171717;
}

@theme inline {
  --color-background: var(--background);
  --color-foreground: var(--foreground);
  --font-sans: var(--font-geist-sans);
  --font-mono: var(--font-geist-mono);
}

@media (prefers-color-scheme: dark) {
  :root {
    --background: #0a0a0a;
    --foreground: #ededed;
  }
}

body {
  background: var(--background);
  color: var(--foreground);
  font-family: Arial, Helvetica, sans-serif;
}

|| END ||


|| START ./postcss.config.mjs ||

const config = {
  plugins: ["@tailwindcss/postcss"],
};

export default config;

|| END ||


|| START ./README.md ||

# Cartoon Annotation Tool

A Next.js application for recording, annotating, and replaying video sessions with synchronized audio, video, drawing capabilities, and animation category tagging.

## Overview

This tool allows users to:
- Record synchronized audio while watching and interacting with videos
- Add visual annotations and drawings directly on the video
- Tag animations with specific categories (e.g., Artistic Style, Character Design)
- Capture all video player interactions (play, pause, seek, etc.)
- Replay entire sessions with perfect audio-video-annotation synchronization
- View selected animation categories during replay
- Save and load feedback sessions as JSON files

## Installation & Setup

1. **Clone the repository**
   ```
   git clone https://github.com/shanedjones/cartoon-annotation-tool
   cd cartoon-annotation-tool
   ```

2. **Install dependencies**
   ```
   npm install
   ```

3. **Run the development server**
   ```
   npm run dev
   ```

4. **Build for production**
   ```
   npm run build
   npm start
   ```

## Key Components

### FeedbackOrchestrator
- Core orchestration component that coordinates all events
- Manages recording and replay synchronization
- Uses an audio-based timeline as the primary synchronization mechanism
- Handles all events based on relative time offsets (video, annotations, categories)
- Processes category events and provides them during replay
- Doesn't render any UI itself (headless component)

### VideoPlayerWrapper
- Container component that integrates the orchestrator with UI components
- Manages UI state for recording/replaying
- Handles file saving and loading, including category data
- Provides reset behavior for recording and replay
- Provides global access for category change recording
- Serves as the main entry point for the application

### Animation Categories
- Allows tagging of animation with predefined categories
- Categories can be selected/deselected during recording
- Categories are stored as both timeline events and session metadata
- During replay, all selected categories are shown in a list view

### VideoPlayer
- Customized video player with annotation capabilities
- Provides controls for play, pause, seek, volume, etc.
- Exposes imperative methods for controlling playback
- Forwards references to the annotation canvas

### AnnotationCanvas
- Canvas-based drawing component overlaid on the video
- Supports real-time drawing with color and width controls
- Handles both user-created and replayed annotations
- Carefully synchronizes with video timeline

### AudioRecorder
- Handles audio recording and playback
- Manages browser compatibility issues
- Provides error handling for permissions issues
- Serializes audio data for storage and replay

## Data Model

The application uses two main data structures:

1. **FeedbackSession**: Modern format used internally
   ```typescript
   interface FeedbackSession {
     id: string;
     videoId: string;
     startTime: number;
     endTime?: number;
     audioTrack: AudioTrack;
     events: TimelineEvent[];
     categories?: Record<string, boolean>; // Added to store selected categories
   }
   
   // Audio track containing all audio recording data
   interface AudioTrack {
     chunks: AudioChunk[];
     totalDuration: number;
   }
   
   // Timeline event - all synchronized to audio timeline
   interface TimelineEvent {
     id: string;
     type: 'video' | 'annotation' | 'marker' | 'category';
     timeOffset: number; // milliseconds from audio start
     duration?: number; // for events with duration
     payload: any; // specific data based on type
   }
   ```

2. **FeedbackData**: Legacy format maintained for backward compatibility
   ```typescript
   interface FeedbackData {
     sessionId: string;
     videoId: string;
     actions: RecordedAction[];
     startTime: number;
     endTime?: number;
     annotations?: DrawingPath[];
     audioChunks?: AudioChunk[];
   }
   ```

3. **Event Payloads**: Different event types have specific payload structures:
   ```typescript
   // Video event payload
   interface VideoEventPayload {
     action: 'play' | 'pause' | 'seek' | 'volume' | 'playbackRate';
     to?: number; // For seek, volume, and playbackRate events
   }
   
   // Annotation event payload
   interface AnnotationEventPayload {
     action: 'draw' | 'clear';
     path?: DrawingPath; // For draw events
   }
   
   // Category event payload
   interface CategoryEventPayload {
     category: string; // The category name (e.g., "artisticStyle")
     checked: boolean; // Whether the category was checked or unchecked
   }
   
   // Marker event payload
   interface MarkerEventPayload {
     text: string; // The marker text
   }
   ```

## Key Features

### Recording Sessions
1. Audio is recorded using the MediaRecorder API
2. Video interactions (play, pause, seek) are captured as events
3. Drawing annotations are captured with timestamps
4. Animation category selections are recorded in real-time
5. All events are synchronized to a common timeline

### Replaying Sessions
1. Audio playback drives the main timeline
2. Video events are replayed at their recorded times
3. Annotations appear at their recorded timestamps
4. Animation categories selected during recording are displayed
5. All components reset properly when replay completes

### Animation Categories
1. Ten predefined categories for animation analysis:
   - Artistic Style
   - Character Design
   - Background Settings
   - Motion Dynamics
   - Color Palette
   - Sound Effects
   - Visual Effects
   - Narrative Techniques
   - Perspective View
   - Lighting & Shadows
2. Categories can be toggled on/off during recording
3. Selected categories are visible during replay
4. Categories are included in saved session data

### Serialization
- Sessions can be saved as JSON files
- Audio data is serialized as base64 strings
- Files can be reloaded for later replay

## Project Structure

```
cartoon-annotation/
├── app/                  # Next.js app directory
│   ├── page.tsx          # Main application page
│   └── layout.tsx        # App layout
├── src/
│   └── components/
│       ├── FeedbackOrchestrator.tsx   # Main coordination component
│       ├── VideoPlayerWrapper.tsx     # Container component
│       ├── VideoPlayer.tsx            # Custom video player
│       ├── AnnotationCanvas.tsx       # Drawing component
│       └── AudioRecorder.tsx          # Audio recording/playback
├── public/               # Static assets
└── package.json          # Dependencies and scripts
```

## Technical Details

- **Built with**: Next.js, React, TypeScript
- **Audio**: Uses MediaRecorder API with format detection
- **Drawing**: HTML5 Canvas for vector drawing
- **State Management**: React's Context and Refs for cross-component communication
- **Styling**: Tailwind CSS for responsive design

## Browser Compatibility

- Chrome (Desktop/Mobile): Full support
- Firefox (Desktop): Full support
- Safari (Desktop/iOS): Partial support - may require user interaction for audio playback
- Edge: Full support

## Potential Future Enhancements

- Visual timeline editor for post-recording edits
- Improved marker/comment system
- Video source selection
- Multiple annotation layers
- Custom animation categories
- Categorization analytics and reporting
- Category-based filtering during replay
- Export to video format
- Shared/collaborative sessions

## Known Issues

- Safari may require explicit user interaction before audio playback
- Large recordings with many annotations may experience performance issues
- Some mobile browsers have limited MediaRecorder support
|| END ||


|| START ./.gitignore ||

# See https://help.github.com/articles/ignoring-files/ for more about ignoring files.

# dependencies
/node_modules
/.pnp
.pnp.*
.yarn/*
!.yarn/patches
!.yarn/plugins
!.yarn/releases
!.yarn/versions

# testing
/coverage

# next.js
/.next/
/out/

# production
/build

# misc
.DS_Store
*.pem

# debug
npm-debug.log*
yarn-debug.log*
yarn-error.log*
.pnpm-debug.log*

# env files (can opt-in for committing if needed)
.env*

# vercel
.vercel

# typescript
*.tsbuildinfo
next-env.d.ts

|| END ||


|| START ./annotation-analysis.md ||

# Annotation System Analysis

## Overview of the Playback Data Structure

The annotation system consists of several interconnected components with a complex data flow for both recording and replaying annotations. Understanding the data flow and state management is crucial to diagnosing the issues with annotation clearing.

### Key Components and Their Roles

1. **AnnotationCanvas**: Handles drawing, storing, and displaying annotations
   - Manages the canvas context and actual rendering
   - Stores annotations in `allDrawings` state
   - Implements visibility system with `visible` and `hiddenAt` flags
   - Processes clear events during replay via `clearEvents` state

2. **VideoPlayer**: Controls video playback and annotation creation
   - Contains the `AnnotationCanvas` component
   - Tracks recording state and handles user interactions
   - Dispatches annotation events to parent components
   - Creates clear events with timestamp information

3. **VideoPlayerWrapper**: Orchestrates recording and replay sessions
   - Manages mode state ('record' vs 'replay')
   - Passes annotation data between components
   - Stores and retrieves session data
   - Forwards events to the `FeedbackOrchestrator`

4. **FeedbackOrchestrator**: Coordinates the recording and replay timeline
   - Records all events with precise timestamps
   - Processes the event timeline during replay
   - Executes annotation and clear events at the appropriate times
   - Manages audio synchronization and session state

### Data Structures

The annotation system uses these primary data structures:

1. **DrawingPath**: Represents a single annotation
   ```typescript
   interface DrawingPath {
     points: Point[];       // The actual drawing points
     color: string;         // Drawing color
     width: number;         // Line width
     timestamp: number;     // When the annotation was created
     videoTime?: number;    // Video position when created (ms)
     id?: string;           // Unique identifier 
     visible?: boolean;     // Visibility flag
     hiddenAt?: number;     // When annotation was cleared (if ever)
   }
   ```

2. **TimelineEvent**: Represents an action in the timeline
   ```typescript
   interface TimelineEvent {
     id: string;
     type: 'video' | 'annotation' | 'marker' | 'category';
     timeOffset: number;    // Milliseconds from session start
     duration?: number;     // For events with duration
     payload: any;          // Type-specific data
   }
   ```

3. **FeedbackSession**: The complete session data
   ```typescript
   interface FeedbackSession {
     id: string;
     videoId: string;
     startTime: number;
     endTime?: number;
     audioTrack: AudioTrack;
     events: TimelineEvent[];
     categories?: Record<string, boolean>;
   }
   ```

## Identified Issues

After analyzing the code, I've identified several issues that likely contribute to the annotation playback problems:

### 1. Inconsistent Visibility State Management

The `AnnotationCanvas` component has a visibility system for annotations with `visible` flag and `hiddenAt` timestamp, but there are issues in how this state is managed:

```typescript
// In AnnotationCanvas.tsx
// During replay, annotations are filtered like this:
const visibleAnnotations = replayAnnotations.filter(annotation => {
  // First check for explicit visibility flag if available
  if (annotation.visible === false) return false;
  
  // If the annotation has a hiddenAt timestamp and it's before current time, hide it
  if (annotation.hiddenAt && annotation.hiddenAt <= videoTimeMs) return false;
  
  // Get annotation timestamp (using multiple possible properties)
  const annotationTime = (annotation as any).timeOffset || annotation.videoTime || annotation.timestamp;
  
  // Only show annotations that happened after the most recent clear event
  const isAfterClear = annotationTime > mostRecentClearTime;
  const isBeforeCurrentTime = annotationTime <= videoTimeMs;
  
  return isAfterClear && isBeforeCurrentTime;
});
```

The issue is that the `clearCanvasDrawings` function has different behavior in recording vs. replay mode:

```typescript
// In AnnotationCanvas.tsx
const clearCanvasDrawings = () => {
  // Visual clearing happens here
  const ctx = getContext();
  if (ctx) {
    ctx.clearRect(0, 0, width, height);
  }
  
  // Mark all existing drawings as hidden with current timestamp
  const currentVideoTime = currentTime * 1000;
  const now = Date.now();
  
  if (isReplaying) {
    // During replay, mark drawings as hidden but keep them in the array
    setAllDrawings(prevDrawings => 
      prevDrawings.map(drawing => ({
        ...drawing,
        visible: false,
        hiddenAt: currentVideoTime || now
      }))
    );
  } else {
    // In recording mode, record hidden state but actually clear the array
    const hiddenDrawings = allDrawings.map(drawing => ({
      ...drawing,
      visible: false,
      hiddenAt: currentVideoTime || now
    }));
    console.log(`Marked ${hiddenDrawings.length} drawings as hidden at ${currentVideoTime}ms`);
    
    // Then clear the visible drawings
    setAllDrawings([]);
  }
  
  return {
    clearTime: currentVideoTime || now,
    clearedCount: allDrawings.length
  };
};
```

The problem is that when recording, it empties the `allDrawings` array, but during replay, the hidden drawings remain in the array. This makes the clear behavior inconsistent between modes.

### 2. Clear Event Timing Information Loss

Clear events need precise timing information to work correctly during replay, but there are inconsistencies in how this timing is captured and propagated:

```typescript
// In VideoPlayer.tsx
const clearAnnotations = () => {
  setShouldClearCanvas(true);
  
  // Record the clear action if recording
  if (isRecording && recordingStartTimeRef.current && onRecordAction) {
    // Create a special clear action with current time information
    const clearTimestamp = Date.now() - recordingStartTimeRef.current;
    const clearTime = currentTime * 1000; // Convert to ms
    
    console.log(`Creating clear event at video time: ${clearTime}ms, timestamp: ${clearTimestamp}ms`);
    
    const action: RecordedAction = {
      type: 'annotation',
      timestamp: clearTimestamp,
      videoTime: currentTime,
      details: { 
        clear: true,
        clearTimestamp: clearTimestamp,
        clearVideoTime: clearTime
      }
    };
    onRecordAction(action);
  }
};
```

While the `clearVideoTime` and `clearTimestamp` are captured, they may not be correctly used during replay, or there may be inconsistency in the time units (seconds vs. milliseconds).

### 3. Disconnected State Between Components

The annotation state is managed in multiple places, which can lead to inconsistencies:

1. `AnnotationCanvas` has its own `allDrawings` state
2. `VideoPlayerWrapper` passes `replayAnnotations` to the `VideoPlayer`
3. `FeedbackOrchestrator` processes events and calls `drawAnnotation` and `clearAnnotations`

If these components get out of sync in terms of which annotations should be visible, it can cause annotations to appear incorrectly during replay.

### 4. Detection and Processing of Clear Events

In the `AnnotationCanvas`, clear events are detected and tracked separately:

```typescript
// Process replay annotations to track clear events
useEffect(() => {
  if (!isReplaying || replayAnnotations.length === 0) return;
  
  // Find all clear events by their timestamps or isClearEvent flag
  const clearTimestamps: number[] = [];
  
  // First look for any clear actions that might be in the annotations array
  replayAnnotations.forEach((annotation, index) => {
    // Various formats to detect clear events
    const isClearEvent = (annotation as any).isClearEvent === true;
    const clearDetails = (annotation as any).details?.clear === true;
    
    if (isClearEvent || clearDetails) {
      // Try to extract timestamp from various properties
      let timeStamp = 0;
      
      // Use clearVideoTime from details if available (most accurate)
      if ((annotation as any).details?.clearVideoTime) {
        timeStamp = (annotation as any).details.clearVideoTime;
      }
      // Other timestamp extraction logic...
      
      if (timeStamp) {
        clearTimestamps.push(timeStamp);
      }
    }
  });
  
  // Sort by time
  clearTimestamps.sort((a, b) => a - b);
  
  // Update state only if there's a change
  if (clearTimestamps.length !== clearEvents.length || 
      clearTimestamps.some((t, i) => t !== clearEvents[i])) {
    setClearEvents(clearTimestamps);
  }
}, [isReplaying, replayAnnotations, clearEvents]);
```

But there could be issues with the annotations array not properly containing clear events, or the events having inconsistent formats.

## Root Cause Analysis

After analyzing the code and observed behavior, here are the likely root causes of the annotation clearing issues:

1. **Clear Event Loss During Serialization**: When recording sessions are saved and loaded, the visibility flags may not be properly serialized, causing clear events to be ineffective during replay.

2. **State Duplication with Inconsistent Updates**: The annotation state exists in multiple components (AnnotationCanvas, VideoPlayerWrapper, FeedbackOrchestrator), and they may get out of sync during complex operations like clearing.

3. **Timing Inconsistencies**: Clear events need precise timing to work correctly, but there are multiple timestamp formats (video time, recording offset time, wall clock time) that may be inconsistently used.

4. **Ineffective Visibility State Management**: While there's a mechanism for marking annotations as hidden, the code may not be consistently using these flags during replay filtering.

5. **Modifying the Original Annotation Arrays**: The code sometimes modifies the original annotation arrays, which can cause unintended side effects when those arrays are used in multiple places.

## Proposed Solutions

### Approach 1: Enhance Visibility State Management

This approach focuses on improving the visibility state tracking and usage:

1. **Consistent Handling of Visibility State**:
   - Ensure all annotations have `visible` and `hiddenAt` properties set correctly
   - Make the `clearCanvasDrawings` function consistent between recording and replay modes
   - During replay, prioritize visibility state over clear event timestamps

```typescript
// Example implementation for AnnotationCanvas:
const clearCanvasDrawings = () => {
  const ctx = getContext();
  if (ctx) {
    ctx.clearRect(0, 0, width, height);
  }
  
  const currentVideoTime = currentTime * 1000;
  const now = Date.now();
  
  // Create a clear marker with precise timing
  const clearMark = {
    timestamp: now,
    videoTime: currentVideoTime, 
    id: `clear-${now}`,
    isClearEvent: true
  };
  
  // Make a copy of current drawings with visibility set to false
  const hiddenDrawings = allDrawings.map(drawing => ({
    ...drawing,
    visible: false,
    hiddenAt: currentVideoTime || now
  }));
  
  // For recording, reset the array but keep track of what was hidden
  if (!isReplaying) {
    // Store hidden state in window.__hiddenAnnotations for replay
    if (typeof window !== 'undefined') {
      window.__hiddenAnnotations = window.__hiddenAnnotations || [];
      window.__hiddenAnnotations.push(...hiddenDrawings);
    }
    setAllDrawings([]);
  } else {
    // During replay, just update visibility
    setAllDrawings(hiddenDrawings);
  }
  
  return clearMark;
};
```

2. **Improved Replay Filtering**:
   - Add additional checks for the visibility state
   - Ensure clear events are properly tracked and have priority in determining visibility

```typescript
// Example improved filtering in AnnotationCanvas:
const visibleAnnotations = replayAnnotations.filter(annotation => {
  // Skip any special markers
  if ((annotation as any).isClearEvent) return false;
  
  // Priority 1: Check explicit visibility flag
  if (annotation.visible === false) return false;
  
  // Priority 2: Check if this was hidden at a past time
  if (annotation.hiddenAt && annotation.hiddenAt <= videoTimeMs) return false;
  
  // Priority 3: Check against clear events
  const annotationTime = annotation.videoTime || annotation.timestamp;
  if (mostRecentClearTime > 0 && annotationTime <= mostRecentClearTime) return false;
  
  // Finally, ensure the annotation should be visible at current time
  return annotationTime <= videoTimeMs;
});
```

### Approach 2: Centralize Annotation State Management

This approach focuses on centralizing the annotation state to avoid inconsistencies:

1. **Single Source of Truth**:
   - Move all annotation state management to the `FeedbackOrchestrator`
   - Provide read-only views of the annotation state to other components
   - Make clearing an explicit operation that updates this central state

2. **Enhanced Clear Event**:
   - Create a more robust clear event type with precise timing info
   - Ensure clear events are properly serialized and deserialized

```typescript
// Example clear event in the FeedbackOrchestrator:
interface ClearEvent {
  id: string;
  type: 'clear';
  timeOffset: number;     // Time from session start
  videoTime: number;      // Video position when cleared
  wallClockTime: number;  // Actual time when cleared
}

// During recording:
const recordClearEvent = () => {
  const now = Date.now();
  const timeOffset = now - recordingStartTimeRef.current;
  const videoTime = videoElementRef.current ? videoElementRef.current.currentTime * 1000 : 0;
  
  const clearEvent: ClearEvent = {
    id: generateId(),
    type: 'clear',
    timeOffset,
    videoTime,
    wallClockTime: now
  };
  
  // Add to events list
  eventsRef.current.push({
    id: clearEvent.id,
    type: 'annotation',
    timeOffset,
    payload: { action: 'clear', ...clearEvent }
  });
  
  // Apply to annotations state
  annotationsRef.current = annotationsRef.current.map(a => ({
    ...a,
    visible: false,
    hiddenAt: videoTime
  }));
  
  // Actually clear visible annotations from canvas
  drawingCanvasRef.current.clearVisual();
  
  return clearEvent;
};
```

## Recommended Solution

I recommend implementing a hybrid of both approaches:

1. **Enhance Visibility State**: Make the visibility properties (`visible` and `hiddenAt`) a central part of the annotation system, ensuring they're properly serialized, tracked, and respected during replay.

2. **Improve Clear Event Processing**: Ensure clear events have precise timing information and properly mark all annotations as hidden rather than removing them from arrays.

3. **Consistent State Handling**: Establish clear responsibilities for each component regarding annotation state, and ensure they work consistently in both recording and replay modes.

4. **Advanced Debugging Tools**: Add more comprehensive logging for the annotation lifecycle, especially during replay, to identify exactly when and why annotations may be incorrectly shown or hidden.

The root issue is likely a combination of inconsistent state management and timing problems. By addressing both aspects, the annotation clearing functionality should work correctly during both recording and replay.
|| END ||


|| START ./next.config.ts ||

import type { NextConfig } from "next";

const nextConfig: NextConfig = {
  /* config options here */
};

export default nextConfig;

|| END ||


|| START ./src/components/VideoPlayerWrapper.tsx ||

'use client';

import { useState, useCallback, useRef, useEffect } from 'react';
import dynamic from 'next/dynamic';
import { v4 as uuidv4 } from 'uuid';
import type { RecordedAction, FeedbackData } from './VideoPlayer';
import type { DrawingPath } from './AnnotationCanvas';
import AudioRecorder from './AudioRecorder';
import FeedbackOrchestrator, { FeedbackSession, AudioTrack, TimelineEvent } from './FeedbackOrchestrator';

// Import the AudioChunk type from the AudioRecorder component
import type { AudioChunk } from './AudioRecorder';

// Dynamically import the VideoPlayer with no SSR
const VideoPlayer = dynamic(() => import('./VideoPlayer'), { ssr: false });

// Helper function to convert Blob to base64 for storage
const blobToBase64 = (blob: Blob): Promise<string> => {
  return new Promise((resolve, reject) => {
    const reader = new FileReader();
    reader.onload = () => {
      const dataUrl = reader.result as string;
      resolve(dataUrl);
    };
    reader.onerror = reject;
    reader.readAsDataURL(blob);
  });
};

// Format category name from camelCase to readable format
const getCategoryLabel = (category: string): string => {
  return category.replace(/([A-Z])/g, ' $1').trim().replace(/^./, str => str.toUpperCase());
};

// Enhanced helper function to convert base64 back to Blob for playback
const base64ToBlob = (base64: string, mimeType: string): Blob => {
  try {
    // Validate input parameters
    if (!base64 || typeof base64 !== 'string') {
      console.error('Invalid base64 input: not a string or empty', typeof base64);
      throw new Error('Invalid base64 input: not a string or empty');
    }
    
    if (!mimeType || typeof mimeType !== 'string') {
      console.warn('Invalid or missing MIME type, using default: audio/webm');
      mimeType = 'audio/webm'; // Fallback to default
    }
    
    // First ensure we have a proper data URL with the correct format
    if (!base64.startsWith('data:')) {
      console.error('Invalid base64 string format - missing data: prefix');
      console.debug('String starts with:', base64.substring(0, Math.min(20, base64.length)));
      throw new Error('Invalid base64 string format - missing data: prefix');
    }
    
    if (!base64.includes(',')) {
      console.error('Invalid base64 string format - missing comma separator');
      throw new Error('Invalid base64 string format - missing comma separator');
    }
    
    // Extract the base64 part after the comma
    const base64Data = base64.split(',')[1];
    if (!base64Data) {
      console.error('Invalid base64 string - no data after comma');
      throw new Error('Invalid base64 string - no data after comma');
    }
    
    // Get actual MIME type from the data URL if present
    const headerPart = base64.split(',')[0];
    const mimeMatch = headerPart.match(/^data:(.*?)(;base64)?$/);
    if (mimeMatch && mimeMatch[1]) {
      // If the data URL contains a MIME type, use it instead of the provided mimeType
      console.log(`Using MIME type from data URL (${mimeMatch[1]}) instead of provided type (${mimeType})`);
      mimeType = mimeMatch[1];
    }
    
    try {
      // Decode the base64 string to binary with error handling
      const byteString = atob(base64Data);
      
      // Create an ArrayBuffer to hold the decoded data
      const ab = new ArrayBuffer(byteString.length);
      const ia = new Uint8Array(ab);
      
      // Copy the decoded binary data to the array buffer
      for (let i = 0; i < byteString.length; i++) {
        ia[i] = byteString.charCodeAt(i);
      }
      
      // Create and return a new Blob from the array buffer
      const blob = new Blob([ab], { type: mimeType });
      
      // Validate created blob
      if (blob.size === 0) {
        console.warn('Created an empty blob from base64 data, possible data corruption');
      } else {
        console.log(`Successfully converted base64 to Blob: size=${blob.size}, type=${blob.type}`);
      }
      
      return blob;
    } catch (binaryError) {
      console.error('Error processing binary data:', binaryError);
      throw new Error(`Failed to process binary data: ${binaryError instanceof Error ? binaryError.message : String(binaryError)}`);
    }
  } catch (error) {
    console.error('Error converting base64 to Blob:', error);
    throw error;
  }
};

// Helper function to prepare audio chunks for saving to JSON
const prepareAudioChunksForSave = async (chunks: AudioChunk[]): Promise<any[]> => {
  if (!chunks || chunks.length === 0) {
    console.log('No audio chunks to prepare for save');
    return [];
  }
  
  console.log(`Preparing ${chunks.length} audio chunks for save...`);
  
  // Create a deep copy of the chunks
  return Promise.all(chunks.map(async (chunk, index) => {
    try {
      console.log(`Processing chunk ${index} for save, blob type:`, 
        chunk.blob instanceof Blob ? 'Blob object' : typeof chunk.blob);
      
      // Only convert if it's a Blob and not already a string
      if (chunk.blob instanceof Blob) {
        console.log(`Chunk ${index}: Converting Blob to base64, size: ${chunk.blob.size}, type: ${chunk.blob.type}`);
        
        // Convert Blob to base64 string for storage
        const base64 = await blobToBase64(chunk.blob);
        
        // Log length of base64 string for debugging
        console.log(`Chunk ${index}: Base64 conversion complete, string length: ${base64.length}`);
        
        // Save with MIME type and other properties
        return {
          ...chunk,
          blob: base64, // Replace Blob with base64 string
          mimeType: chunk.mimeType || chunk.blob.type, // Ensure we save the mime type
          url: undefined // Remove URL property if it exists
        };
      } else if (typeof chunk.blob === 'string' && chunk.blob.startsWith('data:')) {
        // Already a data URL, verify it's properly formatted
        console.log(`Chunk ${index}: Already a data URL, length: ${chunk.blob.length}`);
        
        // Verify data URL format
        const parts = chunk.blob.split(',');
        if (parts.length !== 2) {
          console.warn(`Chunk ${index}: Invalid data URL format - wrong number of parts`);
        }
        
        // Return as is, but ensure all properties are set
        return {
          ...chunk,
          mimeType: chunk.mimeType || 'audio/webm', // Ensure MIME type is set
          url: undefined // Remove URL property if it exists
        };
      } else {
        console.warn(`Chunk ${index}: Unknown blob format: ${typeof chunk.blob}`);
        
        // Return with minimal valid properties
        return {
          ...chunk,
          blob: typeof chunk.blob === 'string' ? chunk.blob : '', // Keep string or use empty string
          mimeType: chunk.mimeType || 'audio/webm', // Ensure MIME type is set
          url: undefined // Remove URL property if it exists
        };
      }
    } catch (error) {
      console.error(`Error converting audio chunk ${index} for storage:`, error);
      return null;
    }
  })).then(results => {
    const validResults = results.filter(Boolean); // Remove any failed conversions
    console.log(`Successfully prepared ${validResults.length} of ${chunks.length} audio chunks for save`);
    return validResults;
  });
};

// Helper function to restore audio chunks when loading saved data
const restoreAudioChunks = (savedChunks: any[]): AudioChunk[] => {
  if (!savedChunks || savedChunks.length === 0) {
    console.log('No audio chunks to restore');
    return [];
  }
  
  console.log(`Restoring ${savedChunks.length} audio chunks...`);
  
  return savedChunks.map((savedChunk, index) => {
    try {
      // If blob is already a Blob object, just return the chunk as is
      if (savedChunk.blob instanceof Blob) {
        console.log(`Chunk ${index}: Already a Blob object`);
        return savedChunk;
      }
      
      // If blob is a string (data URL), validate and keep as a string for compatibility
      if (typeof savedChunk.blob === 'string') {
        if (savedChunk.blob.startsWith('data:')) {
          console.log(`Chunk ${index}: Found data URL, keeping as string for AudioRecorder component`);
          
          // Try to validate the data URL format
          try {
            const dataUrlParts = savedChunk.blob.split(',');
            if (dataUrlParts.length !== 2) {
              console.warn(`Chunk ${index}: Invalid data URL format - wrong number of parts`);
            }
            // Check if the mime type part is valid
            const mimeMatch = dataUrlParts[0].match(/:(.*?);/);
            if (!mimeMatch) {
              console.warn(`Chunk ${index}: Data URL has no valid MIME type`);
            }
          } catch (validationError) {
            console.warn(`Chunk ${index}: Error validating data URL:`, validationError);
          }
          
          // Ensure all required properties are present
          return {
            ...savedChunk,
            blob: savedChunk.blob, // Keep the data URL as is
            mimeType: savedChunk.mimeType || 'audio/webm', // Set default MIME type if missing
            startTime: savedChunk.startTime || 0,
            duration: savedChunk.duration || 0,
            videoTime: savedChunk.videoTime || 0
          };
        } else {
          console.warn(`Chunk ${index}: String blob doesn't start with 'data:' prefix: ${savedChunk.blob.substring(0, 20)}...`);
        }
      }
      
      console.warn(`Unknown blob format in chunk ${index}:`, typeof savedChunk.blob);
      // Log more details to aid debugging
      if (typeof savedChunk.blob === 'string') {
        console.info(`Chunk ${index} string length: ${savedChunk.blob.length}, starts with: ${savedChunk.blob.substring(0, 30)}...`);
      } else if (savedChunk.blob === null) {
        console.warn(`Chunk ${index}: Blob is null`);
      } else if (savedChunk.blob === undefined) {
        console.warn(`Chunk ${index}: Blob is undefined`);
      }
      
      // Return a simplified chunk as a fallback (audio won't play but won't crash either)
      return {
        ...savedChunk,
        blob: savedChunk.blob || '', // Keep as is even if invalid
        mimeType: savedChunk.mimeType || 'audio/webm',
        startTime: savedChunk.startTime || 0,
        duration: savedChunk.duration || 0,
        videoTime: savedChunk.videoTime || 0
      };
    } catch (error) {
      console.error(`Error restoring audio chunk ${index}:`, error);
      return null;
    }
  }).filter(Boolean as any); // Remove any failed conversions
};

// Convert the legacy FeedbackData to the new FeedbackSession format
const convertLegacyDataToSession = (legacyData: FeedbackData): FeedbackSession => {
  // Create a new FeedbackSession from the legacy data
  const audioTrack: AudioTrack = {
    chunks: legacyData.audioChunks || [],
    totalDuration: legacyData.audioChunks?.reduce((total, chunk) => total + chunk.duration, 0) || 0
  };
  
  // Convert actions to timeline events
  const events: TimelineEvent[] = legacyData.actions.map(action => {
    return {
      id: uuidv4(),
      type: action.type === 'annotation' ? 'annotation' : 'video',
      timeOffset: action.timestamp,
      duration: action.type === 'audio' ? action.details?.duration : undefined,
      payload: 
        action.type === 'annotation' 
          ? { action: action.details?.clear ? 'clear' : 'draw', path: action.details?.path } 
          : { action: action.type, ...action.details }
    };
  });
  
  return {
    id: legacyData.sessionId || uuidv4(),
    videoId: legacyData.videoId,
    startTime: legacyData.startTime,
    endTime: legacyData.endTime,
    audioTrack,
    events
  };
};

// Convert FeedbackSession to legacy FeedbackData format for compatibility
const convertSessionToLegacyData = (session: FeedbackSession): FeedbackData => {
  // Create a new FeedbackData object
  const legacyData: FeedbackData = {
    sessionId: session.id,
    videoId: session.videoId,
    startTime: session.startTime,
    endTime: session.endTime,
    actions: [],
    audioChunks: session.audioTrack.chunks,
    annotations: []
  };
  
  // Collect all annotation paths
  const annotations: DrawingPath[] = [];
  
  // Convert timeline events to legacy actions
  session.events.forEach(event => {
    if (event.type === 'video') {
      const action: RecordedAction = {
        type: event.payload.action,
        timestamp: event.timeOffset,
        videoTime: 0, // Will need proper conversion
        details: { ...event.payload }
      };
      delete action.details.action;
      legacyData.actions.push(action);
    } 
    else if (event.type === 'annotation') {
      const action: RecordedAction = {
        type: 'annotation',
        timestamp: event.timeOffset,
        videoTime: 0, // Will need proper conversion
        details: event.payload.action === 'clear' 
          ? { clear: true } 
          : { path: event.payload.path }
      };
      legacyData.actions.push(action);
      
      // Also collect annotation paths for backwards compatibility
      if (event.payload.action === 'draw' && event.payload.path) {
        annotations.push(event.payload.path);
      }
    }
    else if (event.type === 'marker') {
      // Skip markers, as they don't have a direct equivalent in the legacy format
      console.log('Skipping marker event in legacy conversion:', event.payload.text);
    }
  });
  
  // Add annotations
  legacyData.annotations = annotations;
  
  return legacyData;
};

interface VideoPlayerWrapperProps {
  categories?: Record<string, boolean>;
  onCategoriesCleared?: () => void;
  onCategoriesLoaded?: (categories: Record<string, boolean>) => void;
  onReplayModeChange?: (isReplay: boolean) => void;
  videoUrl?: string;
  videoId?: string;
  contentToReview?: any; // Allow passing the full content object for display
}

export default function VideoPlayerWrapper({ 
  categories = {}, 
  onCategoriesCleared,
  onCategoriesLoaded,
  onReplayModeChange,
  videoUrl,
  videoId = 'sample-video',
  contentToReview
}: VideoPlayerWrapperProps) {
  // Log categories passed from parent on every render
  console.log('VideoPlayerWrapper received categories:', categories);
  const [mode, setMode] = useState<'record' | 'replay'>('record');
  const [isActive, setIsActive] = useState(false);
  const [currentSession, setCurrentSession] = useState<FeedbackSession | null>(null);
  const [feedbackData, setFeedbackData] = useState<FeedbackData>({
    sessionId: '',
    videoId: videoId,
    actions: [],
    startTime: 0,
    annotations: [],
    audioChunks: [],
  });
  
  // References
  const videoRef = useRef<HTMLVideoElement | null>(null);
  const canvasRef = useRef<HTMLCanvasElement | null>(null);
  const orchestratorRef = useRef<any>(null);
  const annotationCanvasComponentRef = useRef<any>(null);
  
  // Function to set the video reference from the child component
  const setVideoElementRef = useCallback((el: HTMLVideoElement | null) => {
    videoRef.current = el;
  }, []);
  
  // Start recording
  const startRecording = useCallback(() => {
    setMode('record');
    
    // Clear any existing annotations before starting
    if (annotationCanvasComponentRef.current) {
      console.log('Clearing annotations before starting new recording');
      if (annotationCanvasComponentRef.current.clearCanvasDrawings) {
        annotationCanvasComponentRef.current.clearCanvasDrawings();
      }
    }
    
    // Reset video to beginning if needed
    if (videoRef.current) {
      console.log('Resetting video position before starting recording');
      videoRef.current.currentTime = 0;
    }
    
    if (orchestratorRef.current) {
      orchestratorRef.current.startRecordingSession();
      setIsActive(true);
    }
  }, []);
  
  // Stop recording
  const stopRecording = useCallback(() => {
    if (orchestratorRef.current) {
      console.log('Stopping recording session');
      
      // End recording session
      orchestratorRef.current.endRecordingSession();
      setIsActive(false);
      
      // Reset video to beginning
      if (videoRef.current) {
        console.log('Resetting video position to start');
        videoRef.current.currentTime = 0;
        
        // If it's playing, pause it
        if (!videoRef.current.paused) {
          videoRef.current.pause();
        }
      }
      
      // Clear annotations
      if (annotationCanvasComponentRef.current) {
        console.log('Clearing annotations after recording stopped');
        if (annotationCanvasComponentRef.current.clearCanvasDrawings) {
          annotationCanvasComponentRef.current.clearCanvasDrawings();
        }
      }
      
      // Call the onCategoriesCleared callback if it exists
      if (onCategoriesCleared) {
        onCategoriesCleared();
      }
      
      // Make sure session availability is updated immediately
      if (typeof window !== 'undefined') {
        window.__hasRecordedSession = true;
        console.log('Session available flag set to true after stopping recording');
        
        // Dispatch a custom event to notify about session availability
        window.dispatchEvent(new Event('session-available'));
      }
    }
  }, [onCategoriesCleared, currentSession]);
  
  // Start replaying the recorded session
  const startReplay = useCallback(() => {
    setMode('replay');
    
    // Clear any existing annotations before starting replay
    if (annotationCanvasComponentRef.current) {
      console.log('Clearing annotations before starting replay');
      if (annotationCanvasComponentRef.current.clearCanvasDrawings) {
        annotationCanvasComponentRef.current.clearCanvasDrawings();
      }
    }
    
    // Reset video to beginning
    if (videoRef.current) {
      console.log('Resetting video position before starting replay');
      videoRef.current.currentTime = 0;
    }
    
    if (orchestratorRef.current && currentSession) {
      // Clear categories before replay starts
      if (onCategoriesCleared) {
        console.log('Clearing categories before replay');
        onCategoriesCleared();
      }
      
      // Log the categories of the session we're replaying
      console.log('Replaying session with categories:', currentSession.categories);
      
      orchestratorRef.current.loadSession(currentSession);
      orchestratorRef.current.startReplay();
      setIsActive(true);
    } else {
      alert('No recorded session to replay. Record a session first.');
    }
  }, [currentSession, onCategoriesCleared]);
  
  // Stop replay
  const stopReplay = useCallback(() => {
    if (orchestratorRef.current) {
      orchestratorRef.current.stopReplay();
      setIsActive(false);
      
      // Reset video to beginning
      if (videoRef.current) {
        console.log('Resetting video position to start after replay');
        videoRef.current.currentTime = 0;
        
        // If it's playing, pause it
        if (!videoRef.current.paused) {
          videoRef.current.pause();
        }
      }
      
      // Clear annotations
      if (annotationCanvasComponentRef.current) {
        console.log('Clearing annotations after replay stopped');
        if (annotationCanvasComponentRef.current.clearCanvasDrawings) {
          annotationCanvasComponentRef.current.clearCanvasDrawings();
        }
      }
    }
  }, []);
  
  // Handle session completion
  const handleSessionComplete = useCallback((session: FeedbackSession) => {
    // First create a deep copy of the session to avoid any referential issues
    const sessionCopy = JSON.parse(JSON.stringify(session));
    
    // Deep copy the categories to ensure they're not references
    const categoriesCopy = JSON.parse(JSON.stringify(categories));
    
    // Log the actual values we're trying to save
    console.log('Current categories to save:', categoriesCopy);
    
    // Log which categories are true (selected)
    const selectedCategories = Object.entries(categoriesCopy)
      .filter(([_, value]) => value)
      .map(([key]) => key);
    console.log('Selected categories:', selectedCategories);
    
    // Add the categories to the session
    const sessionWithCategories = {
      ...sessionCopy,
      categories: categoriesCopy
    };
    
    setCurrentSession(sessionWithCategories);
    
    // Also update legacy feedbackData for compatibility
    const legacyData = convertSessionToLegacyData(sessionWithCategories);
    setFeedbackData(legacyData);
    
    // Update session availability flag immediately
    if (typeof window !== 'undefined') {
      window.__hasRecordedSession = true;
      console.log('Session available flag set to true');
      
      // Dispatch a custom event to notify about session availability
      window.dispatchEvent(new Event('session-available'));
    }
    
    console.log('Session completed with categories:', sessionWithCategories);
  }, [categories]);
  
  // Handle audio recording completed
  const handleAudioRecorded = useCallback((audioTrack: AudioTrack) => {
    console.log('Audio recorded:', audioTrack);
  }, []);
  
  // Draw annotation
  const drawAnnotation = useCallback((path: DrawingPath) => {
    // Only update the video time if it's not already set (for new annotations, not replayed ones)
    if (videoRef.current && !path.videoTime) {
      path.videoTime = videoRef.current.currentTime * 1000;
      
      console.log('Setting videoTime for new annotation:', path.videoTime);
    }
    
    // Pass annotation to the annotation canvas component via the VideoPlayer
    if (annotationCanvasComponentRef.current) {
      // Log the attempt to draw to aid debugging
      console.log('Drawing annotation via handleManualAnnotation:', {
        pathPoints: path.points?.length || 0,
        color: path.color,
        width: path.width,
        videoTime: path.videoTime,
        timeOffset: (path as any).timeOffset, // For debug only
        isReplay: !!path.videoTime // If videoTime is set, it's likely a replay
      });
      
      try {
        // Use the handleManualAnnotation method exposed by the AnnotationCanvas
        annotationCanvasComponentRef.current.handleManualAnnotation(path);
      } catch (error) {
        console.error('Error drawing annotation:', error);
      }
    } else {
      console.warn('Could not draw annotation: annotation canvas ref is not available');
    }
  }, []);
  
  // Clear annotations
  const clearAnnotations = useCallback(() => {
    if (annotationCanvasComponentRef.current) {
      console.log('Clearing annotations via clearCanvasDrawings');
      
      try {
        // Get precise timing information for the clear event
        const now = Date.now();
        const currentVideoTime = videoRef.current ? videoRef.current.currentTime * 1000 : now;
        
        // Use the clearCanvasDrawings method exposed by the AnnotationCanvas
        if (annotationCanvasComponentRef.current.clearCanvasDrawings) {
          // The clear method now returns a clear marker with timing info
          const clearResult = annotationCanvasComponentRef.current.clearCanvasDrawings();
          
          // Log that the clear was successful
          console.log('Successfully cleared annotations with result:', clearResult);
          
          // Return enhanced metadata for the clear event
          return {
            id: `clear-${now}`,
            clearVideoTime: currentVideoTime, // Video playback position in ms
            clearTimestamp: now, // Absolute timestamp of the clear action
            timeOffset: now - (mode === 'record' && orchestratorRef.current?.recordingStartTime || now),
            timestamp: now,
            vidTime: videoRef.current?.currentTime,
            result: clearResult
          };
        } else {
          console.warn('clearCanvasDrawings method not found on annotationCanvas');
        }
      } catch (error) {
        console.error('Error clearing annotations:', error);
      }
    } else {
      console.warn('Could not clear annotations: annotation canvas ref is not available');
    }
    
    return null;
  }, [mode, videoRef]);
  
  // Download session data as JSON
  const downloadSessionData = useCallback(async () => {
    if (!currentSession) {
      alert('No recorded session to download.');
      return;
    }
    
    try {
      // Create a deep copy to avoid modifying the original state
      const sessionCopy = JSON.parse(JSON.stringify(currentSession));
      
      // Ensure categories are included in the download
      console.log('Current session categories before download:', sessionCopy.categories);
      
      // Deep copy the categories to ensure they're not references
      const categoriesCopy = JSON.parse(JSON.stringify(categories));
      
      // Log which categories are currently selected
      const selectedCategories = Object.entries(categoriesCopy)
        .filter(([_, value]) => value)
        .map(([key]) => key);
      console.log('Selected categories for download:', selectedCategories);
      
      // Make sure we have the latest categories
      sessionCopy.categories = categoriesCopy;
      console.log('Updated session categories for download:', sessionCopy.categories);
      
      // Prepare audio chunks for serialization
      if (sessionCopy.audioTrack && sessionCopy.audioTrack.chunks.length > 0) {
        try {
          console.log(`Preparing ${sessionCopy.audioTrack.chunks.length} audio chunks for save...`);
          sessionCopy.audioTrack.chunks = await prepareAudioChunksForSave(sessionCopy.audioTrack.chunks);
          console.log('Audio chunks prepared successfully:', sessionCopy.audioTrack.chunks.length);
        } catch (error) {
          console.error('Failed to prepare audio chunks for saving:', error);
          alert('There was an issue preparing audio data for download. Some audio content may be missing.');
        }
      }
      
      const dataStr = JSON.stringify(sessionCopy, null, 2);
      const dataUri = 'data:application/json;charset=utf-8,'+ encodeURIComponent(dataStr);
      
      const linkElement = document.createElement('a');
      linkElement.setAttribute('href', dataUri);
      linkElement.setAttribute('download', `feedback-session-${currentSession.id}.json`);
      document.body.appendChild(linkElement);
      linkElement.click();
      document.body.removeChild(linkElement);
    } catch (error) {
      console.error('Error during download process:', error);
      alert('Failed to download session data. See console for details.');
    }
  }, [currentSession, categories]);
  
  // Handle file upload for previously saved session data
  const handleFileUpload = useCallback((e: React.ChangeEvent<HTMLInputElement>) => {
    if (!e.target.files || e.target.files.length === 0) return;
    
    const file = e.target.files[0];
    const reader = new FileReader();
    
    reader.onload = (event) => {
      try {
        // Parse the JSON data
        const jsonData = JSON.parse(event.target?.result as string);
        
        // Check if it's the new format or legacy format
        if (jsonData.events && jsonData.audioTrack) {
          // It's the new FeedbackSession format
          const loadedSession = jsonData as FeedbackSession;
          
          // Log if we have categories
          console.log('Loaded session categories:', loadedSession.categories);
          
          // Restore audio chunks with proper Blob objects if they exist
          if (loadedSession.audioTrack && loadedSession.audioTrack.chunks) {
            loadedSession.audioTrack.chunks = restoreAudioChunks(loadedSession.audioTrack.chunks);
          }
          
          setCurrentSession(loadedSession);
          // Also update legacy format for compatibility
          setFeedbackData(convertSessionToLegacyData(loadedSession));
          
          console.log('Loaded feedback session:', loadedSession);
        } else {
          // It's the legacy FeedbackData format
          const legacyData = jsonData as FeedbackData;
          
          // Restore audio chunks
          if (legacyData.audioChunks) {
            legacyData.audioChunks = restoreAudioChunks(legacyData.audioChunks);
          }
          
          setFeedbackData(legacyData);
          // Convert to new format
          const newSession = convertLegacyDataToSession(legacyData);
          setCurrentSession(newSession);
          
          console.log('Loaded legacy feedback data and converted to session:', legacyData);
        }
      } catch (error) {
        console.error("Failed to parse uploaded file:", error);
        alert("Invalid feedback data file. Please upload a valid JSON file.");
      }
    };
    
    reader.onerror = (error) => {
      console.error('Error reading file:', error);
      alert('Error reading the file. Please try again.');
    };
    
    reader.readAsText(file);
  }, []);
  
  // Get an orchestrator reference
  const getOrchestratorRef = useCallback((orchestratorInstance: any) => {
    orchestratorRef.current = orchestratorInstance;
  }, []);
  
  // Method to record a category change
  const recordCategoryChange = useCallback((category: string, checked: boolean) => {
    if (orchestratorRef.current && mode === 'record' && isActive) {
      console.log(`Recording category change in orchestrator: ${category} = ${checked}`);
      orchestratorRef.current.handleCategoryEvent(category, checked);
    } else {
      console.warn('Unable to record category change - not in recording mode or not active');
    }
  }, [mode, isActive]);
  
  // Get a reference to the annotation canvas via the video player
  const getVideoPlayerRef = useCallback((videoPlayerInstance: any) => {
    // Store the video player reference
    annotationCanvasComponentRef.current = videoPlayerInstance?.annotationCanvas;
    
    // Log the reference to ensure we have it
    console.log('Got video player ref with annotation canvas:', {
      videoPlayer: !!videoPlayerInstance,
      annotationCanvas: !!videoPlayerInstance?.annotationCanvas,
      canvasMethods: videoPlayerInstance?.annotationCanvas ? 
        Object.keys(videoPlayerInstance.annotationCanvas) : []
    });
  }, []);
  
  // Listen for replay progress to detect completion
  useEffect(() => {
    if (orchestratorRef.current && mode === 'replay' && isActive) {
      // Check if orchestrator has a replayProgress property
      const progress = orchestratorRef.current.replayProgress;
      if (progress === 100) {
        // Replay has completed, reset the UI state
        console.log('Detected replay completion via progress = 100%, resetting UI state');
        setIsActive(false);
        
        // Reset video to beginning
        if (videoRef.current) {
          console.log('Auto-resetting video position to start after replay completion');
          videoRef.current.currentTime = 0;
          
          // If it's playing, pause it
          if (!videoRef.current.paused) {
            videoRef.current.pause();
          }
        }
        
        // Clear annotations
        if (annotationCanvasComponentRef.current) {
          console.log('Auto-clearing annotations after replay completion');
          if (annotationCanvasComponentRef.current.clearCanvasDrawings) {
            annotationCanvasComponentRef.current.clearCanvasDrawings();
          }
        }
      }
    }
  }, [mode, isActive, orchestratorRef.current?.replayProgress]);

  // Clean up resources when component unmounts
  useEffect(() => {
    return () => {
      if (orchestratorRef.current && isActive) {
        if (mode === 'record') {
          orchestratorRef.current.endRecordingSession();
        } else {
          orchestratorRef.current.stopReplay();
        }
      }
    };
  }, [isActive, mode]);

  // Define window type with our custom property
  declare global {
    interface Window {
      __videoPlayerWrapper?: {
        recordCategoryChange: (category: string, checked: boolean) => void;
        isRecording: boolean;
      };
    }
  }
  
  // Expose methods to the parent component and notify about mode changes
  useEffect(() => {
    // This runs once when the component mounts and when dependencies change
    if (typeof window !== 'undefined') {
      // Set global reference available to parent component
      window.__videoPlayerWrapper = {
        recordCategoryChange,
        isRecording: mode === 'record' && isActive
      };
      
      // Update session availability flag
      window.__hasRecordedSession = currentSession !== null;
    }
    
    // Notify parent component about replay mode changes
    if (onReplayModeChange) {
      const isReplay = mode === 'replay';
      console.log(`Notifying parent about replay mode: ${isReplay}`);
      onReplayModeChange(isReplay);
    }
    
    return () => {
      // Clean up on unmount
      if (typeof window !== 'undefined' && window.__videoPlayerWrapper) {
        delete window.__videoPlayerWrapper;
      }
    };
  }, [recordCategoryChange, mode, isActive, onReplayModeChange, currentSession]);
  
  return (
    <div className="w-full">
      {/* Hidden buttons that will be triggered by parent */}
      <div className="hidden">
        <button
          id="startRecordingButton"
          onClick={startRecording}
        >
          Start Recording
        </button>
        
        <button
          id="startReplayButton"
          onClick={startReplay}
          disabled={!currentSession}
        >
          Replay Session
        </button>
        
        <button
          id="downloadDataButton"
          onClick={downloadSessionData}
          disabled={!currentSession}
        >
          Download Data
        </button>
        
        <label>
          <input 
            id="fileUploadInput"
            type="file" 
            accept=".json" 
            onChange={handleFileUpload} 
          />
          Load Data
        </label>
        
        <button
          id="stopButton"
          onClick={mode === 'record' ? stopRecording : stopReplay}
        >
          Stop
        </button>
      </div>
      
      {/* Video metadata section */}
      <div className="mb-4">
        {videoId && <h2 className="text-lg font-medium">{videoId.replace(/-/g, ' ').replace(/\b\w/g, l => l.toUpperCase())}</h2>}
        {videoUrl && <p className="text-sm text-gray-600">Source: {videoUrl}</p>}
      </div>
      
      {/* Feedback Orchestrator handles all coordination */}
      <div className="relative">
        <VideoPlayer 
          ref={getVideoPlayerRef}
          isRecording={mode === 'record' && isActive}
          isReplaying={mode === 'replay' && isActive}
          setVideoRef={setVideoElementRef}
          replayAnnotations={feedbackData.annotations || []}
          videoUrl={videoUrl}
          onRecordAction={(action) => {
            // Forward video actions to the orchestrator
            if (orchestratorRef.current && mode === 'record' && isActive) {
              switch(action.type) {
                case 'play':
                case 'pause':
                case 'seek':
                case 'playbackRate':
                case 'keyboardShortcut':
                  orchestratorRef.current.handleVideoEvent(action.type, action.details);
                  break;
                case 'annotation':
                  if (action.details?.clear) {
                    // Pass the entire action to preserve timing details
                    const clearMetadata = {
                      clearVideoTime: videoRef.current?.currentTime ? videoRef.current.currentTime * 1000 : undefined,
                      clearTimestamp: Date.now(),
                      ...action.details
                    };
                    orchestratorRef.current.handleAnnotationEvent('clear', { ...action, details: clearMetadata });
                  } else if (action.details?.path) {
                    // Make sure path has visibility properties
                    const pathWithVisibility = {
                      ...action.details.path,
                      visible: true,
                      id: action.details.path.id || `annotation-${Date.now()}-${Math.random().toString(36).substr(2, 9)}`
                    };
                    orchestratorRef.current.handleAnnotationEvent('draw', pathWithVisibility);
                  }
                  break;
              }
            }
          }}
          onAnnotationAdded={(annotation) => {
            // Forward annotation events to the orchestrator
            if (orchestratorRef.current && mode === 'record' && isActive) {
              orchestratorRef.current.handleAnnotationEvent('draw', annotation);
            }
          }}
        />
        
        {/* Initialize the Orchestrator */}
        <FeedbackOrchestrator
          videoElementRef={videoRef}
          canvasRef={canvasRef}
          drawAnnotation={drawAnnotation}
          clearAnnotations={clearAnnotations}
          onAudioRecorded={handleAudioRecorded}
          onSessionComplete={handleSessionComplete}
          initialSession={currentSession}
          mode={mode}
          onCategoriesLoaded={(loadedCategories) => {
            // When a session is loaded with categories, we need to notify the parent component
            if (loadedCategories) {
              console.log('WRAPPER: Received loaded categories from orchestrator:', loadedCategories);
              
              // First clear existing categories
              if (onCategoriesCleared) {
                console.log('WRAPPER: Clearing existing categories before loading new ones');
                onCategoriesCleared();
              }
              
              // Check if we have any true categories
              const hasCheckedCategories = Object.values(loadedCategories).some(isChecked => isChecked);
              console.log(`WRAPPER: Has checked categories: ${hasCheckedCategories}`);
              
              // Then load the saved categories using the callback if available
              if (hasCheckedCategories && onCategoriesLoaded) {
                console.log('WRAPPER: Passing categories to parent component');
                
                // Delay slightly to ensure UI state is updated properly after clearing
                setTimeout(() => {
                  onCategoriesLoaded(loadedCategories);
                }, 100);
              } else {
                console.log('WRAPPER: No checked categories or no callback available');
              }
            } else {
              console.warn('WRAPPER: No categories data received from orchestrator');
            }
          }}
          ref={getOrchestratorRef}
        />
      </div>
      
      {currentSession && (
        <div className="mt-6 p-4 bg-gray-100 rounded-lg">
          <h3 className="text-lg font-semibold mb-2">Recorded Session</h3>
          <div className="text-sm">
            <p><strong>Session ID:</strong> {currentSession.id}</p>
            <p><strong>Start Time:</strong> {new Date(currentSession.startTime).toLocaleString()}</p>
            <p><strong>Events:</strong> {currentSession.events.length} recorded actions</p>
            <p><strong>Audio Duration:</strong> {(currentSession.audioTrack.totalDuration / 1000).toFixed(2)}s</p>
            
            {currentSession.categories && Object.keys(currentSession.categories).length > 0 && (
              <div className="mt-2">
                <p><strong>Categories:</strong></p>
                <ul className="list-disc ml-5">
                  {Object.entries(currentSession.categories)
                    .filter(([_, isSelected]) => isSelected)
                    .map(([category]) => (
                      <li key={category}>
                        {category.replace(/([A-Z])/g, ' $1').trim().replace(/^./, str => str.toUpperCase())}
                      </li>
                    ))}
                </ul>
              </div>
            )}
          </div>
          
          <div className="max-h-48 overflow-y-auto mt-4">
            <h4 className="font-medium text-sm mb-2">Timeline Events:</h4>
            <ul className="text-xs bg-white rounded p-2">
              {currentSession.events.map((event, index) => (
                <li key={index} className="mb-1 p-2 border-b">
                  <span className="font-semibold">{event.type}</span> at{' '}
                  <span className="font-mono">{(event.timeOffset / 1000).toFixed(2)}s</span>
                  {event.type === 'video' && (
                    <span className="block text-gray-600">
                      Action: {event.payload.action}
                      {event.payload.to !== undefined && ` (to: ${event.payload.to})`}
                    </span>
                  )}
                  {event.type === 'annotation' && (
                    <span className="block text-gray-600">
                      {event.payload.action === 'clear' 
                        ? "Cleared annotations" 
                        : `Drew annotation with ${event.payload.path?.points?.length || 0} points`}
                    </span>
                  )}
                  {event.type === 'marker' && (
                    <span className="block text-gray-600">
                      Marker: {event.payload.text}
                    </span>
                  )}
                  {event.type === 'category' && (
                    <span className="block text-gray-600">
                      Category: {getCategoryLabel(event.payload.category)} {event.payload.checked ? '(added)' : '(removed)'}
                    </span>
                  )}
                </li>
              ))}
            </ul>
          </div>
        </div>
      )}
    </div>
  );
}
|| END ||


|| START ./src/components/AnnotationCanvas.tsx ||

'use client';

import React, { useRef, useState, useEffect, useMemo, forwardRef, useImperativeHandle } from 'react';

export interface Point {
  x: number;
  y: number;
}

// Add type for window global storage
declare global {
  interface Window {
    __hiddenAnnotations?: DrawingPath[];
  }
}

export interface DrawingPath {
  points: Point[];
  color: string;
  width: number;
  timestamp: number;
  videoTime?: number; // Time in the video when this annotation was created (in ms)
  id: string; // Unique identifier for the drawing - now required
  visible: boolean; // Whether the drawing should be visible - now required
  hiddenAt?: number; // Time when the drawing was hidden (cleared)
  isClearEvent?: boolean; // Flag to identify clear events
}

interface AnnotationCanvasProps {
  width: number;
  height: number;
  isEnabled: boolean;
  currentTime: number;
  isRecording?: boolean;
  isReplaying?: boolean;
  onAnnotationAdded?: (path: DrawingPath) => void;
  replayAnnotations?: DrawingPath[];
  toolColor?: string;
  toolWidth?: number;
  clearCanvas?: boolean;
  onClearComplete?: () => void;
}

const AnnotationCanvas = forwardRef<any, AnnotationCanvasProps>(({
  width,
  height,
  isEnabled,
  currentTime,
  isRecording = false,
  isReplaying = false,
  onAnnotationAdded,
  replayAnnotations = [],
  toolColor = '#ff0000',
  toolWidth = 4,
  clearCanvas = false,
  onClearComplete
}, ref) => {
  const canvasRef = useRef<HTMLCanvasElement>(null);
  const [isDrawing, setIsDrawing] = useState(false);
  const [currentPath, setCurrentPath] = useState<Point[]>([]);
  const [allDrawings, setAllDrawings] = useState<DrawingPath[]>([]);
  
  // Reference to track recording start time for consistent relative timestamps
  const recordingStartTimeRef = useRef<number | null>(null);
  
  // Set recording start time when recording is enabled
  useEffect(() => {
    if (isRecording && !recordingStartTimeRef.current) {
      recordingStartTimeRef.current = Date.now();
      console.log('Recording started at:', recordingStartTimeRef.current);
    } else if (!isRecording) {
      recordingStartTimeRef.current = null;
    }
  }, [isRecording]);
  
  // Get canvas context in a memoized way
  const getContext = () => {
    const canvas = canvasRef.current;
    if (!canvas) return null;
    const ctx = canvas.getContext('2d');
    if (!ctx) return null;
    return ctx;
  };

  // Clear the canvas
  const clearCanvasDrawings = () => {
    // First, clear the visual canvas
    const ctx = getContext();
    if (ctx) {
      ctx.clearRect(0, 0, width, height);
    }
    
    // Get precise timestamps for this clear operation
    const currentVideoTime = currentTime * 1000; // Convert to milliseconds
    const now = Date.now();
    
    // Create a clear marker with precise timing information
    const clearMark = {
      id: `clear-${now}`,
      timestamp: now,
      videoTime: currentVideoTime,
      clearTimestamp: now,
      clearVideoTime: currentVideoTime,
      isClearEvent: true
    };
    
    // Make a copy of current drawings with visibility explicitly set to false
    const hiddenDrawings = allDrawings.map(drawing => ({
      ...drawing,
      visible: false,
      hiddenAt: currentVideoTime || now
    }));
    
    if (isReplaying) {
      // In replay mode, we mark drawings as hidden but keep them in the array
      // for proper timeline tracking and history
      setAllDrawings(prevDrawings => 
        prevDrawings.map(drawing => ({
          ...drawing,
          visible: false,
          hiddenAt: currentVideoTime || now
        }))
      );
      console.log(`REPLAY: Marked ${hiddenDrawings.length} drawings as hidden at ${currentVideoTime}ms`);
    } else {
      // In recording mode, we need to:
      // 1. Keep track of which drawings were hidden and when
      // 2. Clear the drawings array for immediate visual effect
      
      if (typeof window !== 'undefined') {
        // Store hidden drawings in window for later retrieval during playback
        window.__hiddenAnnotations = window.__hiddenAnnotations || [];
        window.__hiddenAnnotations.push(...hiddenDrawings);
        console.log(`RECORD: Stored ${hiddenDrawings.length} hidden drawings in window.__hiddenAnnotations`);
      }
      
      // Then clear the visible drawings for immediate visual effect
      setAllDrawings([]);
      console.log(`RECORD: Cleared ${hiddenDrawings.length} drawings from display at ${currentVideoTime}ms`);
    }
    
    // Return clear marker with precise timing for the orchestrator
    return clearMark;
  };

  // Listen for external clear command
  useEffect(() => {
    if (clearCanvas) {
      // Execute the clear canvas operation and get the result
      const result = clearCanvasDrawings();
      
      console.log('Clear canvas command received, cleared drawings with result:', result);
      
      // Store clear event information globally for cross-component access
      if (typeof window !== 'undefined') {
        const now = Date.now();
        const videoTimeMs = currentTime * 1000;
        
        // Add to window.__clearEvents array (used by annotation filtering)
        window.__clearEvents = window.__clearEvents || [];
        window.__clearEvents.push({
          timestamp: now,
          videoTime: videoTimeMs,
          absoluteTime: now
        });
        
        console.log(`Added clear event to window.__clearEvents, video time: ${videoTimeMs}ms`);
      }
      
      // Notify parent component that clearing is complete
      if (onClearComplete) {
        onClearComplete();
      }
    }
  }, [clearCanvas, onClearComplete, currentTime]);

  // Initialize canvas
  useEffect(() => {
    const canvas = canvasRef.current;
    if (!canvas) return;
    
    canvas.width = width;
    canvas.height = height;

    const ctx = canvas.getContext('2d');
    if (!ctx) return;
    
    ctx.lineCap = 'round';
    ctx.lineJoin = 'round';
  }, [width, height]);

  // Handle drawing a path
  const drawPath = (ctx: CanvasRenderingContext2D, path: DrawingPath) => {
    if (path.points.length < 2) return;

    ctx.beginPath();
    ctx.strokeStyle = path.color;
    ctx.lineWidth = path.width;
    
    ctx.moveTo(path.points[0].x, path.points[0].y);
    
    for (let i = 1; i < path.points.length; i++) {
      ctx.lineTo(path.points[i].x, path.points[i].y);
    }
    
    ctx.stroke();
  };

  // Track clear events during replay
  const [clearEvents, setClearEvents] = useState<number[]>([]);
  
  // Reference to all clear events including from window storage
  const allClearEventsRef = useRef<number[]>([]);
  
  // Process replay annotations to track clear events
  useEffect(() => {
    if (!isReplaying || replayAnnotations.length === 0) return;
    
    // Find all clear events by their timestamps or isClearEvent flag
    const clearTimestamps: number[] = [];
    const clearEvents: Record<number, any> = {};
    
    console.log('Examining all annotations for clear events:', replayAnnotations.length);
    
    // First look for any clear actions that might be in the annotations array
    replayAnnotations.forEach((annotation, index) => {
      // Standardized detection of clear events using multiple possible formats
      const isClearEvent = annotation.isClearEvent === true;
      const clearDetails = (annotation as any).details?.clear === true;
      const isExplicitClear = annotation.points?.length === 0 && annotation.hiddenAt;
      
      if (isClearEvent || clearDetails || isExplicitClear) {
        // Log only the first few events to avoid console spam
        if (index < 5) {
          console.log(`Found clear event candidate in annotation ${index}:`, annotation);
        }
        
        // Try to extract timestamp from various properties in order of reliability
        let clearTime = 0;
        
        // 1. Use clearVideoTime from various locations (most reliable)
        if (annotation.clearVideoTime) {
          clearTime = annotation.clearVideoTime;
        }
        else if ((annotation as any).details?.clearVideoTime) {
          clearTime = (annotation as any).details.clearVideoTime;
        }
        // 2. Try hiddenAt timestamp (for explicitly hidden annotations)
        else if (annotation.hiddenAt) {
          clearTime = annotation.hiddenAt;
        }
        // 3. Use various timestamp properties
        else if (annotation.clearTimestamp) {
          clearTime = annotation.clearTimestamp;
        }
        else if ((annotation as any).details?.clearTimestamp) {
          clearTime = (annotation as any).details.clearTimestamp;
        }
        // 4. Fall back to standard timing properties as last resort
        else {
          clearTime = (annotation as any).timeOffset 
            || annotation.videoTime 
            || annotation.timestamp;
        }
        
        if (clearTime) {
          // Apply a small negative offset to ensure clear events happen
          // slightly before the next annotation in the timeline
          // This helps fix the "one event behind" playback issue
          const adjustedClearTime = clearTime - 10; // 10ms earlier
          
          // Round to nearest 10ms to handle slight timing differences
          const roundedClearTime = Math.round(adjustedClearTime / 10) * 10;
          clearTimestamps.push(roundedClearTime);
          
          // Store the full clear event for reference
          clearEvents[roundedClearTime] = annotation;
          
          if (index < 5) {
            console.log(`Found clear event at timestamp: ${roundedClearTime}ms (adjusted from ${clearTime}ms)`);
          }
        }
      }
    });
    
    // Also check for hidden annotations from window storage (recording mode)
    if (typeof window !== 'undefined' && window.__hiddenAnnotations?.length > 0) {
      console.log(`Found ${window.__hiddenAnnotations.length} hidden annotations in window storage`);
      
      window.__hiddenAnnotations.forEach(annotation => {
        if (annotation.hiddenAt) {
          // Apply the same adjustment as above to ensure consistency
          const adjustedClearTime = annotation.hiddenAt - 10; // 10ms earlier
          const clearTime = Math.round(adjustedClearTime / 10) * 10;
          
          if (!clearTimestamps.includes(clearTime)) {
            clearTimestamps.push(clearTime);
            console.log(`Added clear event from window storage at ${clearTime}ms (adjusted from ${annotation.hiddenAt}ms)`);
          }
        }
      });
    }
    
    // Log summary of what we found
    if (clearTimestamps.length > 0) {
      console.log(`Found ${clearTimestamps.length} clear events`);
    } else {
      console.warn('No clear events found in replay annotations!');
    }
    
    // Sort by time
    clearTimestamps.sort((a, b) => a - b);
    
    if (clearTimestamps.length > 0) {
      console.log(`Clear events detected at: ${clearTimestamps.slice(0, 5).join(', ')}${clearTimestamps.length > 5 ? '...' : ''}`);
    }
    
    // Update state only if there's a change to avoid extra renders
    if (clearTimestamps.length !== clearEvents.length || 
        clearTimestamps.some((t, i) => t !== clearEvents[i])) {
      setClearEvents(clearTimestamps);
    }
  }, [isReplaying, replayAnnotations]);
  
  // Draw annotations during replay
  useEffect(() => {
    if (!isReplaying || replayAnnotations.length === 0) return;
    
    const ctx = getContext();
    if (!ctx) return;
    
    // Clear canvas before drawing
    ctx.clearRect(0, 0, width, height);
    
    // Draw all annotations that should be visible at the current time
    // Use the video's currentTime (in seconds) to determine which annotations to show
    const videoTimeMs = currentTime * 1000; // Convert to milliseconds
    
    // Find the most recent clear event before current time
    // Initial most recent clear time calculation
    const initialMostRecentClearTime = clearEvents.length > 0 
      ? Math.max(0, ...clearEvents.filter(clearTime => clearTime <= videoTimeMs)) 
      : 0;
    
    if (videoTimeMs % 1000 === 0) { // Log only once per second to reduce spam
      console.log(`Initial check - Current: ${videoTimeMs}ms, Last clear: ${initialMostRecentClearTime}ms`);
    }
    
    // Also check window storage for clear events
    if (typeof window !== 'undefined' && window.__clearEvents?.length > 0) {
      // Add any clear events from window storage that aren't already in clearEvents
      const windowClearTimes = window.__clearEvents.map(e => e.videoTime);
      
      // Update allClearEventsRef with combined clear events
      const combinedClearEvents = [...clearEvents];
      
      windowClearTimes.forEach(clearTime => {
        if (!combinedClearEvents.includes(clearTime)) {
          combinedClearEvents.push(clearTime);
        }
      });
      
      // Sort and update the reference
      combinedClearEvents.sort((a, b) => a - b);
      allClearEventsRef.current = combinedClearEvents;
      
      if (combinedClearEvents.length !== clearEvents.length) {
        console.log(`Using ${combinedClearEvents.length} clear events (${clearEvents.length} from state + ${windowClearTimes.length} from window storage)`);
      }
    } else {
      // Just use the state-based clear events
      allClearEventsRef.current = [...clearEvents];
    }
    
    // Find the most recent clear event using our enhanced combined list
    const mostRecentClearTime = allClearEventsRef.current.length > 0 
      ? Math.max(0, ...allClearEventsRef.current.filter(clearTime => clearTime <= videoTimeMs)) 
      : 0;
    
    // Combine replay annotations with any hidden annotations from recording
    let combinedAnnotations = [...replayAnnotations];
    
    // Include hidden annotations from window storage if available
    if (typeof window !== 'undefined' && window.__hiddenAnnotations?.length > 0) {
      combinedAnnotations = [...combinedAnnotations, ...window.__hiddenAnnotations];
    }
    
    // Enhanced multi-step filtering for annotations
    // 1. First filter clear events and explicit invisible annotations 
    const filteredAnnotations = combinedAnnotations.filter(annotation => {
      // Skip clear events markers (they're not visible drawings)
      if (annotation.isClearEvent || (annotation as any).details?.clear) {
        return false;
      }
      
      // Skip annotations with points length of 0 (these are likely clear markers)
      if (!annotation.points || annotation.points.length === 0) {
        return false;
      }
      
      return true;
    });
    
    // 2. Apply visibility rules
    const visibleAnnotations = filteredAnnotations.filter(annotation => {
      // Priority 1: Check explicit visibility flag
      if (annotation.visible === false) {
        return false;
      }
      
      // Priority 2: Check hiddenAt timestamp (direct clearing)
      if (annotation.hiddenAt && annotation.hiddenAt <= videoTimeMs) {
        return false;
      }
      
      // Get annotation timestamp using various possible properties
      const annotationTime = (annotation as any).timeOffset || 
                            annotation.videoTime || 
                            annotation.timestamp;
      
      if (!annotationTime) {
        console.warn('Annotation has no timing information:', annotation);
        return false;
      }
      
      // Priority 3: Check against global clear events
      if (mostRecentClearTime > 0 && annotationTime <= mostRecentClearTime) {
        return false;
      }
      
      // Priority 4: Check timing - annotation must be before current time
      return annotationTime <= videoTimeMs;
    });
    
    // Log some stats about what we're showing (only once per second)
    if (videoTimeMs % 1000 === 0) {
      if (visibleAnnotations.length > 0) {
        console.log(`Showing ${visibleAnnotations.length} of ${filteredAnnotations.length} annotations at ${videoTimeMs}ms`);
      } else {
        console.log(`No annotations to show at ${videoTimeMs}ms (${filteredAnnotations.length} total annotations)`);
      }
    }
    
    // Draw the visible annotations
    visibleAnnotations.forEach(path => {
      drawPath(ctx, path);
    });
  }, [isReplaying, replayAnnotations, currentTime, width, height, clearEvents]);

  // Draw all stored paths
  useEffect(() => {
    if (isReplaying) return; // Don't draw local paths during replay
    
    const ctx = getContext();
    if (!ctx) return;
    
    // Clear canvas before drawing
    ctx.clearRect(0, 0, width, height);
    
    // Draw all stored paths
    allDrawings.forEach(path => {
      drawPath(ctx, path);
    });
  }, [allDrawings, isReplaying, width, height]);

  // Method to handle an annotation that was generated programmatically
  const handleManualAnnotation = (path: DrawingPath) => {
    // Ensure the annotation has a unique ID
    const id = path.id || `annotation-${Date.now()}-${Math.random().toString(36).substr(2, 9)}`;
    
    // Get precise timing information
    const now = Date.now();
    const videoTimeMs = path.videoTime || (currentTime * 1000);
    
    // Log additional timing information for debugging
    console.log('Handling manual annotation:', {
      id,
      points: path.points?.length || 0,
      videoTime: videoTimeMs,
      timestamp: path.timestamp || now,
      timeOffset: (path as any).timeOffset,
      currentVideoTime: currentTime * 1000
    });
    
    // Ensure annotation has complete and consistent metadata
    const enhancedAnnotation: DrawingPath = {
      ...path,
      id,
      visible: true, // Explicitly set to visible
      timestamp: path.timestamp || now,
      videoTime: videoTimeMs,
      // Add any missing required properties based on interface
      points: path.points || [],
      color: path.color || toolColor,
      width: path.width || toolWidth
    };
    
    // Add annotation to local state
    setAllDrawings(prev => [...prev, enhancedAnnotation]);
    
    // Report the annotation if we're recording
    if (isRecording && onAnnotationAdded) {
      // Pass the fully enhanced annotation with all metadata
      onAnnotationAdded(enhancedAnnotation);
    }
    
    return enhancedAnnotation;
  };

  // Event handlers for drawing
  const startDrawing = (e: React.MouseEvent<HTMLCanvasElement> | React.TouchEvent<HTMLCanvasElement>) => {
    if (isReplaying) return; // Drawing is always enabled, only check for replay mode
    
    setIsDrawing(true);
    setCurrentPath([]);

    const canvas = canvasRef.current;
    if (!canvas) return;

    const rect = canvas.getBoundingClientRect();
    let clientX: number, clientY: number;

    if ('touches' in e) {
      // Touch event
      clientX = e.touches[0].clientX;
      clientY = e.touches[0].clientY;
    } else {
      // Mouse event
      clientX = e.clientX;
      clientY = e.clientY;
    }

    const x = clientX - rect.left;
    const y = clientY - rect.top;
    
    setCurrentPath([{ x, y }]);
  };

  const draw = (e: React.MouseEvent<HTMLCanvasElement> | React.TouchEvent<HTMLCanvasElement>) => {
    if (!isDrawing || isReplaying) return; // Drawing is always enabled
    
    const canvas = canvasRef.current;
    if (!canvas) return;

    const rect = canvas.getBoundingClientRect();
    let clientX: number, clientY: number;

    if ('touches' in e) {
      // Touch event
      clientX = e.touches[0].clientX;
      clientY = e.touches[0].clientY;
      
      // Prevent scrolling while drawing
      e.preventDefault();
    } else {
      // Mouse event
      clientX = e.clientX;
      clientY = e.clientY;
    }

    const x = clientX - rect.left;
    const y = clientY - rect.top;
    
    setCurrentPath(prev => [...prev, { x, y }]);
    
    const ctx = getContext();
    if (ctx && currentPath.length > 0) {
      ctx.beginPath();
      ctx.strokeStyle = toolColor;
      ctx.lineWidth = toolWidth;
      
      const lastPoint = currentPath[currentPath.length - 1];
      ctx.moveTo(lastPoint.x, lastPoint.y);
      ctx.lineTo(x, y);
      ctx.stroke();
    }
  };

  const endDrawing = () => {
    if (!isDrawing || isReplaying) return; // Drawing is always enabled
    
    setIsDrawing(false);
    
    if (currentPath.length > 1) {
      // Generate a unique ID for this annotation
      const id = `annotation-${Date.now()}-${Math.random().toString(36).substr(2, 9)}`;
      const now = Date.now();
      const videoTimeMs = currentTime * 1000;
      
      // Create a new annotation with complete metadata
      const newPath: DrawingPath = {
        points: [...currentPath],
        color: toolColor,
        width: toolWidth,
        timestamp: now,
        id,
        visible: true,
        videoTime: videoTimeMs
      };
      
      // Add to local drawings for immediate display
      setAllDrawings(prev => [...prev, newPath]);
      
      // Report the annotation if we're recording
      if (isRecording && onAnnotationAdded) {
        // Include complete metadata
        const annotationWithMetadata = {
          ...newPath,
          timeOffset: now - (recordingStartTimeRef.current || now)
        };
        onAnnotationAdded(annotationWithMetadata);
        
        console.log(`Created annotation with ID ${id} at video time ${videoTimeMs}ms`);
      }
    }
    
    setCurrentPath([]);
  };

  // Expose methods to parent component
  useImperativeHandle(ref, () => ({
    // Core canvas ref and state
    canvasRef,
    allDrawings,
    
    // Canvas utility methods
    getContext: () => getContext(),
    
    // Drawing manipulation methods
    clearCanvasDrawings: () => {
      console.log('AnnotationCanvas: Clearing all drawings');
      clearCanvasDrawings();
    },
    
    handleManualAnnotation: (path: DrawingPath) => {
      console.log('AnnotationCanvas: Handling manual annotation:', {
        pointsCount: path.points?.length || 0,
        color: path.color,
        width: path.width,
        videoTime: path.videoTime
      });
      handleManualAnnotation(path);
    }
  }));

  return (
    <canvas
      ref={canvasRef}
      className="absolute top-0 left-0 z-10 cursor-crosshair"
      width={width}
      height={height}
      onMouseDown={startDrawing}
      onMouseMove={draw}
      onMouseUp={endDrawing}
      onMouseLeave={endDrawing}
      onTouchStart={startDrawing}
      onTouchMove={draw}
      onTouchEnd={endDrawing}
    />
  );
});

AnnotationCanvas.displayName = 'AnnotationCanvas';

export default AnnotationCanvas;
|| END ||


|| START ./src/components/VideoPlayer.tsx ||

'use client';

import React, { useState, useRef, useEffect } from 'react';

// Define the types of events we want to record
// Add window global definitions
declare global {
  interface Window {
    __clearEvents?: Array<{
      timestamp: number;
      videoTime: number;
      absoluteTime: number;
    }>;
  }
}

export type ActionType = 'play' | 'pause' | 'seek' | 'playbackRate' | 'keyboardShortcut' | 'annotation' | 'audio';

// Define the structure of a recorded action
export interface RecordedAction {
  type: ActionType;
  timestamp: number; // Time in milliseconds since recording started
  videoTime: number; // Current time in the video
  details?: {
    [key: string]: any; // Additional details specific to the action
  };
}

import AnnotationCanvas, { DrawingPath } from './AnnotationCanvas';

import { AudioChunk } from './AudioRecorder';

export interface FeedbackData {
  sessionId: string;
  videoId: string;
  actions: RecordedAction[];
  startTime: number;
  endTime?: number;
  annotations?: DrawingPath[];
  audioChunks?: AudioChunk[];
}

interface VideoPlayerProps {
  isRecording?: boolean;
  isReplaying?: boolean;
  onRecordAction?: (action: RecordedAction) => void;
  setVideoRef?: (ref: HTMLVideoElement | null) => void;
  replayAnnotations?: DrawingPath[];
  onAnnotationAdded?: (annotation: DrawingPath) => void;
  videoUrl?: string;
}

interface VideoPlayerImperativeHandle {
  video: HTMLVideoElement | null;
  annotationCanvas: any;
}

const VideoPlayer = React.forwardRef<VideoPlayerImperativeHandle, VideoPlayerProps>(({ 
  isRecording = false, 
  isReplaying = false,
  onRecordAction,
  setVideoRef,
  replayAnnotations = [],
  onAnnotationAdded,
  videoUrl = "https://commondatastorage.googleapis.com/gtv-videos-bucket/sample/BigBuckBunny.mp4"
}: VideoPlayerProps, ref) => {
  const [playing, setPlaying] = useState(false);
  const [currentTime, setCurrentTime] = useState(0);
  const [duration, setDuration] = useState(0);
  const [playbackRate, setPlaybackRate] = useState(1);
  const [isAnnotationEnabled, setIsAnnotationEnabled] = useState(true);
  const [annotationColor, setAnnotationColor] = useState('#ff0000'); // Default red
  const [annotationWidth, setAnnotationWidth] = useState(3);
  const [videoDimensions, setVideoDimensions] = useState({ width: 0, height: 0 });
  const [shouldClearCanvas, setShouldClearCanvas] = useState(false);
  
  const videoRef = useRef<HTMLVideoElement>(null);
  const videoContainerRef = useRef<HTMLDivElement>(null);
  const recordingStartTimeRef = useRef<number | null>(null);
  const annotationCanvasRef = useRef<any>(null);

  // Initialize recording start time when recording begins
  useEffect(() => {
    if (isRecording && !recordingStartTimeRef.current) {
      recordingStartTimeRef.current = Date.now();
    } else if (!isRecording) {
      recordingStartTimeRef.current = null;
    }
  }, [isRecording]);
  
  // Pass video element reference to parent component
  useEffect(() => {
    if (setVideoRef && videoRef.current) {
      setVideoRef(videoRef.current);
    }
    
    return () => {
      if (setVideoRef) {
        setVideoRef(null);
      }
    };
  }, [setVideoRef, videoRef.current]);
  
  // Update video dimensions when video metadata is loaded
  useEffect(() => {
    const updateVideoDimensions = () => {
      if (videoRef.current && videoContainerRef.current) {
        const containerRect = videoContainerRef.current.getBoundingClientRect();
        setVideoDimensions({
          width: containerRect.width,
          height: containerRect.height
        });
      }
    };
    
    // Initial update
    if (videoRef.current) {
      if (videoRef.current.readyState >= 1) {
        updateVideoDimensions();
      } else {
        videoRef.current.addEventListener('loadedmetadata', updateVideoDimensions);
      }
    }
    
    // Update dimensions on window resize
    window.addEventListener('resize', updateVideoDimensions);
    
    return () => {
      window.removeEventListener('resize', updateVideoDimensions);
      if (videoRef.current) {
        videoRef.current.removeEventListener('loadedmetadata', updateVideoDimensions);
      }
    };
  }, [videoRef.current]);
  
  // Handle annotation being added
  const handleAnnotationAdded = (path: DrawingPath) => {
    // Generate a unique ID for the annotation if it doesn't have one
    const id = path.id || `annotation-${Date.now()}-${Math.random().toString(36).substr(2, 9)}`;
    
    // Update the annotation with the video currentTime and make it visible
    const annotationWithVideoTime = {
      ...path,
      id,
      visible: true,
      // Store both the original timestamp (relative to the recording start)
      // and update the timestamp to match the video's current playback position
      videoTime: currentTime * 1000
    };
    
    // If recording, pass the annotation to the parent
    if (isRecording && onAnnotationAdded) {
      onAnnotationAdded(annotationWithVideoTime);
    }
    
    // Record the annotation action
    if (isRecording && recordingStartTimeRef.current && onRecordAction) {
      const action: RecordedAction = {
        type: 'annotation',
        timestamp: Date.now() - recordingStartTimeRef.current,
        videoTime: currentTime,
        details: { path: annotationWithVideoTime }
      };
      onRecordAction(action);
    }
  };
  
  // Toggle annotation mode - removed as drawing is always enabled
  
  // Clear annotations
  const clearAnnotations = () => {
    setShouldClearCanvas(true);
    
    // Record the clear action if recording
    if (isRecording && recordingStartTimeRef.current && onRecordAction) {
      // Get precise timing information
      const now = Date.now();
      const clearTimestamp = now - recordingStartTimeRef.current;
      const clearVideoTime = currentTime * 1000; // Convert to ms
      
      console.log(`Creating clear event at video time: ${clearVideoTime}ms, timestamp: ${clearTimestamp}ms`);
      
      // Create a more comprehensive clear action with multiple timestamps
      const action: RecordedAction = {
        type: 'annotation',
        timestamp: clearTimestamp,
        videoTime: currentTime,
        details: { 
          clear: true,
          clearTimestamp: clearTimestamp,
          clearVideoTime: clearVideoTime,
          absoluteTimestamp: now,
          id: `clear-${now}`
        }
      };
      
      // Send the clear action to be recorded
      onRecordAction(action);
      
      // Store the clear event in window for cross-component access
      if (typeof window !== 'undefined') {
        window.__clearEvents = window.__clearEvents || [];
        window.__clearEvents.push({
          timestamp: clearTimestamp,
          videoTime: clearVideoTime,
          absoluteTime: now
        });
        console.log(`Stored clear event in window.__clearEvents, total: ${window.__clearEvents.length}`);
      }
    }
  };
  
  // Handle canvas clear completion
  const handleClearComplete = () => {
    setShouldClearCanvas(false);
  };

  // Function to record an action if recording is enabled
  const recordAction = (type: ActionType, details?: {[key: string]: any}) => {
    if (isRecording && recordingStartTimeRef.current && onRecordAction) {
      const action: RecordedAction = {
        type,
        timestamp: Date.now() - recordingStartTimeRef.current,
        videoTime: currentTime,
        details
      };
      onRecordAction(action);
    }
  };

  const togglePlay = () => {
    if (videoRef.current) {
      if (playing) {
        videoRef.current.pause();
        recordAction('pause');
      } else {
        videoRef.current.play();
        recordAction('play');
      }
      setPlaying(!playing);
    }
  };

  const handleTimeUpdate = () => {
    if (videoRef.current) {
      setCurrentTime(videoRef.current.currentTime);
    }
  };

  const handleLoadedMetadata = () => {
    if (videoRef.current) {
      setDuration(videoRef.current.duration);
    }
  };

  const handleSeek = (e: React.ChangeEvent<HTMLInputElement>) => {
    const time = parseFloat(e.target.value);
    if (videoRef.current) {
      const previousTime = videoRef.current.currentTime;
      videoRef.current.currentTime = time;
      setCurrentTime(time);
      recordAction('seek', { from: previousTime, to: time });
    }
  };


  const handlePlaybackRateChange = (rate: number) => {
    if (videoRef.current) {
      const previousRate = videoRef.current.playbackRate;
      videoRef.current.playbackRate = rate;
      setPlaybackRate(rate);
      recordAction('playbackRate', { from: previousRate, to: rate });
    }
  };

  const formatTime = (time: number) => {
    const minutes = Math.floor(time / 60);
    const seconds = Math.floor(time % 60);
    return `${minutes}:${seconds < 10 ? '0' : ''}${seconds}`;
  };

  // Add keyboard shortcuts
  useEffect(() => {
    const handleKeyDown = (e: KeyboardEvent) => {
      if (e.key === ' ' || e.key === 'k') {
        togglePlay();
        recordAction('keyboardShortcut', { key: e.key, action: playing ? 'pause' : 'play' });
      } else if (e.key === 'ArrowLeft') {
        if (videoRef.current) {
          const previousTime = videoRef.current.currentTime;
          videoRef.current.currentTime = Math.max(0, videoRef.current.currentTime - 5);
          setCurrentTime(videoRef.current.currentTime);
          recordAction('keyboardShortcut', { 
            key: e.key, 
            action: 'rewind',
            from: previousTime,
            to: videoRef.current.currentTime 
          });
        }
      } else if (e.key === 'ArrowRight') {
        if (videoRef.current) {
          const previousTime = videoRef.current.currentTime;
          videoRef.current.currentTime = Math.min(duration, videoRef.current.currentTime + 5);
          setCurrentTime(videoRef.current.currentTime);
          recordAction('keyboardShortcut', { 
            key: e.key, 
            action: 'forward',
            from: previousTime,
            to: videoRef.current.currentTime
          });
        }
      // 'm' shortcut removed
      }
    };

    window.addEventListener('keydown', handleKeyDown);
    return () => {
      window.removeEventListener('keydown', handleKeyDown);
    };
  }, [duration, playing, isRecording, currentTime]);

  // Expose the video element and annotation canvas to parent component
  React.useImperativeHandle(ref, () => ({
    video: videoRef.current,
    annotationCanvas: annotationCanvasRef.current
  }));

  // Expose handlers for AnnotationCanvas 
  const handleManualAnnotation = (path: DrawingPath) => {
    if (annotationCanvasRef.current) {
      annotationCanvasRef.current.handleManualAnnotation(path);
    }
  };

  const clearAllAnnotations = () => {
    if (annotationCanvasRef.current) {
      annotationCanvasRef.current.clearCanvasDrawings();
    }
  };

  // Add methods to imperativeHandle 
  React.useImperativeHandle(ref, () => ({
    // Expose the video element
    video: videoRef.current,
    
    // Expose the annotation canvas and its methods
    annotationCanvas: annotationCanvasRef.current,
    
    // Expose annotation methods directly at the top level for easier access
    handleManualAnnotation: (path: DrawingPath) => {
      if (annotationCanvasRef.current) {
        console.log('VideoPlayer: Forwarding manual annotation to canvas');
        annotationCanvasRef.current.handleManualAnnotation(path);
        
        // If recording is active, also record this event
        if (isRecording && onRecordAction) {
          const action: RecordedAction = {
            type: 'annotation',
            timestamp: Date.now() - (recordingStartTimeRef.current || 0),
            videoTime: currentTime,
            details: { path }
          };
          onRecordAction(action);
        }
      } else {
        console.warn('VideoPlayer: Cannot forward annotation - canvas ref not available');
      }
    },
    
    clearAllAnnotations: () => {
      if (annotationCanvasRef.current) {
        console.log('VideoPlayer: Forwarding clear annotation to canvas');
        annotationCanvasRef.current.clearCanvasDrawings();
        
        // If recording is active, also record this event
        if (isRecording && onRecordAction) {
          const action: RecordedAction = {
            type: 'annotation',
            timestamp: Date.now() - (recordingStartTimeRef.current || 0),
            videoTime: currentTime,
            details: { clear: true }
          };
          onRecordAction(action);
        }
      } else {
        console.warn('VideoPlayer: Cannot clear annotations - canvas ref not available');
      }
    }
  }));

  return (
    <div className="flex flex-col w-full max-w-3xl bg-gray-100 rounded-lg shadow-md overflow-hidden">
      <div className="relative" ref={videoContainerRef}>
        {isRecording && (
          <div className="absolute top-2 right-2 z-20 flex items-center px-2 py-1 bg-red-500 text-white rounded-md text-sm">
            <span className="animate-pulse mr-1">●</span> Recording
          </div>
        )}
        <video
          ref={videoRef}
          className="w-full aspect-video"
          onTimeUpdate={handleTimeUpdate}
          onLoadedMetadata={handleLoadedMetadata}
          src={videoUrl}
          playsInline
          muted
        />
        
        {videoDimensions.width > 0 && videoDimensions.height > 0 && (
          <AnnotationCanvas
            ref={annotationCanvasRef}
            width={videoDimensions.width}
            height={videoDimensions.height}
            isEnabled={isAnnotationEnabled && !isReplaying}
            currentTime={currentTime}
            isRecording={isRecording}
            isReplaying={isReplaying}
            onAnnotationAdded={handleAnnotationAdded}
            replayAnnotations={replayAnnotations}
            toolColor={annotationColor}
            toolWidth={annotationWidth}
            clearCanvas={shouldClearCanvas}
            onClearComplete={handleClearComplete}
          />
        )}
      </div>
      
      <div className="p-4 bg-white">
        <div className="flex items-center mb-2">
          <input
            type="range"
            min="0"
            max={duration || 0}
            value={currentTime}
            onChange={handleSeek}
            className="w-full h-2 bg-gray-200 rounded-lg appearance-none cursor-pointer"
          />
        </div>
        
        <div className="flex justify-between items-center mb-2">
          <div className="flex items-center space-x-2">
            <button
              onClick={togglePlay}
              className="p-2 rounded-full bg-gray-200 hover:bg-gray-300"
            >
              {playing ? 
                <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="currentColor" className="w-6 h-6">
                  <path fillRule="evenodd" d="M6.75 5.25a.75.75 0 0 1 .75-.75H9a.75.75 0 0 1 .75.75v13.5a.75.75 0 0 1-.75.75H7.5a.75.75 0 0 1-.75-.75V5.25Zm7.5 0A.75.75 0 0 1 15 4.5h1.5a.75.75 0 0 1 .75.75v13.5a.75.75 0 0 1-.75.75H15a.75.75 0 0 1-.75-.75V5.25Z" clipRule="evenodd" />
                </svg>
                :
                <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="currentColor" className="w-6 h-6">
                  <path fillRule="evenodd" d="M4.5 5.653c0-1.427 1.529-2.33 2.779-1.643l11.54 6.347c1.295.712 1.295 2.573 0 3.286l-11.54 6.347c-1.25.687-2.779-.217-2.779-1.643V5.653Z" clipRule="evenodd" />
                </svg>
              }
            </button>
            
            
            <span className="text-sm text-gray-600">
              {formatTime(currentTime)} / {formatTime(duration)}
            </span>
          </div>
          
          <div className="flex items-center space-x-2">
            <select 
              value={playbackRate}
              onChange={(e) => handlePlaybackRateChange(parseFloat(e.target.value))}
              className="bg-gray-200 text-sm rounded px-2 py-1"
            >
              <option value="0.5">0.5x</option>
              <option value="1">1x</option>
              <option value="1.5">1.5x</option>
              <option value="2">2x</option>
            </select>
          </div>
        </div>
        
        {/* Annotation controls */}
        <div className="flex flex-wrap items-center justify-between pt-2 border-t border-gray-200">
          <div className="flex items-center space-x-2">
            
            <div className="flex items-center space-x-1">
              <label className="text-xs text-gray-600">Color:</label>
              <select
                value={annotationColor}
                onChange={(e) => setAnnotationColor(e.target.value)}
                className="bg-gray-100 text-xs rounded p-1 border border-gray-300"
              >
                <option value="#ff0000">Red</option>
                <option value="#0000ff">Blue</option>
                <option value="#00ff00">Green</option>
                <option value="#ffff00">Yellow</option>
                <option value="#000000">Black</option>
                <option value="#ffffff">White</option>
              </select>
            </div>
            
            <div className="flex items-center space-x-1">
              <label className="text-xs text-gray-600">Width:</label>
              <select
                value={annotationWidth}
                onChange={(e) => setAnnotationWidth(parseInt(e.target.value))}
                className="bg-gray-100 text-xs rounded p-1 border border-gray-300"
              >
                <option value="1">Thin</option>
                <option value="3">Medium</option>
                <option value="5">Thick</option>
                <option value="8">Very Thick</option>
              </select>
            </div>
            
            <button
              onClick={clearAnnotations}
              className="py-1 px-3 text-xs bg-red-100 hover:bg-red-200 text-red-700 rounded-md transition-colors"
              disabled={isReplaying}
            >
              Clear
            </button>
          </div>
        </div>
      </div>
    </div>
  );
});

// Add displayName for better debugging in React DevTools
VideoPlayer.displayName = 'VideoPlayer';

export default VideoPlayer;
|| END ||


|| START ./src/components/AudioRecorder.tsx ||

'use client';

import React, { useState, useRef, useEffect, useCallback } from 'react';

export interface AudioChunk {
  blob: Blob | string;      // The audio data as Blob or string (for serialization)
  startTime: number;        // Relative to recording start
  duration: number;         // Length of audio chunk in ms
  videoTime: number;        // Video timestamp when this audio was recorded
  url?: string;             // URL for playback (created during replay)
  mimeType?: string;        // MIME type for proper playback
}

interface AudioRecorderProps {
  isRecording: boolean;
  isReplaying: boolean;
  currentVideoTime: number;
  onAudioChunk?: (chunk: AudioChunk) => void;
  replayAudioChunks?: AudioChunk[];
}

export default function AudioRecorder({
  isRecording,
  isReplaying,
  currentVideoTime,
  onAudioChunk,
  replayAudioChunks = []
}: AudioRecorderProps) {
  const [isRecordingAudio, setIsRecordingAudio] = useState(false);
  const [audioPermissionGranted, setAudioPermissionGranted] = useState(false);
  const [error, setError] = useState<string | null>(null);
  const [recordingFormat, setRecordingFormat] = useState('');
  const [elapsedTime, setElapsedTime] = useState(0);
  
  // Refs for managing audio recording state
  const mediaRecorderRef = useRef<MediaRecorder | null>(null);
  const streamRef = useRef<MediaStream | null>(null);
  const recordingStartTimeRef = useRef<number | null>(null);
  const chunksRef = useRef<Blob[]>([]);
  const timerRef = useRef<NodeJS.Timeout | null>(null);
  
  // Refs for managing audio playback during replay
  const audioPlayersRef = useRef<Map<number, HTMLAudioElement>>(new Map());
  const playingChunksRef = useRef<Set<number>>(new Set());
  
  // Check for audio permission when component mounts
  useEffect(() => {
    const checkAudioPermission = async () => {
      try {
        const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
        setAudioPermissionGranted(true);
        
        // Stop the tracks after permission check
        stream.getTracks().forEach(track => track.stop());
      } catch (err) {
        console.error('Error accessing microphone:', err);
        setError('Microphone access denied. Please enable microphone permissions in your browser.');
        setAudioPermissionGranted(false);
      }
    };
    
    checkAudioPermission();
  }, []);
  
  // Start or stop recording based on props
  useEffect(() => {
    if (isRecording && audioPermissionGranted && !isRecordingAudio) {
      // We intentionally set recordingStartTimeRef.current to null here
      // to ensure a clean start for each new recording session.
      // It will be properly set in startAudioRecording() before the recorder starts.
      recordingStartTimeRef.current = null;
      
      console.log('Starting audio recording because isRecording=true');
      startAudioRecording();
    } else if (!isRecording && isRecordingAudio) {
      console.log('Stopping audio recording because isRecording=false');
      stopAudioRecording();
    }
    
    // Clean up on unmount
    return () => {
      if (isRecordingAudio) {
        stopAudioRecording();
      }
      cleanupAudioPlayers();
    };
  }, [isRecording, audioPermissionGranted, isRecordingAudio]);
  
  // Start recording audio
  const startAudioRecording = async () => {
    try {
      // Reset state
      chunksRef.current = [];
      setElapsedTime(0);
      setError(null);
      
      // Request microphone access with quality settings
      const stream = await navigator.mediaDevices.getUserMedia({
        audio: {
          echoCancellation: true,
          noiseSuppression: true,
          autoGainControl: true,
          channelCount: 1, // Mono for voice clarity
          sampleRate: 48000 // Higher sample rate for better quality
        }
      });
      
      streamRef.current = stream;
      
      // Find the best supported audio format
      let mimeType = '';
      const formats = [
        'audio/webm;codecs=opus',
        'audio/webm',
        'audio/mp4;codecs=opus',
        'audio/mp4',
        'audio/ogg;codecs=opus',
        'audio/ogg',
        'audio/wav'
      ];
      
      for (const format of formats) {
        if (MediaRecorder.isTypeSupported(format)) {
          mimeType = format;
          break;
        }
      }
      
      setRecordingFormat(mimeType || 'default format');
      console.log('Using audio format:', mimeType || 'default');
      
      // Create recorder with quality settings
      const recorderOptions = {
        mimeType: mimeType || undefined,
        audioBitsPerSecond: 128000
      };
      
      const recorder = new MediaRecorder(stream, recorderOptions);
      mediaRecorderRef.current = recorder;
      
      // Handle data available event
      recorder.ondataavailable = (e) => {
        if (e.data && e.data.size > 0) {
          chunksRef.current.push(e.data);
        }
      };
      
      // Handle recording stop
      recorder.onstop = async () => {
        if (chunksRef.current.length === 0) {
          console.warn('No audio chunks recorded');
          return;
        }
        
        const audioBlob = new Blob(chunksRef.current, { type: mimeType || 'audio/webm' });
        
        // Check if recordingStartTimeRef is set, if not use current time as fallback
        if (!recordingStartTimeRef.current) {
          console.warn('Recording start time not set, using fallback current time');
          recordingStartTimeRef.current = Date.now() - (elapsedTime * 1000);
        }
        
        // Calculate the actual duration of the recording based on the current time
        const now = Date.now();
        const calculatedDuration = recordingStartTimeRef.current ? now - recordingStartTimeRef.current : 0;
        
        // Compare the timer-based duration with the calculated duration
        console.log('Duration calculation:', {
          timerBasedDuration: elapsedTime * 1000,
          calculatedDuration: calculatedDuration,
          usingCalculated: calculatedDuration > 0
        });
        
        // Use the calculated duration if it's greater than 0, otherwise fall back to the timer-based duration
        const finalDuration = calculatedDuration > 0 ? calculatedDuration : elapsedTime * 1000;
        
        // Only create and report the audio chunk if we have valid data
        if (audioBlob.size > 0 && recordingStartTimeRef.current && onAudioChunk) {
          try {
            // Create the audio chunk with all required properties
            const audioChunk: AudioChunk = {
              blob: audioBlob,
              startTime: recordingStartTimeRef.current,
              duration: finalDuration, // Use the calculated duration in ms
              videoTime: currentVideoTime * 1000, // Convert to ms
              mimeType: mimeType || 'audio/webm', // Store the MIME type explicitly
            };
            
            // Pass the chunk to the parent component
            onAudioChunk(audioChunk);
            
            console.log('Audio chunk recorded successfully:', {
              duration: `${(finalDuration/1000).toFixed(2)}s`,
              size: `${Math.round(audioBlob.size / 1024)} KB`,
              format: mimeType || 'audio/webm',
              videoPosition: `${currentVideoTime.toFixed(2)}s`,
              audioStartTime: new Date(recordingStartTimeRef.current).toLocaleTimeString(),
              calculatedDuration: `${(calculatedDuration/1000).toFixed(2)}s`,
              timerDuration: `${elapsedTime}s`
            });
          } catch (error) {
            console.error('Error processing audio chunk:', error);
            setError(`Failed to process audio recording: ${error instanceof Error ? error.message : String(error)}`);
          }
        } else {
          console.warn('No valid audio data to save', {
            chunks: chunksRef.current.length,
            blobSize: audioBlob.size,
            hasStartTime: !!recordingStartTimeRef.current
          });
        }
        
        // Stop all tracks to release the microphone
        if (streamRef.current) {
          streamRef.current.getTracks().forEach(track => track.stop());
          streamRef.current = null;
        }
        
        // Clear the timer
        if (timerRef.current) {
          clearInterval(timerRef.current);
          timerRef.current = null;
        }
        
        setIsRecordingAudio(false);
      };
      
      // Start the recorder and explicitly set the recording start time
      // CRITICAL: This is where we set the recordingStartTimeRef that will be used
      // to calculate audio chunk duration and is essential for proper audio recording
      const startTime = Date.now();
      recordingStartTimeRef.current = startTime;
      
      console.log('Setting recording start time:', startTime, 'at', new Date(startTime).toLocaleTimeString());
      
      // Start the recorder AFTER setting the start time reference to ensure proper timing
      recorder.start();
      setIsRecordingAudio(true);
      
      // Double-check that the start time was set correctly
      if (!recordingStartTimeRef.current) {
        console.warn('Warning: recordingStartTimeRef.current is null after setting it!');
        // Emergency fallback - set it again
        recordingStartTimeRef.current = Date.now();
        console.log('Emergency reset of recordingStartTimeRef to:', recordingStartTimeRef.current);
      }
      
      // Update elapsed time every second
      timerRef.current = setInterval(() => {
        setElapsedTime(prev => prev + 1);
      }, 1000);
      
    } catch (error) {
      console.error('Error starting recording:', error);
      setError(`Could not start recording: ${error instanceof Error ? error.message : String(error)}`);
    }
  };
  
  // Stop recording
  const stopAudioRecording = () => {
    // First, capture the final duration info before stopping
    // This helps ensure we have valid timing information for the recording
    const recordingEndTime = Date.now();
    const finalElapsedTimeMs = elapsedTime * 1000; // Convert seconds to ms
    
    // If recordingStartTimeRef is missing, try to reconstruct it from elapsed time
    if (!recordingStartTimeRef.current) {
      console.warn('stopAudioRecording: recordingStartTimeRef.current is null, calculating from current time');
      recordingStartTimeRef.current = recordingEndTime - finalElapsedTimeMs;
      console.log('Reconstructed recordingStartTimeRef.current:', recordingStartTimeRef.current);
    }
    
    // Calculate duration for logging
    const calculatedDuration = recordingStartTimeRef.current 
      ? recordingEndTime - recordingStartTimeRef.current 
      : 0;
      
    console.log('Stopping audio recording with duration info:', {
      timerDuration: `${finalElapsedTimeMs}ms`,
      calculatedDuration: `${calculatedDuration}ms`,
      startTime: recordingStartTimeRef.current 
        ? new Date(recordingStartTimeRef.current).toLocaleTimeString() 
        : 'unknown',
      endTime: new Date(recordingEndTime).toLocaleTimeString()
    });
    
    // Now stop the media recorder
    if (mediaRecorderRef.current && mediaRecorderRef.current.state !== 'inactive') {
      try {
        mediaRecorderRef.current.stop();
      } catch (e) {
        console.error('Error stopping media recorder:', e);
      }
    }
    
    // Ensure stream is stopped
    if (streamRef.current) {
      streamRef.current.getTracks().forEach(track => track.stop());
      streamRef.current = null;
    }
    
    // Clear timer
    if (timerRef.current) {
      clearInterval(timerRef.current);
      timerRef.current = null;
    }
    
    setIsRecordingAudio(false);
    // We've already handled any null recordingStartTimeRef, now we can clear it
    recordingStartTimeRef.current = null;
  };
  
  // Format time as MM:SS
  const formatTime = (seconds: number) => {
    const mins = Math.floor(seconds / 60);
    const secs = seconds % 60;
    return `${mins}:${secs < 10 ? '0' : ''}${secs}`;
  };
  
  // Enhanced helper function to convert data URL to Blob
  const dataURLToBlob = (dataURL: string): Blob => {
    try {
      // More comprehensive validation of data URL format
      if (!dataURL || typeof dataURL !== 'string') {
        console.error('Invalid data URL: not a string or empty', typeof dataURL);
        throw new Error('Invalid data URL: not a string or empty');
      }
      
      if (!dataURL.startsWith('data:')) {
        console.error('Invalid data URL format - missing data: prefix');
        console.debug('URL starts with:', dataURL.substring(0, Math.min(20, dataURL.length)));
        throw new Error('Invalid data URL format - missing data: prefix');
      }
      
      // Split the data URL into parts - header and payload
      const parts = dataURL.split(',');
      if (parts.length !== 2) {
        console.error('Invalid data URL format - wrong number of parts:', parts.length);
        throw new Error('Invalid data URL format - wrong number of parts');
      }
      
      // Extract the MIME type with better validation
      const headerPart = parts[0];
      let mime = 'audio/webm'; // Default fallback
      
      // More robust MIME type extraction
      const mimeMatch = headerPart.match(/^data:(.*?)(;base64)?$/);
      if (mimeMatch && mimeMatch[1]) {
        mime = mimeMatch[1];
      } else {
        console.warn('Could not extract MIME type from data URL, using default:', mime);
      }
      
      // Verify that we have a base64 encoded payload
      if (!headerPart.includes(';base64')) {
        console.warn('Data URL does not specify base64 encoding, may cause issues');
      }
      
      // Get base64 data
      const base64Data = parts[1];
      if (!base64Data) {
        console.error('Empty base64 data in data URL');
        throw new Error('Empty base64 data in data URL');
      }
      
      try {
        // Convert base64 to binary with error handling
        const binary = atob(base64Data);
        
        // Create array buffer with proper size validation
        const arrayBuffer = new ArrayBuffer(binary.length);
        const uint8Array = new Uint8Array(arrayBuffer);
        
        // Fill array buffer with binary data
        for (let i = 0; i < binary.length; i++) {
          uint8Array[i] = binary.charCodeAt(i);
        }
        
        // Create and return the blob
        const blob = new Blob([uint8Array], { type: mime });
        
        // Validate the created blob
        if (blob.size === 0) {
          console.warn('Created an empty blob from data URL, possible data corruption');
        } else {
          console.log(`Successfully converted data URL to Blob: size=${blob.size}, type=${blob.type}`);
        }
        
        return blob;
      } catch (binaryError) {
        console.error('Error processing binary data:', binaryError);
        throw new Error(`Failed to process binary data: ${binaryError instanceof Error ? binaryError.message : String(binaryError)}`);
      }
    } catch (error) {
      console.error('Error converting data URL to Blob:', error);
      // Return an empty blob instead of throwing to prevent UI failures
      return new Blob([], { type: 'audio/webm' });
    }
  };

  // Clean up audio players
  const cleanupAudioPlayers = () => {
    audioPlayersRef.current.forEach((player, key) => {
      try {
        player.pause();
        if (player.src && player.src.startsWith('blob:')) {
          URL.revokeObjectURL(player.src);
        }
      } catch (e) {
        console.warn('Error cleaning up audio player:', e);
      }
    });
    
    audioPlayersRef.current.clear();
    playingChunksRef.current.clear();
  };
  
  // Handle audio playback during replay
  useEffect(() => {
    if (!isReplaying) {
      // Clean up audio players when not replaying
      cleanupAudioPlayers();
      return;
    }
    
    // Current video time in milliseconds
    const videoTimeMs = currentVideoTime * 1000;
    
    // Process each chunk to determine if it should play
    replayAudioChunks.forEach((chunk, index) => {
      const chunkId = chunk.startTime; // Use startTime as unique ID
      
      // Log more details about the chunk the first time we see it during replay
      if (replayAudioChunks.length > 0 && index === 0) {
        console.log(`Audio replay details (${replayAudioChunks.length} chunks):`, {
          currentVideoTimeMs: videoTimeMs,
          firstChunk: {
            startTime: new Date(chunk.startTime).toLocaleTimeString(),
            durationMs: chunk.duration,
            videoTimeMs: chunk.videoTime,
            blobType: chunk.blob instanceof Blob ? 'Blob' : typeof chunk.blob,
            blobLength: typeof chunk.blob === 'string' ? chunk.blob.length : (chunk.blob instanceof Blob ? chunk.blob.size : 'unknown'),
            mimeType: chunk.mimeType || 'not specified'
          }
        });
      }
      
      // Check if this chunk should be playing based on video time
      const shouldPlay = 
        videoTimeMs >= chunk.videoTime && 
        videoTimeMs <= chunk.videoTime + chunk.duration + 500; // Add 500ms buffer
      
      // Is this chunk already playing?
      const isPlaying = playingChunksRef.current.has(chunkId);
      
      // Log playback state changes for debugging
      if (shouldPlay !== isPlaying) {
        console.log(`Chunk ${index} playback state changing:`, {
          chunkId,
          shouldPlay,
          isPlaying,
          videoTimeMs,
          chunkVideoTime: chunk.videoTime,
          chunkEndTime: chunk.videoTime + chunk.duration,
          duration: chunk.duration,
          diff: videoTimeMs - chunk.videoTime
        });
      }
      
      // If should play but not playing yet
      if (shouldPlay && !isPlaying) {
        // Create audio element if one doesn't exist for this chunk
        if (!audioPlayersRef.current.has(chunkId)) {
          try {
            // Handle blob data in different formats
            let audioUrl: string;
            
            try {
              if (chunk.url) {
                // Use provided URL if available
                audioUrl = chunk.url;
                console.log(`Chunk ${chunkId}: Using provided URL for playback`);
              } else if (chunk.blob instanceof Blob) {
                // Create URL from Blob
                audioUrl = URL.createObjectURL(chunk.blob);
                console.log(`Chunk ${chunkId}: Created URL from Blob object for audio playback:`, {
                  blobSize: chunk.blob.size,
                  blobType: chunk.blob.type
                });
              } else if (typeof chunk.blob === 'string' && chunk.blob.startsWith('data:')) {
                // Data URL - can either use directly or convert to blob first
                console.log(`Chunk ${chunkId}: Processing data URL for playback, length: ${chunk.blob.length}`);
                
                // Option 1: Convert data URL to blob first (more reliable across browsers)
                const convertedBlob = dataURLToBlob(chunk.blob);
                audioUrl = URL.createObjectURL(convertedBlob);
                console.log(`Chunk ${chunkId}: Converted data URL to Blob URL for playback:`, {
                  blobSize: convertedBlob.size,
                  blobType: convertedBlob.type
                });
                
                // Option 2 (alternative): Use data URL directly
                // audioUrl = chunk.blob;
                // console.log(`Chunk ${chunkId}: Using data URL directly for playback`);
              } else {
                console.error(`Chunk ${chunkId}: Invalid audio blob format:`, typeof chunk.blob);
                if (typeof chunk.blob === 'string') {
                  console.error(`Chunk ${chunkId}: String blob does not start with 'data:' - first 30 chars:`, 
                    chunk.blob.substring(0, 30));
                }
                return;
              }
            } catch (formatError) {
              console.error(`Chunk ${chunkId}: Error processing audio format:`, formatError);
              return;
            }
            
            // Create and configure audio element
            const audio = new Audio(audioUrl);
            
            // Enhanced error and event handling for debugging
            audio.onloadedmetadata = () => {
              console.log(`Chunk ${chunkId}: Audio metadata loaded:`, {
                duration: audio.duration,
                readyState: audio.readyState
              });
            };
            
            audio.oncanplay = () => {
              console.log(`Chunk ${chunkId}: Audio can play now, ready state:`, audio.readyState);
            };
            
            audio.onplay = () => {
              console.log(`Chunk ${chunkId}: Audio playback started`);
            };
            
            audio.onended = () => {
              console.log(`Chunk ${chunkId}: Audio playback ended normally, duration:`, audio.duration);
              playingChunksRef.current.delete(chunkId);
              // Only revoke if we created the URL (not for data URLs or provided URLs)
              if (!chunk.url && chunk.blob instanceof Blob) {
                URL.revokeObjectURL(audioUrl);
              }
            };
            
            audio.onerror = () => {
              const errorDetails = {
                errorCode: audio.error?.code,
                errorMessage: audio.error?.message,
                audioSrc: audio.src.substring(0, 30) + '...',
                audioReadyState: audio.readyState,
                chunkDetails: {
                  blobType: chunk.blob instanceof Blob ? 'Blob' : typeof chunk.blob,
                  mimeType: chunk.mimeType,
                  duration: chunk.duration
                }
              };
              
              console.error(`Chunk ${chunkId}: Error playing audio:`, errorDetails);
              setError(`Audio playback error: ${audio.error?.message || 'Unknown error'}`);
              playingChunksRef.current.delete(chunkId);
              if (!chunk.url && chunk.blob instanceof Blob) {
                URL.revokeObjectURL(audioUrl);
              }
            };
            
            // Store the audio element
            audioPlayersRef.current.set(chunkId, audio);
          } catch (e) {
            console.error('Error creating audio player:', e);
            return;
          }
        }
        
        // Get the audio element
        const audio = audioPlayersRef.current.get(chunkId);
        if (!audio) return;
        
        // Calculate offset in the audio if needed
        const audioOffset = Math.max(0, (videoTimeMs - chunk.videoTime) / 1000);
        if (audioOffset > 0 && audioOffset < chunk.duration / 1000) {
          audio.currentTime = audioOffset;
        }
        
        // Play the audio
        const playPromise = audio.play();
        if (playPromise) {
          playPromise.catch(error => {
            if (error.name === 'NotAllowedError') {
              setError('Audio playback requires user interaction. Click anywhere to enable audio.');
              
              // Set up a one-time click handler to try again
              const clickHandler = () => {
                audio.play().catch(e => console.warn('Still could not play audio after interaction:', e));
                document.removeEventListener('click', clickHandler);
                setError(null);
              };
              document.addEventListener('click', clickHandler, { once: true });
            } else {
              console.error('Error playing audio:', error);
            }
          });
        }
        
        // Mark as playing
        playingChunksRef.current.add(chunkId);
      } 
      // If should not play but is currently playing
      else if (!shouldPlay && isPlaying) {
        const audio = audioPlayersRef.current.get(chunkId);
        if (audio) {
          audio.pause();
          playingChunksRef.current.delete(chunkId);
        }
      }
    });
    
    // Clean up on unmount or when replay status changes
    return () => {
      cleanupAudioPlayers();
    };
  }, [isReplaying, currentVideoTime, replayAudioChunks]);
  
  return (
    <div className="mb-4">
      {error && (
        <div className="text-red-500 text-sm mb-2 p-2 bg-red-50 border border-red-200 rounded">
          {error}
        </div>
      )}
      
      <div className={`flex items-center ${isRecordingAudio ? 'text-red-500' : 'text-gray-500'}`}>
        {isRecordingAudio && (
          <span className="inline-block h-3 w-3 bg-red-500 rounded-full animate-pulse mr-2"></span>
        )}
        <span className="text-sm font-medium">
          {isRecordingAudio 
            ? `Recording audio: ${formatTime(elapsedTime)}` 
            : isRecording 
              ? 'Microphone ready' 
              : 'Audio recording ready'}
        </span>
        {isRecordingAudio && (
          <span className="ml-2 text-xs bg-red-100 text-red-800 px-2 py-1 rounded-full">
            Live
          </span>
        )}
      </div>
      
      {!audioPermissionGranted && (
        <div className="mt-2 text-sm text-amber-600 bg-amber-50 p-2 rounded flex items-center">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="currentColor" className="w-5 h-5 mr-1">
            <path fillRule="evenodd" d="M9.401 3.003c1.155-2 4.043-2 5.197 0l7.355 12.748c1.154 2-.29 4.5-2.599 4.5H4.645c-2.309 0-3.752-2.5-2.598-4.5L9.4 3.003ZM12 8.25a.75.75 0 0 1 .75.75v3.75a.75.75 0 0 1-1.5 0V9a.75.75 0 0 1 .75-.75Zm0 8.25a.75.75 0 1 0 0-1.5.75.75 0 0 0 0 1.5Z" clipRule="evenodd" />
          </svg>
          <span>Microphone access is required for audio recording. Please allow microphone permissions.</span>
        </div>
      )}
      
      {isReplaying && replayAudioChunks && replayAudioChunks.length > 0 && (
        <div className="mt-2 text-xs text-blue-700 bg-blue-50 p-2 rounded">
          <div className="flex items-center">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="currentColor" className="w-5 h-5 mr-1">
              <path d="M13.5 4.06c0-1.336-1.616-2.005-2.56-1.06l-4.5 4.5H4.508c-1.141 0-2.318.664-2.66 1.905A9.76 9.76 0 0 0 1 15c0 1.614.332 3.151.927 4.55.35 1.256 1.518 1.95 2.661 1.95h1.93l4.5 4.5c.945.945 2.561.276 2.561-1.06V4.06ZM18.584 5.106a.75.75 0 0 1 1.06 0c3.808 3.807 3.808 9.98 0 13.788a.75.75 0 0 1-1.06-1.06 8.25 8.25 0 0 0 0-11.668.75.75 0 0 1 0-1.06Z" />
            </svg>
            <span>
              Replaying {replayAudioChunks.length} audio segment{replayAudioChunks.length !== 1 ? 's' : ''}.
              Audio will play automatically at the correct timestamps.
            </span>
          </div>
        </div>
      )}
      
      {isReplaying && (!replayAudioChunks || replayAudioChunks.length === 0) && (
        <div className="mt-2 text-xs text-gray-600 bg-gray-100 p-2 rounded flex items-center">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="currentColor" className="w-5 h-5 mr-1 text-gray-500">
            <path fillRule="evenodd" d="M1.5 4.5a3 3 0 0 1 3-3h1.372c.86 0 1.61.586 1.819 1.42l1.105 4.423a1.875 1.875 0 0 1-.694 1.955l-1.293.97c-.135.101-.164.249-.126.352a11.285 11.285 0 0 0 6.697 6.697c.103.038.25.009.352-.126l.97-1.293a1.875 1.875 0 0 1 1.955-.694l4.423 1.105c.834.209 1.42.959 1.42 1.82V19.5a3 3 0 0 1-3 3h-2.25C8.552 22.5 1.5 15.448 1.5 6.75V4.5Z" clipRule="evenodd" />
          </svg>
          <span>No audio recordings to replay.</span>
        </div>
      )}
      
      <div className="mt-3 flex flex-wrap gap-2">
        {isRecordingAudio && (
          <div className="px-3 py-1 bg-red-100 text-red-800 text-xs rounded-md flex items-center">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="currentColor" className="w-4 h-4 mr-1">
              <path d="M8.25 4.5a3.75 3.75 0 1 1 7.5 0v8.25a3.75 3.75 0 1 1-7.5 0V4.5Z" />
              <path d="M6 10.5a.75.75 0 0 1 .75.75v1.5a5.25 5.25 0 1 0 10.5 0v-1.5a.75.75 0 0 1 1.5 0v1.5a6.751 6.751 0 0 1-6 6.709v2.291h3a.75.75 0 0 1 0 1.5h-7.5a.75.75 0 0 1 0-1.5h3v-2.291a6.751 6.751 0 0 1-6-6.709v-1.5A.75.75 0 0 1 6 10.5Z" />
            </svg>
            <span>Speak to add verbal feedback</span>
          </div>
        )}
        
        {isRecordingAudio && (
          <div className="text-xs text-gray-500 mt-1">
            Recording format: {recordingFormat}
          </div>
        )}
      </div>
    </div>
  );
}
|| END ||


|| START ./src/components/FeedbackOrchestrator.tsx ||

'use client';

import { useState, useRef, useEffect, useCallback, forwardRef, useImperativeHandle } from 'react';
import type { AudioChunk } from './AudioRecorder';
import type { DrawingPath } from './AnnotationCanvas';
import type { RecordedAction } from './VideoPlayer';

/**
 * Main feedback session structure
 */
export interface FeedbackSession {
  id: string;
  videoId: string;
  startTime: number;
  endTime?: number;
  audioTrack: AudioTrack;
  events: TimelineEvent[];
  categories?: Record<string, boolean>;
}

/**
 * Audio track containing all audio recording data
 */
export interface AudioTrack {
  chunks: AudioChunk[];
  totalDuration: number;
}

/**
 * Timeline event - all synchronized to audio timeline
 */
export interface TimelineEvent {
  id: string;
  type: 'video' | 'annotation' | 'marker' | 'category';
  timeOffset: number; // milliseconds from audio start
  duration?: number; // for events with duration
  payload: any; // specific data based on type
}

/**
 * Props for the FeedbackOrchestrator component
 */
interface FeedbackOrchestratorProps {
  // Video component ref
  videoElementRef: React.RefObject<HTMLVideoElement>;
  // Annotation canvas component ref and methods
  canvasRef: React.RefObject<any>;
  drawAnnotation: (path: DrawingPath) => void;
  clearAnnotations: () => void;
  // Audio recording methods and callbacks
  onAudioRecorded: (audioTrack: AudioTrack) => void;
  // Session management callbacks
  onSessionComplete: (session: FeedbackSession) => void;
  // Optional initial session for replay
  initialSession?: FeedbackSession | null;
  // Operation mode
  mode: 'record' | 'replay';
  // Callback for when categories are loaded during replay
  onCategoriesLoaded?: (categories: Record<string, boolean>) => void;
}

/**
 * Feedback Orchestrator Component
 * Coordinates all aspects of recording and playback for a feedback session
 */
const FeedbackOrchestrator = forwardRef<any, FeedbackOrchestratorProps>(({
  videoElementRef,
  canvasRef,
  drawAnnotation,
  clearAnnotations,
  onAudioRecorded,
  onSessionComplete,
  initialSession,
  mode,
  onCategoriesLoaded
}, ref) => {
  // State for tracking active session
  const [isActive, setIsActive] = useState(false);
  const [currentSession, setCurrentSession] = useState<FeedbackSession | null>(initialSession || null);
  const [replayProgress, setReplayProgress] = useState(0);
  const [audioPlayer, setAudioPlayer] = useState<HTMLAudioElement | null>(null);
  
  // Refs for tracking internal state
  const recordingStartTimeRef = useRef<number | null>(null);
  const audioChunksRef = useRef<AudioChunk[]>([]);
  const eventsRef = useRef<TimelineEvent[]>([]);
  const audioRecorderRef = useRef<MediaRecorder | null>(null);
  const streamRef = useRef<MediaStream | null>(null);
  const pendingEventsRef = useRef<TimelineEvent[]>([]);
  const replayTimeoutIdsRef = useRef<number[]>([]);
  
  /**
   * Generate a unique ID
   */
  const generateId = useCallback(() => {
    return Date.now().toString(36) + Math.random().toString(36).substring(2);
  }, []);
  
  /**
   * Initialize a new recording session
   */
  const startRecordingSession = useCallback(async () => {
    if (isActive) return;
    
    try {
      // Start audio recording
      const stream = await navigator.mediaDevices.getUserMedia({
        audio: {
          echoCancellation: true,
          noiseSuppression: true,
          autoGainControl: true,
          channelCount: 1,
          sampleRate: 48000
        }
      });
      
      streamRef.current = stream;
      
      // Determine best audio format
      let mimeType = '';
      const formats = [
        'audio/webm;codecs=opus',
        'audio/webm',
        'audio/mp4;codecs=opus',
        'audio/mp4',
        'audio/ogg;codecs=opus',
        'audio/ogg',
        'audio/wav'
      ];
      
      for (const format of formats) {
        if (MediaRecorder.isTypeSupported(format)) {
          mimeType = format;
          break;
        }
      }
      
      console.log(`Using audio format: ${mimeType || 'default'}`);
      
      // Configure recorder
      const recorder = new MediaRecorder(stream, {
        mimeType: mimeType || undefined,
        audioBitsPerSecond: 128000
      });
      
      audioRecorderRef.current = recorder;
      
      // Set up data handling
      const chunks: Blob[] = [];
      audioChunksRef.current = [];
      eventsRef.current = [];
      
      recorder.ondataavailable = (e) => {
        if (e.data && e.data.size > 0) {
          chunks.push(e.data);
        }
      };
      
      // Handle recording stop
      recorder.onstop = async () => {
        if (chunks.length === 0) return;
        
        // Create the main audio blob
        const audioBlob = new Blob(chunks, { type: mimeType || 'audio/webm' });
        
        // Calculate duration
        const recordingEndTime = Date.now();
        const startTime = recordingStartTimeRef.current || 0;
        const duration = recordingEndTime - startTime;
        
        // Create audio chunk
        const audioChunk: AudioChunk = {
          blob: audioBlob,
          startTime: startTime,
          duration: duration,
          videoTime: 0,
          mimeType: mimeType || 'audio/webm'
        };
        
        // Create and finalize session
        audioChunksRef.current = [audioChunk];
        
        const audioTrack: AudioTrack = {
          chunks: audioChunksRef.current,
          totalDuration: duration
        };
        
        const session: FeedbackSession = {
          id: generateId(),
          videoId: 'video-' + generateId(),
          startTime: startTime,
          endTime: recordingEndTime,
          audioTrack: audioTrack,
          events: eventsRef.current
        };
        
        setCurrentSession(session);
        onAudioRecorded(audioTrack);
        onSessionComplete(session);
        
        // Clean up
        stream.getTracks().forEach(track => track.stop());
        streamRef.current = null;
      };
      
      // Start recording
      const startTime = Date.now();
      recordingStartTimeRef.current = startTime;
      
      console.log(`Starting recording session at ${new Date(startTime).toISOString()}`);
      recorder.start();
      
      // Create the new session object
      const newSession: FeedbackSession = {
        id: generateId(),
        videoId: 'video-' + generateId(),
        startTime: startTime,
        audioTrack: { chunks: [], totalDuration: 0 },
        events: []
      };
      
      setCurrentSession(newSession);
      setIsActive(true);
      
    } catch (error) {
      console.error('Failed to start recording session:', error);
      alert(`Could not start recording: ${error instanceof Error ? error.message : String(error)}`);
    }
  }, [isActive, generateId, onAudioRecorded, onSessionComplete]);
  
  /**
   * End the current recording session
   */
  const endRecordingSession = useCallback(() => {
    if (!isActive) return;
    
    console.log('Ending recording session');
    
    // Stop audio recording
    if (audioRecorderRef.current && audioRecorderRef.current.state !== 'inactive') {
      audioRecorderRef.current.stop();
    }
    
    if (streamRef.current) {
      streamRef.current.getTracks().forEach(track => track.stop());
      streamRef.current = null;
    }
    
    // Reset recording state
    setIsActive(false);
    recordingStartTimeRef.current = null;
    
    // Note: Video reset and annotation clearing are now handled by VideoPlayerWrapper
  }, [isActive]);
  
  /**
   * Record a timeline event during recording
   */
  const recordEvent = useCallback((type: 'video' | 'annotation' | 'marker', payload: any, duration?: number) => {
    if (!isActive || !recordingStartTimeRef.current) return;
    
    const now = Date.now();
    const timeOffset = now - recordingStartTimeRef.current;
    
    const event: TimelineEvent = {
      id: generateId(),
      type,
      timeOffset,
      duration,
      payload
    };
    
    eventsRef.current.push(event);
    console.log(`Recorded ${type} event at ${timeOffset}ms:`, payload);
    
    return event;
  }, [isActive, generateId]);
  
  /**
   * Handle video events (play, pause, seek, etc.)
   */
  const handleVideoEvent = useCallback((action: string, details?: any) => {
    return recordEvent('video', { action, ...details });
  }, [recordEvent]);
  
  /**
   * Handle annotation events (drawing, clearing)
   */
  const handleAnnotationEvent = useCallback((action: string, payload?: any) => {
    console.log(`Recording annotation event: ${action}`, payload);
    
    if (action === 'clear') {
      // Handle clear event with more timing information
      // payload could be the whole RecordedAction or a DrawingPath
      if (payload && payload.type === 'annotation' && payload.details) {
        console.log('Recording clear event with detailed timing information:', payload);
        
        // Use clearVideoTime from details if available
        const clearVideoTime = payload.details.clearVideoTime || (payload.videoTime ? payload.videoTime * 1000 : undefined);
        const clearTimestamp = payload.details.clearTimestamp || payload.timestamp;
        
        // Pass the timing information to ensure precise replay
        const event = recordEvent('annotation', { 
          action: 'clear',
          clearVideoTime: clearVideoTime,
          clearTimestamp: clearTimestamp,
          details: payload.details,
          timestamp: Date.now()
        });
        
        if (event) {
          console.log(`Clear event recorded with ID: ${event.id}`, {
            timeOffset: event.timeOffset,
            clearVideoTime: clearVideoTime,
            clearTimestamp: clearTimestamp,
            eventCount: eventsRef.current.length
          });
        }
        
        return event;
      } else {
        // Standard clear without extra info
        console.log('Recording standard clear event');
        return recordEvent('annotation', { 
          action: 'clear',
          clearVideoTime: videoElementRef.current ? videoElementRef.current.currentTime * 1000 : undefined,
          clearTimestamp: Date.now(),
          timestamp: Date.now()
        });
      }
    } else if (action === 'draw') {
      // Drawing event with path information
      const path = payload as DrawingPath;
      console.log(`Recording draw event`, {
        hasPath: !!path,
        pointsCount: path?.points?.length || 0,
        color: path?.color,
        width: path?.width,
        id: path?.id,
        visible: path?.visible
      });
      
      // Ensure path has visibility properties
      const enhancedPath = {
        ...path,
        visible: path.visible !== undefined ? path.visible : true,
        id: path.id || `annotation-${Date.now()}-${Math.random().toString(36).substr(2, 9)}`
      };
      
      // Record the event in the timeline
      const event = recordEvent('annotation', { action, path: enhancedPath });
      
      // For debugging during development
      if (event) {
        console.log(`Draw event recorded with ID: ${event.id}`, {
          timeOffset: event.timeOffset,
          eventCount: eventsRef.current.length
        });
      } else {
        console.warn('Failed to record draw event - recording may not be active');
      }
      
      return event;
    } else {
      // Other annotation events
      console.log(`Recording other annotation event: ${action}`);
      return recordEvent('annotation', { action, payload });
    }
  }, [recordEvent, videoElementRef]);
  
  /**
   * Add a marker at the current time
   */
  const addMarker = useCallback((text: string) => {
    return recordEvent('marker', { text });
  }, [recordEvent]);
  
  /**
   * Record a category change
   */
  const handleCategoryEvent = useCallback((category: string, checked: boolean) => {
    console.log(`Recording category change: ${category} = ${checked}`);
    return recordEvent('category', { category, checked });
  }, [recordEvent]);
  
  /**
   * Start replay of a feedback session
   */
  const startReplay = useCallback(() => {
    if (!currentSession || isActive) return;
    
    setIsActive(true);
    setReplayProgress(0);
    
    // Clear any previous timeouts
    replayTimeoutIdsRef.current.forEach(id => window.clearTimeout(id));
    replayTimeoutIdsRef.current = [];
    
    // Clear any pending events
    pendingEventsRef.current = [];
    
    // Create a copy of events to process
    pendingEventsRef.current = [...currentSession.events].sort((a, b) => a.timeOffset - b.timeOffset);
    
    // Create audio player for the main timeline
    if (currentSession.audioTrack.chunks.length > 0) {
      const mainAudioChunk = currentSession.audioTrack.chunks[0];
      
      if (!mainAudioChunk.blob) {
        console.error('No valid audio blob in the session');
        return;
      }
      
      try {
        let audioUrl: string;
        
        if (mainAudioChunk.blob instanceof Blob) {
          audioUrl = URL.createObjectURL(mainAudioChunk.blob);
        } else if (typeof mainAudioChunk.blob === 'string' && mainAudioChunk.blob.startsWith('data:')) {
          // Handle data URL
          // This could be improved with proper conversion
          const parts = mainAudioChunk.blob.split(',');
          if (parts.length !== 2) {
            throw new Error('Invalid data URL format');
          }
          
          const mimeMatch = parts[0].match(/:(.*?);/);
          const mime = mimeMatch ? mimeMatch[1] : mainAudioChunk.mimeType || 'audio/webm';
          
          const binary = atob(parts[1]);
          const arrayBuffer = new ArrayBuffer(binary.length);
          const uint8Array = new Uint8Array(arrayBuffer);
          
          for (let i = 0; i < binary.length; i++) {
            uint8Array[i] = binary.charCodeAt(i);
          }
          
          const blob = new Blob([uint8Array], { type: mime });
          audioUrl = URL.createObjectURL(blob);
        } else {
          throw new Error('Unsupported audio format');
        }
        
        const audio = new Audio(audioUrl);
        
        // Set up audio events
        audio.onplay = () => {
          console.log('Audio playback started');
        };
        
        audio.ontimeupdate = () => {
          const currentTime = audio.currentTime * 1000; // Convert to ms
          const totalDuration = currentSession.audioTrack.totalDuration;
          setReplayProgress((currentTime / totalDuration) * 100);
          
          // Process any pending events that should occur by this time
          processPendingEvents(currentTime);
        };
        
        audio.onended = () => {
          console.log('Audio playback complete, cleaning up and resetting UI...');
          completeReplay();
          // Ensure the active state in parent component is also updated
          setIsActive(false);
        };
        
        audio.onerror = (e) => {
          console.error('Audio playback error:', audio.error);
        };
        
        setAudioPlayer(audio);
        
        // Start playback
        audio.play().catch(error => {
          console.error('Failed to start audio playback:', error);
          alert('Failed to start audio playback. Try clicking on the page first.');
        });
      } catch (error) {
        console.error('Error creating audio player for replay:', error);
      }
    } else {
      console.warn('No audio track found for replay. Using simulated timeline.');
      
      // Simulate timeline with setTimeout if no audio
      const totalDuration = currentSession.events.length > 0 
        ? Math.max(...currentSession.events.map(e => e.timeOffset)) + 5000
        : 30000; // Default 30s if no events
      
      let elapsed = 0;
      const interval = 100; // 100ms updates
      
      const timelineInterval = window.setInterval(() => {
        elapsed += interval;
        setReplayProgress((elapsed / totalDuration) * 100);
        
        processPendingEvents(elapsed);
        
        if (elapsed >= totalDuration) {
          clearInterval(timelineInterval);
          console.log('Simulated timeline complete, cleaning up and resetting...');
          completeReplay();
          // Ensure the active state is updated
          setIsActive(false);
        }
      }, interval);
      
      replayTimeoutIdsRef.current.push(timelineInterval as unknown as number);
    }
  }, [currentSession, isActive]);
  
  // Forward declaration of executeEvent to avoid reference-before-initialization error
  const executeEventRef = useRef<(event: TimelineEvent) => void>();
  
  // Keep track of the last executed event to help with timing diagnostics
  const lastExecutedEventRef = useRef<{type: string, action?: string, time: number} | null>(null);
  
  /**
   * Process pending events based on current timeline position
   */
  const processPendingEvents = useCallback((currentTimeMs: number) => {
    // Nothing to process
    if (pendingEventsRef.current.length === 0) return;
    
    // Find all events that should be executed by now using a small lookahead buffer
    // This ensures we process events slightly before they're needed to prevent the delay
    const lookaheadBuffer = 50; // ms - process events slightly ahead of time
    const processingTime = currentTimeMs + lookaheadBuffer;
    
    const eventsToExecute: TimelineEvent[] = [];
    const remainingEvents: TimelineEvent[] = [];
    
    pendingEventsRef.current.forEach(event => {
      if (event.timeOffset <= processingTime) {
        eventsToExecute.push(event);
      } else {
        remainingEvents.push(event);
      }
    });
    
    // Skip logging unless we have events to execute
    if (eventsToExecute.length > 0) {
      // Pre-sort events by timeOffset to ensure proper execution order
      eventsToExecute.sort((a, b) => a.timeOffset - b.timeOffset);
      
      // Split events by type and identify clear events first (they need priority)
      const clearEvents = eventsToExecute.filter(e => 
        e.type === 'annotation' && e.payload.action === 'clear'
      );
      
      const drawEvents = eventsToExecute.filter(e => 
        e.type === 'annotation' && e.payload.action === 'draw'
      );
      
      const otherEvents = eventsToExecute.filter(e => 
        !(e.type === 'annotation' && (e.payload.action === 'clear' || e.payload.action === 'draw'))
      );
      
      // Log summary of events being executed
      console.log(`Processing ${eventsToExecute.length} events at time ${currentTimeMs}ms:`, {
        clearEvents: clearEvents.length,
        drawEvents: drawEvents.length,
        otherEvents: otherEvents.length,
        remainingCount: remainingEvents.length,
        nextEventAt: remainingEvents.length > 0 ? remainingEvents[0].timeOffset : 'none'
      });
      
      // Update the pending events reference immediately to prevent double-processing
      pendingEventsRef.current = remainingEvents;
      
      // Execute in the specific order: clear events first, then draw events, then others
      // This ensures that clear operations happen before any drawing in the same time window
      
      // 1. Execute clear events first, adjusting their timing if needed
      clearEvents.forEach(event => {
        // Update timing diagnostics before execution
        const lastEvent = lastExecutedEventRef.current;
        const timingDetails = lastEvent ? ` (${currentTimeMs - lastEvent.time}ms after ${lastEvent.type}${lastEvent.action ? ' ' + lastEvent.action : ''})` : '';
        
        console.log(`Executing CLEAR event at ${event.timeOffset}ms${timingDetails}`);
        
        if (executeEventRef.current) {
          // Execute the event
          executeEventRef.current(event);
          
          // Record this execution
          lastExecutedEventRef.current = {
            type: 'annotation',
            action: 'clear',
            time: currentTimeMs
          };
        }
      });
      
      // 2. Execute draw events with forced delay after any clear events
      // This creates a small gap between clear and draw operations
      setTimeout(() => {
        drawEvents.forEach(event => {
          // Update timing diagnostics before execution
          const lastEvent = lastExecutedEventRef.current;
          const timingDetails = lastEvent ? ` (${currentTimeMs - lastEvent.time}ms after ${lastEvent.type}${lastEvent.action ? ' ' + lastEvent.action : ''})` : '';
          
          console.log(`Executing DRAW event at ${event.timeOffset}ms${timingDetails}`);
          
          if (executeEventRef.current) {
            // Execute the draw event
            executeEventRef.current(event);
            
            // Record this execution
            lastExecutedEventRef.current = {
              type: 'annotation',
              action: 'draw',
              time: currentTimeMs
            };
          }
        });
      }, clearEvents.length > 0 ? 15 : 0); // Add a 15ms delay if we had clear events
      
      // 3. Execute other non-category events
      const otherNonCategoryEvents = otherEvents.filter(e => e.type !== 'category');
      otherNonCategoryEvents.forEach(event => {
        if (executeEventRef.current) {
          executeEventRef.current(event);
        }
      });
      
      // 4. Handle category events separately with delays to avoid state batching
      const categoryEvents = eventsToExecute.filter(e => e.type === 'category');
      if (categoryEvents.length > 0) {
        console.log(`Processing ${categoryEvents.length} category events with delays`);
        
        // Process category events with a delay between each one
        categoryEvents.forEach((event, index) => {
          setTimeout(() => {
            if (executeEventRef.current) {
              executeEventRef.current(event);
            }
          }, index * 50); // 50ms between each category event
        });
      }
    } else {
      // Still update the pending events reference
      pendingEventsRef.current = remainingEvents;
    }
  }, []);
  
  /**
   * Execute a timeline event
   */
  const executeEvent = useCallback((event: TimelineEvent) => {
    console.log(`Executing ${event.type} event:`, event.payload);
    
    switch (event.type) {
      case 'video':
        if (videoElementRef.current) {
          const video = videoElementRef.current;
          const payload = event.payload;
          
          switch (payload.action) {
            case 'play':
              video.play().catch(err => console.warn('Failed to play video:', err));
              break;
            case 'pause':
              video.pause();
              break;
            case 'seek':
              if (payload.to !== undefined) {
                video.currentTime = payload.to;
              }
              break;
            case 'playbackRate':
              if (payload.to !== undefined) {
                video.playbackRate = payload.to;
              }
              break;
          }
        }
        break;
      case 'annotation':
        if (drawAnnotation && clearAnnotations) {
          const payload = event.payload;
          
          switch (payload.action) {
            case 'draw':
              if (payload.path) {
                try {
                  // Apply a very small positive offset to ensure draw events happen
                  // slightly after clear events in the same time window
                  // This ensures proper sequencing of clear->draw operations
                  const adjustedTimeOffset = event.timeOffset + 5; // 5ms later
                  
                  // Create a copy of the path with enhanced timing information
                  const pathWithTiming = { 
                    ...payload.path,
                    // Use adjusted timeOffset to ensure proper sequencing
                    timeOffset: adjustedTimeOffset,
                    // If videoTime isn't already set, set it to the event's timeOffset
                    // This helps with filtering in the AnnotationCanvas component
                    videoTime: payload.path.videoTime || adjustedTimeOffset,
                    // Always ensure visibility is set
                    visible: true,
                    // Add or preserve ID
                    id: payload.path.id || `draw-${event.id || Date.now()}`
                  };
                  
                  console.log(`Drawing annotation at adjusted time ${adjustedTimeOffset}ms (original: ${event.timeOffset}ms)`);
                  drawAnnotation(pathWithTiming);
                } catch (error) {
                  console.error('Error during annotation drawing:', error);
                }
              }
              break;
            case 'clear':
              try {
                // Extract clearing info from event payload if available
                const clearVideoTime = payload.clearVideoTime || 
                  (videoElementRef.current ? videoElementRef.current.currentTime * 1000 : event.timeOffset);
                
                // Apply a small negative offset to ensure clear events happen
                // before any draw events that might have the same timeOffset
                const adjustedClearTime = clearVideoTime - 10; // 10ms earlier
                
                console.log(`Executing clear event at timeOffset: ${event.timeOffset}ms, ` +
                  `videoTime: ${adjustedClearTime}ms (adjusted from ${clearVideoTime}ms)`);
                
                // Create a complete clear event object with consistent metadata
                const clearEvent: DrawingPath = {
                  id: `clear-${event.id || Date.now()}`,
                  points: [], // Empty points array
                  color: 'transparent',
                  width: 0,
                  timestamp: Date.now(),
                  videoTime: adjustedClearTime, // Use adjusted time in ms
                  timeOffset: event.timeOffset - 10, // Also adjust the time offset
                  isClearEvent: true,
                  visible: false, // Not a visible annotation itself
                  hiddenAt: adjustedClearTime
                };
                
                // Add the clear event to be tracked in the annotation timeline
                // This ensures it will be found during clear event detection
                drawAnnotation(clearEvent);
                
                // Immediately clear the canvas with the exact timestamp from the event
                // This ensures it uses the original recording time for accuracy
                const clearResult = clearAnnotations();
                
                console.log(`Clear event executed successfully:`, {
                  id: clearEvent.id,
                  videoTime: clearVideoTime,
                  timeOffset: event.timeOffset,
                  result: clearResult
                });
              } catch (error) {
                console.error('Error during annotation clearing:', error);
              }
              break;
          }
        }
        break;
      case 'marker':
        // Could display a marker UI
        console.log('Marker:', event.payload.text);
        break;
      case 'category':
        // We're now handling all categories at the start of replay instead of during timeline events
        console.log(`Skipping category event during replay: ${event.payload?.category} = ${event.payload?.checked}`);
        break;
    }
  }, [videoElementRef, drawAnnotation, clearAnnotations, onCategoriesLoaded]);
  
  // Update the executeEventRef whenever executeEvent changes
  useEffect(() => {
    executeEventRef.current = executeEvent;
  }, [executeEvent]);
  
  /**
   * Complete the replay process
   */
  const completeReplay = useCallback(() => {
    // Clean up audio player
    if (audioPlayer) {
      audioPlayer.pause();
      if (audioPlayer.src.startsWith('blob:')) {
        URL.revokeObjectURL(audioPlayer.src);
      }
      setAudioPlayer(null);
    }
    
    // Clear any timeouts
    replayTimeoutIdsRef.current.forEach(id => window.clearTimeout(id));
    replayTimeoutIdsRef.current = [];
    
    // Reset video position to the beginning
    if (videoElementRef.current) {
      console.log('Replay complete: resetting video position to start');
      videoElementRef.current.currentTime = 0;
      
      // If it's playing, pause it
      if (!videoElementRef.current.paused) {
        videoElementRef.current.pause();
      }
    }
    
    // Clear all annotations when replay is done
    try {
      console.log('Replay complete: clearing annotations');
      clearAnnotations();
    } catch (error) {
      console.error('Error clearing annotations on replay completion:', error);
    }
    
    setIsActive(false);
    setReplayProgress(100);
  }, [audioPlayer, videoElementRef, clearAnnotations]);
  
  /**
   * Stop the current replay
   */
  const stopReplay = useCallback(() => {
    if (!isActive) return;
    
    console.log('Stopping replay session');
    
    // Clean up audio player
    if (audioPlayer) {
      audioPlayer.pause();
      if (audioPlayer.src.startsWith('blob:')) {
        URL.revokeObjectURL(audioPlayer.src);
      }
      setAudioPlayer(null);
    }
    
    // Clear any timeouts
    replayTimeoutIdsRef.current.forEach(id => window.clearTimeout(id));
    replayTimeoutIdsRef.current = [];
    
    // Reset replay state
    setIsActive(false);
    setReplayProgress(0);
    
    // Note: Video reset and annotation clearing are now handled by VideoPlayerWrapper
  }, [isActive, audioPlayer]);
  
  /**
   * Load a session for replay
   */
  const loadSession = useCallback((session: FeedbackSession) => {
    console.log('Loading session for replay, session ID:', session.id);
    console.log('Total events in session:', session.events.length);
    console.log('All event types:', session.events.map(e => e.type));
    
    setCurrentSession(session);
    
    // Detailed log of all events to debug
    session.events.forEach((event, index) => {
      console.log(`Event ${index}: type=${event.type}, timeOffset=${event.timeOffset}, payload=`, event.payload);
    });
    
    // Try direct approach first: check if session has categories property
    if (session.categories && Object.keys(session.categories).length > 0) {
      console.log('Session has categories property directly:', session.categories);
      
      if (onCategoriesLoaded) {
        console.log('Using categories directly from session:', session.categories);
        
        // Use the categories directly since they're already in the right format
        setTimeout(() => {
          onCategoriesLoaded(session.categories);
        }, 100);
        
        // We've handled categories directly, so we can return
        return;
      }
    }
    
    // Fallback to category events if no direct categories property
    console.log('No direct categories property, looking for category events...');
    
    // Find all category events, regardless of checked status first
    const allCategoryEvents = session.events.filter(event => event.type === 'category');
    console.log(`Found ${allCategoryEvents.length} total category events in session`);
    
    // Use the most recent state of each category (last event for each category determines if it's checked)
    const categoriesState: Record<string, boolean> = {};
    
    // Process events in chronological order so we end up with the final state
    allCategoryEvents.forEach(event => {
      if (event.payload?.category) {
        categoriesState[event.payload.category] = !!event.payload.checked;
        console.log(`Category ${event.payload.category} = ${event.payload.checked}`);
      }
    });
    
    // Find which categories are checked in the final state
    const checkedCategories = Object.entries(categoriesState)
      .filter(([_, isChecked]) => isChecked)
      .map(([category]) => category);
    
    console.log(`Final state has ${checkedCategories.length} checked categories:`, checkedCategories);
    
    // If we have categories and a callback, send all the categories at once
    if (Object.keys(categoriesState).length > 0 && onCategoriesLoaded) {
      console.log('Final categories state to send:', categoriesState);
      
      // Notify parent about all categories - use setTimeout to ensure it happens after component mount
      setTimeout(() => {
        if (onCategoriesLoaded) {
          console.log('Calling onCategoriesLoaded with:', categoriesState);
          onCategoriesLoaded(categoriesState);
        }
      }, 100);
    } else {
      console.warn('No category events found in the session or callback missing', {
        allCategoryEvents: allCategoryEvents.length,
        categoriesState: Object.keys(categoriesState).length,
        hasCallback: !!onCategoriesLoaded
      });
    }
  }, [onCategoriesLoaded]);
  
  /**
   * Clean up resources when component unmounts
   */
  useEffect(() => {
    return () => {
      // Stop recording if active
      if (audioRecorderRef.current && audioRecorderRef.current.state !== 'inactive') {
        audioRecorderRef.current.stop();
      }
      
      if (streamRef.current) {
        streamRef.current.getTracks().forEach(track => track.stop());
      }
      
      // Clean up audio player
      if (audioPlayer) {
        audioPlayer.pause();
        if (audioPlayer.src.startsWith('blob:')) {
          URL.revokeObjectURL(audioPlayer.src);
        }
      }
      
      // Clear any timeouts
      replayTimeoutIdsRef.current.forEach(id => window.clearTimeout(id));
    };
  }, [audioPlayer]);
  
  // Expose imperative methods to parent component using the ref
  useImperativeHandle(ref, () => ({
    // Status
    isActive,
    currentSession,
    replayProgress,
    
    // Recording methods
    startRecordingSession,
    endRecordingSession,
    handleVideoEvent,
    handleAnnotationEvent,
    addMarker,
    handleCategoryEvent,
    
    // Replay methods
    startReplay,
    stopReplay,
    completeReplay,
    loadSession,
  }));
  
  // Return null as this is a controller component without UI
  return null;
});

FeedbackOrchestrator.displayName = 'FeedbackOrchestrator';

export default FeedbackOrchestrator;
|| END ||


