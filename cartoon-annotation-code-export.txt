# Cartoon Annotation Tool - Complete Source Code Export
# Generated on Tue Apr  8 06:42:44 PDT 2025
# This file contains all source code from the project

|| START ./AUTHENTICATION.md ||

# Authentication Implementation

## Overview
A complete username/password authentication system has been implemented for the Cartoon Annotation Tool using NextAuth.js. This document outlines what was implemented and provides instructions for setup and testing.

## Implemented Features

1. **User Authentication**
   - Username/password login with secure password hashing via bcrypt
   - Session management with JWT tokens
   - Protected routes with middleware

2. **Core Authentication Components**
   - NextAuth.js API route for authentication
   - User management in Cosmos DB
   - Login and registration pages
   - Context providers for authentication state

3. **Protection and Authorization**
   - Middleware to protect routes
   - Auth context for managing user state
   - Custom hooks for requiring authentication

## File Structure

- `app/api/auth/[...nextauth]/route.ts` - NextAuth.js configuration
- `app/api/auth/register/route.ts` - API route for user registration
- `app/auth/signin/page.tsx` - Login page
- `app/auth/register/page.tsx` - Registration page
- `src/contexts/AuthContext.tsx` - Authentication context provider
- `src/lib/auth.ts` - Authentication utility functions
- `middleware.ts` - Route protection middleware
- `scripts/seed-users.js` - Script to seed initial admin user

## Setup Instructions

1. **Environment Variables**
   
   A `.env.local` file has been created with the following variables:
   ```
   NEXTAUTH_URL=http://localhost:3000
   NEXTAUTH_SECRET=your_random_secret_replace_this_in_production
   ```

   For production, replace the NEXTAUTH_SECRET with a secure random string.

2. **Database Setup**

   Run the following command to create an initial admin user:
   ```
   npm run seed-users
   ```

   This creates a user with:
   - Email: admin@example.com
   - Password: password123

   **Important:** Change these credentials in production!

3. **Access Protected Routes**

   After signing in, you'll be able to access protected routes like:
   - The homepage (/)
   - Inbox (/inbox)

## Testing the Authentication

1. **Start the Development Server**
   ```
   npm run dev
   ```

2. **Access the Application**
   - Navigate to http://localhost:3000
   - You should be redirected to the login page

3. **Login with Admin Credentials**
   - Email: admin@example.com
   - Password: password123
   - After login, you will be redirected to the inbox page

4. **Register a New User**
   - Navigate to http://localhost:3000/auth/register
   - Fill in the registration form to create a new user

## Customization

- **Session Duration**: You can customize the session duration in `app/api/auth/[...nextauth]/route.ts`
- **Protected Routes**: Edit the public routes list in `middleware.ts` to change which routes are accessible without authentication

## Security Considerations

- The NEXTAUTH_SECRET should be a strong, random value in production
- Password hashing is handled by bcrypt with a cost factor of 12
- User input is validated on both client and server sides
- JWT tokens are used for session management

---

For any issues or questions, check the [NextAuth.js documentation](https://next-auth.js.org/getting-started/introduction) for more details.
|| END ||


|| START ./middleware.ts ||

import { NextResponse } from 'next/server';
import type { NextRequest } from 'next/server';
import { getToken } from 'next-auth/jwt';

// This function can be marked `async` if using `await` inside
export async function middleware(request: NextRequest) {
  const { pathname } = request.nextUrl;
  
  // Define public routes that don't require authentication
  const publicRoutes = [
    '/auth/signin',
    '/auth/register',
    '/_next',
    '/api/auth',
    '/favicon.ico'
  ];
  
  // Check if the pathname is a public route
  const isPublicRoute = publicRoutes.some(route => pathname.startsWith(route));
  
  // If it's a public route, allow the request
  if (isPublicRoute) {
    return NextResponse.next();
  }

  // Get the session token
  const token = await getToken({
    req: request,
    secret: process.env.NEXTAUTH_SECRET
  });

  // If there's no token and the route is not public, redirect to sign in
  if (!token) {
    const url = new URL('/auth/signin', request.url);
    url.searchParams.set('callbackUrl', encodeURI(request.url));
    return NextResponse.redirect(url);
  }

  // If there is a token, allow the request
  return NextResponse.next();
}

// See "Matching Paths" below to learn more
export const config = {
  matcher: [
    /*
     * Match all request paths except for the ones starting with:
     * - api/auth (API routes)
     * - _next/static (static files)
     * - _next/image (image optimization files)
     * - favicon.ico (favicon file)
     */
    '/((?!api/auth|_next/static|_next/image|favicon.ico).*)',
  ],
};
|| END ||


|| START ./app/inbox/page.tsx ||

'use client';

import { useState, useEffect } from 'react';
import Link from 'next/link';
// Removed Image import

// Define the video interface
interface Video {
  id: string;
  title: string;
  description: string;
  thumbnailUrl: string;
  videoUrl: string;
  duration: string;
  dateAdded: string;
  status: 'Not Started' | 'Completed' | 'Archived';
  tags: string[];
  metrics: Record<string, string | number>;
}

export default function InboxPage() {
  const [videos, setVideos] = useState<Video[]>([]);
  const [filteredVideos, setFilteredVideos] = useState<Video[]>([]);
  const [loading, setLoading] = useState(true);
  const [error, setError] = useState('');
  const [statusFilter, setStatusFilter] = useState<string>('All');
  const [searchTerm, setSearchTerm] = useState('');

  // Fetch videos data from Cosmos DB API
  useEffect(() => {
    const fetchVideos = async () => {
      try {
        setLoading(true);
        const url = statusFilter !== 'All' 
          ? `/api/videos?status=${encodeURIComponent(statusFilter)}`
          : '/api/videos';
          
        const response = await fetch(url);
        
        if (!response.ok) {
          throw new Error(`Failed to fetch videos: ${response.status} ${response.statusText}`);
        }
        
        const data = await response.json();
        setVideos(data);
        
        // Apply search filter separately
        if (searchTerm) {
          filterVideosBySearch(data, searchTerm);
        } else {
          setFilteredVideos(data);
        }
        
        setLoading(false);
      } catch (err) {
        console.error('Error fetching videos:', err);
        setError(err instanceof Error ? err.message : 'Failed to fetch videos');
        setLoading(false);
      }
    };

    fetchVideos();
  }, [statusFilter]); // Re-fetch when status filter changes
  
  // Function to filter videos by search term
  const filterVideosBySearch = (videosToFilter: Video[], term: string) => {
    if (!term.trim()) {
      setFilteredVideos(videosToFilter);
      return;
    }
    
    const lowerCaseSearch = term.toLowerCase();
    const results = videosToFilter.filter(video => 
      video.title.toLowerCase().includes(lowerCaseSearch) || 
      video.description.toLowerCase().includes(lowerCaseSearch) ||
      video.tags.some(tag => tag.toLowerCase().includes(lowerCaseSearch))
    );
    
    setFilteredVideos(results);
  };

  // Apply only search filter when search term changes
  // (Status filter is handled in the API call)
  useEffect(() => {
    // Don't reapply if we don't have videos yet
    if (videos.length === 0) return;
    
    // Apply search filter locally
    filterVideosBySearch(videos, searchTerm);
  }, [searchTerm, videos]);

  // Function to get appropriate status color
  const getStatusColor = (status: string) => {
    switch (status) {
      case 'Not Started':
        return 'bg-gray-200 text-gray-800';
      case 'Completed':
        return 'bg-green-100 text-green-800';
      case 'Archived':
        return 'bg-blue-100 text-blue-800';
      default:
        return 'bg-gray-200 text-gray-800';
    }
  };

  // Function to format date
  const formatDate = (dateString: string) => {
    const options: Intl.DateTimeFormatOptions = { year: 'numeric', month: 'short', day: 'numeric' };
    return new Date(dateString).toLocaleDateString(undefined, options);
  };

  if (loading) {
    return (
      <div className="flex justify-center items-center min-h-screen">
        <div className="animate-spin rounded-full h-12 w-12 border-t-2 border-b-2 border-gray-900"></div>
      </div>
    );
  }

  if (error) {
    return (
      <div className="flex justify-center items-center min-h-screen">
        <div className="bg-red-100 text-red-800 p-4 rounded-lg">
          <p className="font-bold">Error:</p>
          <p>{error}</p>
        </div>
      </div>
    );
  }

  return (
    <div className="container mx-auto p-4 max-w-6xl">
      <div className="mb-8">
        <h1 className="text-3xl font-bold">Inbox</h1>
      </div>

      {/* Filters and search */}
      <div className="mb-6 flex flex-col sm:flex-row gap-4">
        <div className="flex-1">
          <input
            type="text"
            placeholder="Search sessions..."
            className="w-full px-4 py-2 border rounded-md"
            value={searchTerm}
            onChange={(e) => setSearchTerm(e.target.value)}
          />
        </div>
        <div className="sm:w-48">
          <select
            className="w-full px-4 py-2 border rounded-md"
            value={statusFilter}
            onChange={(e) => setStatusFilter(e.target.value)}
          >
            <option value="All">All Statuses</option>
            <option value="Not Started">Not Started</option>
            <option value="Completed">Completed</option>
            <option value="Archived">Archived</option>
          </select>
        </div>
      </div>

      {/* Videos list */}
      {filteredVideos.length === 0 ? (
        <div className="bg-gray-100 rounded-lg p-8 text-center">
          <p className="text-lg text-gray-600">No sessions found matching your filters.</p>
        </div>
      ) : (
        <div className="overflow-x-auto bg-white rounded-lg shadow-md">
          <table className="min-w-full divide-y divide-gray-200">
            <thead className="bg-gray-50">
              <tr>
                <th scope="col" className="px-6 py-3 text-left text-xs font-medium text-gray-500 uppercase tracking-wider">
                  Title
                </th>
                <th scope="col" className="px-6 py-3 text-left text-xs font-medium text-gray-500 uppercase tracking-wider">
                  Date Added
                </th>
                <th scope="col" className="px-6 py-3 text-left text-xs font-medium text-gray-500 uppercase tracking-wider">
                  Duration
                </th>
                <th scope="col" className="px-6 py-3 text-left text-xs font-medium text-gray-500 uppercase tracking-wider">
                  Status
                </th>
                <th scope="col" className="px-6 py-3 text-left text-xs font-medium text-gray-500 uppercase tracking-wider">
                  Tags
                </th>
                <th scope="col" className="px-6 py-3 text-right text-xs font-medium text-gray-500 uppercase tracking-wider">
                  Actions
                </th>
              </tr>
            </thead>
            <tbody className="bg-white divide-y divide-gray-200">
              {filteredVideos.map((video) => (
                <tr key={video.id} className="hover:bg-gray-50">
                  <td className="px-6 py-4 whitespace-nowrap">
                    <div>
                      <div className="text-sm font-medium text-gray-900">{video.title}</div>
                      <div className="text-sm text-gray-500 truncate max-w-xs">{video.description}</div>
                    </div>
                  </td>
                  <td className="px-6 py-4 whitespace-nowrap text-sm text-gray-500">
                    {formatDate(video.dateAdded)}
                  </td>
                  <td className="px-6 py-4 whitespace-nowrap text-sm text-gray-500">
                    {video.duration}
                  </td>
                  <td className="px-6 py-4 whitespace-nowrap">
                    <span className={`px-2 inline-flex text-xs leading-5 font-semibold rounded-full ${getStatusColor(video.status)}`}>
                      {video.status}
                    </span>
                  </td>
                  <td className="px-6 py-4 whitespace-nowrap text-sm text-gray-500">
                    <div className="flex flex-wrap gap-1">
                      {video.tags.slice(0, 2).map((tag, index) => (
                        <span key={index} className="inline-flex items-center px-2 py-0.5 rounded text-xs font-medium bg-gray-100 text-gray-800">
                          {tag}
                        </span>
                      ))}
                      {video.tags.length > 2 && (
                        <span className="inline-flex items-center px-2 py-0.5 rounded text-xs font-medium bg-gray-100 text-gray-800">
                          +{video.tags.length - 2} more
                        </span>
                      )}
                    </div>
                  </td>
                  <td className="px-6 py-4 whitespace-nowrap text-right text-sm font-medium">
                    <div className="flex space-x-3 justify-end">
                      {video.status === 'Completed' ? (
                        <Link
                          href={`/?videoId=${video.id}&replay=true`}
                          className="text-green-600 hover:text-green-900"
                        >
                          Replay
                        </Link>
                      ) : (
                        <Link
                          href={`/?videoId=${video.id}`}
                          className="text-blue-600 hover:text-blue-900"
                        >
                          Review
                        </Link>
                      )}
                    </div>
                  </td>
                </tr>
              ))}
            </tbody>
          </table>
        </div>
      )}
      
      {/* Status summary */}
      <div className="mt-8 bg-gray-50 rounded-lg p-4">
        <h3 className="text-lg font-semibold mb-2">Summary</h3>
        <div className="grid grid-cols-2 md:grid-cols-4 gap-4">
          <div className="bg-white p-4 rounded-md shadow-sm">
            <div className="text-sm text-gray-500">Total</div>
            <div className="text-2xl font-bold">{videos.length}</div>
          </div>
          <div className="bg-white p-4 rounded-md shadow-sm">
            <div className="text-sm text-gray-500">Not Started</div>
            <div className="text-2xl font-bold">
              {videos.filter(v => v.status === 'Not Started').length}
            </div>
          </div>
          <div className="bg-white p-4 rounded-md shadow-sm">
            <div className="text-sm text-gray-500">Completed</div>
            <div className="text-2xl font-bold">
              {videos.filter(v => v.status === 'Completed').length}
            </div>
          </div>
          <div className="bg-white p-4 rounded-md shadow-sm">
            <div className="text-sm text-gray-500">Archived</div>
            <div className="text-2xl font-bold">
              {videos.filter(v => v.status === 'Archived').length}
            </div>
          </div>
        </div>
      </div>
    </div>
  );
}
|| END ||


|| START ./app/auth/register/page.tsx ||

'use client'
import { useState } from "react";
import { useRouter } from "next/navigation";
import { signIn } from "next-auth/react";

export default function Register() {
  const router = useRouter();
  const [email, setEmail] = useState("");
  const [name, setName] = useState("");
  const [password, setPassword] = useState("");
  const [error, setError] = useState("");
  const [loading, setLoading] = useState(false);
  
  const handleSubmit = async (e: React.FormEvent) => {
    e.preventDefault();
    setError("");
    setLoading(true);
    
    try {
      // Register the user
      const response = await fetch('/api/auth/register', {
        method: 'POST',
        headers: { 'Content-Type': 'application/json' },
        body: JSON.stringify({ email, name, password }),
      });
      
      const data = await response.json();
      
      if (!response.ok) {
        throw new Error(data.message || 'Registration failed');
      }
      
      // Sign in the user after successful registration
      await signIn('credentials', {
        redirect: false,
        email,
        password,
      });
      
      router.push('/inbox');
    } catch (error: any) {
      setError(error.message || 'An error occurred during registration');
      setLoading(false);
    }
  };
  
  return (
    <div className="flex min-h-full flex-col justify-center py-12 sm:px-6 lg:px-8">
      <div className="sm:mx-auto sm:w-full sm:max-w-md">
        <h2 className="mt-6 text-center text-3xl font-bold tracking-tight text-gray-900">
          Create a new account
        </h2>
      </div>

      <div className="mt-8 sm:mx-auto sm:w-full sm:max-w-md">
        <div className="bg-white py-8 px-4 shadow sm:rounded-lg sm:px-10">
          <form className="space-y-6" onSubmit={handleSubmit}>
            {error && (
              <div className="text-red-500 text-sm">{error}</div>
            )}
            
            <div>
              <label htmlFor="name" className="block text-sm font-medium text-gray-700">
                Name
              </label>
              <div className="mt-1">
                <input
                  id="name"
                  name="name"
                  type="text"
                  autoComplete="name"
                  required
                  value={name}
                  onChange={(e) => setName(e.target.value)}
                  className="block w-full appearance-none rounded-md border border-gray-300 px-3 py-2 shadow-sm focus:border-indigo-500 focus:outline-none focus:ring-indigo-500 sm:text-sm"
                />
              </div>
            </div>
            
            <div>
              <label htmlFor="email" className="block text-sm font-medium text-gray-700">
                Email address
              </label>
              <div className="mt-1">
                <input
                  id="email"
                  name="email"
                  type="email"
                  autoComplete="email"
                  required
                  value={email}
                  onChange={(e) => setEmail(e.target.value)}
                  className="block w-full appearance-none rounded-md border border-gray-300 px-3 py-2 shadow-sm focus:border-indigo-500 focus:outline-none focus:ring-indigo-500 sm:text-sm"
                />
              </div>
            </div>

            <div>
              <label htmlFor="password" className="block text-sm font-medium text-gray-700">
                Password
              </label>
              <div className="mt-1">
                <input
                  id="password"
                  name="password"
                  type="password"
                  autoComplete="new-password"
                  required
                  value={password}
                  onChange={(e) => setPassword(e.target.value)}
                  className="block w-full appearance-none rounded-md border border-gray-300 px-3 py-2 shadow-sm focus:border-indigo-500 focus:outline-none focus:ring-indigo-500 sm:text-sm"
                />
              </div>
            </div>

            <div>
              <button
                type="submit"
                disabled={loading}
                className="flex w-full justify-center rounded-md border border-transparent bg-indigo-600 py-2 px-4 text-sm font-medium text-white shadow-sm hover:bg-indigo-700 focus:outline-none focus:ring-2 focus:ring-indigo-500 focus:ring-offset-2 disabled:opacity-50 disabled:cursor-not-allowed"
              >
                {loading ? "Creating account..." : "Create account"}
              </button>
            </div>
            
            <div className="text-center text-sm">
              Already have an account?{" "}
              <a href="/auth/signin" className="font-medium text-indigo-600 hover:text-indigo-500">
                Sign in
              </a>
            </div>
          </form>
        </div>
      </div>
    </div>
  );
}

|| END ||


|| START ./app/auth/signin/page.tsx ||

'use client'
import { useState } from "react";
import { signIn } from "next-auth/react";
import { useRouter } from "next/navigation";

export default function SignIn() {
  const router = useRouter();
  const [email, setEmail] = useState("");
  const [password, setPassword] = useState("");
  const [error, setError] = useState("");
  const [loading, setLoading] = useState(false);
  
  const handleSubmit = async (e: React.FormEvent) => {
    e.preventDefault();
    setError("");
    setLoading(true);
    
    try {
      const result = await signIn("credentials", {
        redirect: false,
        email,
        password,
      });
      
      if (result?.error) {
        setError("Invalid email or password");
        setLoading(false);
        return;
      }
      
      router.push("/inbox");
    } catch (error) {
      setError("An error occurred. Please try again.");
      setLoading(false);
    }
  };
  
  return (
    <div className="flex min-h-full flex-col justify-center py-12 sm:px-6 lg:px-8">
      <div className="sm:mx-auto sm:w-full sm:max-w-md">
        <h2 className="mt-6 text-center text-3xl font-bold tracking-tight text-gray-900">
          Sign in to Annotation Tool
        </h2>
      </div>

      <div className="mt-8 sm:mx-auto sm:w-full sm:max-w-md">
        <div className="bg-white py-8 px-4 shadow sm:rounded-lg sm:px-10">
          <form className="space-y-6" onSubmit={handleSubmit}>
            {error && (
              <div className="text-red-500 text-sm">{error}</div>
            )}
            <div>
              <label htmlFor="email" className="block text-sm font-medium text-gray-700">
                Email address
              </label>
              <div className="mt-1">
                <input
                  id="email"
                  name="email"
                  type="email"
                  autoComplete="email"
                  required
                  value={email}
                  onChange={(e) => setEmail(e.target.value)}
                  className="block w-full appearance-none rounded-md border border-gray-300 px-3 py-2 shadow-sm focus:border-indigo-500 focus:outline-none focus:ring-indigo-500 sm:text-sm"
                />
              </div>
            </div>

            <div>
              <label htmlFor="password" className="block text-sm font-medium text-gray-700">
                Password
              </label>
              <div className="mt-1">
                <input
                  id="password"
                  name="password"
                  type="password"
                  autoComplete="current-password"
                  required
                  value={password}
                  onChange={(e) => setPassword(e.target.value)}
                  className="block w-full appearance-none rounded-md border border-gray-300 px-3 py-2 shadow-sm focus:border-indigo-500 focus:outline-none focus:ring-indigo-500 sm:text-sm"
                />
              </div>
            </div>

            <div>
              <button
                type="submit"
                disabled={loading}
                className="flex w-full justify-center rounded-md border border-transparent bg-indigo-600 py-2 px-4 text-sm font-medium text-white shadow-sm hover:bg-indigo-700 focus:outline-none focus:ring-2 focus:ring-indigo-500 focus:ring-offset-2 disabled:opacity-50 disabled:cursor-not-allowed"
              >
                {loading ? "Signing in..." : "Sign in"}
              </button>
            </div>
          </form>
        </div>
      </div>
    </div>
  );
}

|| END ||


|| START ./app/serialization-test/page.tsx ||

'use client';

import React, { useState, useRef, useEffect } from 'react';
import Link from 'next/link';

// Define minimal audio chunk interface
interface AudioChunk {
  blob: Blob | string;
  mimeType?: string;
}

// Client-only component for browser info
const BrowserInfo = () => {
  const [userAgent, setUserAgent] = useState('Loading...');
  
  useEffect(() => {
    setUserAgent(navigator.userAgent);
  }, []);
  
  return <span>Browser: {userAgent}</span>;
};

// Minimal serialization test page
export default function SerializationTestPage() {
  // Recording state
  const [isRecording, setIsRecording] = useState(false);
  const [recordedBlob, setRecordedBlob] = useState<Blob | null>(null);
  const [recordingFormat, setRecordingFormat] = useState('');
  
  // Serialization state
  const [serializedString, setSerializedString] = useState<string | null>(null);
  const [deserializedBlob, setDeserializedBlob] = useState<Blob | null>(null);
  const [deserializedAudioUrl, setDeserializedAudioUrl] = useState<string | null>(null);
  
  // Original audio URL (direct from recorder)
  const [directAudioUrl, setDirectAudioUrl] = useState<string | null>(null);
  
  // Debug information
  const [debugInfo, setDebugInfo] = useState<any>({});
  const [error, setError] = useState<string | null>(null);
  
  // Refs
  const mediaRecorderRef = useRef<MediaRecorder | null>(null);
  const chunksRef = useRef<Blob[]>([]);
  const directAudioRef = useRef<HTMLAudioElement | null>(null);
  const deserializedAudioRef = useRef<HTMLAudioElement | null>(null);
  
  // Helper function to convert blob to base64
  const blobToBase64 = (blob: Blob): Promise<string> => {
    return new Promise((resolve, reject) => {
      const reader = new FileReader();
      reader.onload = () => {
        const dataUrl = reader.result as string;
        resolve(dataUrl);
      };
      reader.onerror = reject;
      reader.readAsDataURL(blob);
    });
  };
  
  // Helper function to convert base64 back to blob
  const base64ToBlob = (base64: string, mimeType: string): Blob => {
    try {
      const byteString = atob(base64.split(',')[1]);
      const ab = new ArrayBuffer(byteString.length);
      const ia = new Uint8Array(ab);
      
      for (let i = 0; i < byteString.length; i++) {
        ia[i] = byteString.charCodeAt(i);
      }
      
      return new Blob([ab], { type: mimeType });
    } catch (error) {
      console.error('Error converting base64 to Blob:', error);
      throw error;
    }
  };
  
  // Start recording audio
  const startRecording = async () => {
    try {
      // Reset state
      chunksRef.current = [];
      setRecordedBlob(null);
      setDirectAudioUrl(null);
      setSerializedString(null);
      setDeserializedBlob(null);
      setDeserializedAudioUrl(null);
      setDebugInfo({});
      setError(null);
      
      // Request microphone access
      const stream = await navigator.mediaDevices.getUserMedia({
        audio: {
          echoCancellation: true,
          noiseSuppression: true,
          autoGainControl: true,
          channelCount: 1
        }
      });
      
      // Find the best supported audio format
      let mimeType = '';
      const formats = [
        'audio/webm;codecs=opus',
        'audio/webm',
        'audio/mp4;codecs=opus',
        'audio/mp4',
        'audio/ogg;codecs=opus',
        'audio/ogg',
        'audio/wav'
      ];
      
      for (const format of formats) {
        if (MediaRecorder.isTypeSupported(format)) {
          mimeType = format;
          break;
        }
      }
      
      setRecordingFormat(mimeType || 'default format');
      console.log('Using audio format:', mimeType || 'default');
      
      // Create recorder
      const recorderOptions = {
        mimeType: mimeType || undefined,
        audioBitsPerSecond: 128000
      };
      
      const recorder = new MediaRecorder(stream, recorderOptions);
      mediaRecorderRef.current = recorder;
      
      // Handle data available event
      recorder.ondataavailable = (e) => {
        if (e.data.size > 0) {
          chunksRef.current.push(e.data);
        }
      };
      
      // Handle recording stop
      recorder.onstop = async () => {
        try {
          // Create audio blob from chunks
          const audioBlob = new Blob(chunksRef.current, { type: mimeType || 'audio/webm' });
          setRecordedBlob(audioBlob);
          
          // Create direct URL for immediate playback
          const directUrl = URL.createObjectURL(audioBlob);
          setDirectAudioUrl(directUrl);
          
          // Log original blob info
          console.log('Original audio blob:', {
            type: audioBlob.type,
            size: audioBlob.size,
            chunks: chunksRef.current.length
          });
          
          // Start serialization process
          await serializeAndDeserialize(audioBlob, mimeType || 'audio/webm');
          
          // Stop all tracks to release the microphone
          stream.getTracks().forEach(track => track.stop());
        } catch (err) {
          setError(`Error processing recording: ${err instanceof Error ? err.message : String(err)}`);
          console.error('Recording processing error:', err);
        }
      };
      
      // Start the recorder
      recorder.start();
      setIsRecording(true);
    } catch (error) {
      setError(`Could not start recording: ${error instanceof Error ? error.message : String(error)}`);
      console.error('Error starting recording:', error);
    }
  };
  
  // Stop recording
  const stopRecording = () => {
    if (mediaRecorderRef.current && mediaRecorderRef.current.state !== 'inactive') {
      mediaRecorderRef.current.stop();
      setIsRecording(false);
    }
  };
  
  // Serialize and deserialize audio blob
  const serializeAndDeserialize = async (blob: Blob, mimeType: string) => {
    try {
      // Step 1: Convert blob to base64 string
      console.log('Step 1: Converting blob to base64...');
      const startTime1 = performance.now();
      const base64String = await blobToBase64(blob);
      const endTime1 = performance.now();
      
      // Set serialized string state
      setSerializedString(base64String);
      console.log('Base64 conversion complete:', {
        originalSize: blob.size,
        base64Length: base64String.length,
        timeMs: (endTime1 - startTime1).toFixed(2)
      });
      
      // Step 2: Convert base64 back to blob
      console.log('Step 2: Converting base64 back to blob...');
      const startTime2 = performance.now();
      const newBlob = base64ToBlob(base64String, mimeType);
      const endTime2 = performance.now();
      
      // Set deserialized blob state
      setDeserializedBlob(newBlob);
      console.log('Blob conversion complete:', {
        newSize: newBlob.size, 
        newType: newBlob.type,
        timeMs: (endTime2 - startTime2).toFixed(2)
      });
      
      // Step 3: Create audio URL from new blob
      const deserializedUrl = URL.createObjectURL(newBlob);
      setDeserializedAudioUrl(deserializedUrl);
      
      // Set debug info for display
      setDebugInfo({
        originalBlob: {
          size: blob.size,
          type: blob.type,
          mimeType: mimeType
        },
        base64String: {
          length: base64String.length,
          preview: base64String.substring(0, 50) + '...',
          conversionTimeMs: (endTime1 - startTime1).toFixed(2)
        },
        deserializedBlob: {
          size: newBlob.size,
          type: newBlob.type,
          conversionTimeMs: (endTime2 - startTime2).toFixed(2)
        },
        comparison: {
          sizeMatch: blob.size === newBlob.size,
          typeMatch: blob.type === newBlob.type
        }
      });
    } catch (error) {
      setError(`Serialization error: ${error instanceof Error ? error.message : String(error)}`);
      console.error('Serialization process failed:', error);
    }
  };
  
  // Clean up on unmount
  useEffect(() => {
    return () => {
      if (directAudioUrl) URL.revokeObjectURL(directAudioUrl);
      if (deserializedAudioUrl) URL.revokeObjectURL(deserializedAudioUrl);
    };
  }, [directAudioUrl, deserializedAudioUrl]);
  
  return (
    <div className="p-8 max-w-4xl mx-auto">
      <div className="mb-6">
        <Link href="/" className="text-blue-500 hover:underline mb-4 inline-block">
          &larr; Back to main app
        </Link>
        <h1 className="text-3xl font-bold mb-4">Audio Serialization Test</h1>
        <p className="text-gray-600 mb-4">
          This minimal test focuses only on recording audio and testing the serialization/deserialization process.
        </p>
      </div>
      
      <div className="bg-white p-6 rounded-lg shadow-md mb-6">
        {/* Recording controls */}
        <div className="mb-6">
          <h2 className="text-xl font-semibold mb-2">1. Record Audio</h2>
          <p className="text-sm text-gray-500 mb-4">
            Recording format: {recordingFormat || 'Not determined yet'}
          </p>
          
          <div className="flex items-center space-x-4 mb-4">
            {!isRecording ? (
              <button
                onClick={startRecording}
                className="flex items-center px-4 py-2 bg-red-500 text-white rounded-md hover:bg-red-600"
              >
                <span className="h-3 w-3 bg-white rounded-full mr-2"></span>
                Start Recording
              </button>
            ) : (
              <button
                onClick={stopRecording}
                className="flex items-center px-4 py-2 bg-gray-700 text-white rounded-md hover:bg-gray-800"
              >
                <span className="h-3 w-3 bg-white mr-2"></span>
                Stop Recording
              </button>
            )}
          </div>
          
          {isRecording && (
            <div className="flex items-center mb-4">
              <span className="h-3 w-3 bg-red-500 rounded-full animate-pulse mr-2"></span>
              <span className="text-red-500">Microphone active</span>
            </div>
          )}
        </div>
        
        {/* Error message */}
        {error && (
          <div className="mb-6 p-3 bg-red-100 border border-red-300 text-red-700 rounded-md">
            <strong>Error:</strong> {error}
          </div>
        )}
        
        {/* Audio playback comparison */}
        {directAudioUrl && (
          <div className="mb-6">
            <h2 className="text-xl font-semibold mb-4">2. Playback Comparison</h2>
            
            <div className="grid grid-cols-1 md:grid-cols-2 gap-6">
              {/* Direct playback */}
              <div className="bg-gray-100 p-4 rounded-md">
                <h3 className="font-semibold text-lg mb-2 text-blue-700">Direct Playback</h3>
                <p className="text-sm text-gray-500 mb-2">
                  Original recording (no serialization)
                </p>
                
                <audio 
                  ref={directAudioRef}
                  src={directAudioUrl} 
                  controls 
                  className="w-full mb-3"
                />
                
                <div className="text-xs text-gray-600 mb-2">
                  Size: {recordedBlob ? `${(recordedBlob.size / 1024).toFixed(2)} KB` : 'Unknown'}
                </div>
                
                <button
                  onClick={() => {
                    if (directAudioRef.current) {
                      directAudioRef.current.currentTime = 0;
                      directAudioRef.current.play();
                    }
                  }}
                  className="px-3 py-1 bg-blue-500 text-white text-sm rounded hover:bg-blue-600"
                >
                  Replay
                </button>
              </div>
              
              {/* Serialized playback */}
              <div className="bg-gray-100 p-4 rounded-md">
                <h3 className="font-semibold text-lg mb-2 text-purple-700">Deserialized Playback</h3>
                <p className="text-sm text-gray-500 mb-2">
                  After base64 conversion and back
                </p>
                
                {deserializedAudioUrl ? (
                  <audio 
                    ref={deserializedAudioRef}
                    src={deserializedAudioUrl} 
                    controls 
                    className="w-full mb-3"
                  />
                ) : (
                  <div className="w-full h-12 bg-gray-200 flex items-center justify-center text-gray-500 mb-3">
                    Waiting for deserialization...
                  </div>
                )}
                
                <div className="text-xs text-gray-600 mb-2">
                  Size: {deserializedBlob ? `${(deserializedBlob.size / 1024).toFixed(2)} KB` : 'Unknown'}
                </div>
                
                <button
                  onClick={() => {
                    if (deserializedAudioRef.current) {
                      deserializedAudioRef.current.currentTime = 0;
                      deserializedAudioRef.current.play();
                    }
                  }}
                  disabled={!deserializedAudioUrl}
                  className={`px-3 py-1 text-white text-sm rounded ${deserializedAudioUrl ? 'bg-purple-500 hover:bg-purple-600' : 'bg-gray-400'}`}
                >
                  Replay
                </button>
              </div>
            </div>
          </div>
        )}
        
        {/* Serialization details */}
        {Object.keys(debugInfo).length > 0 && (
          <div className="mt-6 border-t pt-4">
            <h3 className="font-semibold text-lg mb-2">3. Serialization Details</h3>
            
            <div className="bg-gray-800 text-green-300 p-4 rounded font-mono text-xs overflow-x-auto">
              <pre>
{`Serialization Process:
------------------------
Original Blob:
  Size: ${debugInfo.originalBlob?.size} bytes (${(debugInfo.originalBlob?.size / 1024).toFixed(2)} KB)
  Type: ${debugInfo.originalBlob?.type}
  MIME: ${debugInfo.originalBlob?.mimeType}

Base64 Conversion:
  Length: ${debugInfo.base64String?.length} characters
  Time: ${debugInfo.base64String?.conversionTimeMs}ms
  Preview: ${debugInfo.base64String?.preview}

Deserialized Blob:
  Size: ${debugInfo.deserializedBlob?.size} bytes (${(debugInfo.deserializedBlob?.size / 1024).toFixed(2)} KB)
  Type: ${debugInfo.deserializedBlob?.type}
  Time: ${debugInfo.deserializedBlob?.conversionTimeMs}ms

Comparison:
  Size Match: ${debugInfo.comparison?.sizeMatch ? '✅ Yes' : '❌ No'}
  Type Match: ${debugInfo.comparison?.typeMatch ? '✅ Yes' : '❌ No'}
`}
              </pre>
            </div>
            
            {/* Base64 string preview */}
            {serializedString && (
              <div className="mt-4">
                <h4 className="font-semibold mb-2">Base64 String (first 100 chars)</h4>
                <div className="bg-gray-100 p-3 rounded overflow-x-auto">
                  <code className="text-xs break-all">
                    {serializedString.substring(0, 100)}...
                  </code>
                </div>
                <button
                  onClick={() => {
                    if (serializedString) {
                      const a = document.createElement('a');
                      const blob = new Blob([serializedString], { type: 'text/plain' });
                      a.href = URL.createObjectURL(blob);
                      a.download = `audio-base64-${new Date().toISOString()}.txt`;
                      document.body.appendChild(a);
                      a.click();
                      document.body.removeChild(a);
                      URL.revokeObjectURL(a.href);
                    }
                  }}
                  className="mt-2 px-3 py-1 bg-gray-500 text-white text-sm rounded hover:bg-gray-600"
                >
                  Download Base64 String
                </button>
              </div>
            )}
          </div>
        )}
      </div>
      
      <div className="text-xs text-gray-500 mt-4">
        <BrowserInfo />
      </div>
    </div>
  );
}
|| END ||


|| START ./app/audio-test/page.tsx ||

'use client';

import React, { useState, useRef, useEffect } from 'react';
import Link from 'next/link';
import dynamic from 'next/dynamic';

// Import the proper VideoPlayer component from the main app
// Use dynamic import to avoid SSR issues
const VideoPlayer = dynamic(() => import('../../src/components/VideoPlayer'), { ssr: false });

// Define AudioChunk interface identical to the one in AudioRecorder.tsx
interface AudioChunk {
  blob: Blob | string;      // The audio data as Blob or string (for serialization)
  startTime: number;        // Relative to recording start
  duration: number;         // Length of audio chunk in ms
  videoTime: number;        // Video timestamp when this audio was recorded
  url?: string;             // URL for playback (created during replay)
  mimeType?: string;        // MIME type for proper playback
}

// Client-side component for browser info to prevent hydration mismatch
const BrowserInfo = () => {
  const [userAgent, setUserAgent] = useState('Loading...');
  
  useEffect(() => {
    setUserAgent(navigator.userAgent);
  }, []);
  
  return <span>Browser: {userAgent}</span>;
};

// Client-side component for MIME type support to prevent hydration mismatch
const MimeTypesInfo = () => {
  const [supportedTypes, setSupportedTypes] = useState<Record<string, boolean>>({});
  const mimeTypes = ['audio/webm', 'audio/webm;codecs=opus', 'audio/mp4', 'audio/ogg'];
  
  useEffect(() => {
    const support: Record<string, boolean> = {};
    
    mimeTypes.forEach(type => {
      if (typeof MediaRecorder !== 'undefined') {
        support[type] = MediaRecorder.isTypeSupported(type);
      } else {
        support[type] = false;
      }
    });
    
    setSupportedTypes(support);
  }, []);
  
  return (
    <>
      <span>Supported MIME types:</span>
      <ul className="pl-4 mt-1">
        {mimeTypes.map(type => (
          <li 
            key={type} 
            className={supportedTypes[type] ? 'text-green-600' : 'text-red-600'}
          >
            {type}: {Object.keys(supportedTypes).length > 0 ? 
              (supportedTypes[type] ? 'Supported' : 'Not supported') : 
              'Checking...'}
          </li>
        ))}
      </ul>
    </>
  );
};

// Simple audio recorder and player for testing - enhanced with serialization testing
export default function AudioTestPage() {
  const [isRecording, setIsRecording] = useState(false);
  const [recordedAudio, setRecordedAudio] = useState<string | null>(null);
  const [recordingDuration, setRecordingDuration] = useState(0);
  const [elapsedTime, setElapsedTime] = useState(0);
  const [recordingFormat, setRecordingFormat] = useState('');
  const [recordingStartTime, setRecordingStartTime] = useState<number | null>(null);
  
  // State for storing the recorded audio blob for serialization testing
  const [recordedBlob, setRecordedBlob] = useState<Blob | null>(null);
  const [serializedAudio, setSerializedAudio] = useState<string | null>(null);
  const [deserializedAudio, setDeserializedAudio] = useState<string | null>(null);
  const [audioChunk, setAudioChunk] = useState<AudioChunk | null>(null);
  const [debugInfo, setDebugInfo] = useState<Record<string, any>>({});
  
  // State for the programmatic playback test
  const [playbackTestResult, setPlaybackTestResult] = useState<{success: boolean; details?: string} | null>(null);
  const [playbackTestRunning, setPlaybackTestRunning] = useState(false);
  const [syncTestResult, setSyncTestResult] = useState<{success: boolean; details?: string} | null>(null);
  
  const mediaRecorderRef = useRef<MediaRecorder | null>(null);
  const chunksRef = useRef<Blob[]>([]);
  const timerRef = useRef<NodeJS.Timeout | null>(null);
  const audioRef = useRef<HTMLAudioElement | null>(null);
  const audioSerializedRef = useRef<HTMLAudioElement | null>(null);
  const videoPlayerRef = useRef<HTMLVideoElement | null>(null);
  
  // State for tracking video synchronization
  const [syncWithVideo, setSyncWithVideo] = useState(true);
  const [videoPlaying, setVideoPlaying] = useState(false);
  
  // Helper function to convert Blob to base64 for storage (copied from VideoPlayerWrapper)
  const blobToBase64 = (blob: Blob): Promise<string> => {
    return new Promise((resolve, reject) => {
      const reader = new FileReader();
      reader.onload = () => {
        const dataUrl = reader.result as string;
        resolve(dataUrl);
      };
      reader.onerror = reject;
      reader.readAsDataURL(blob);
    });
  };

  // Helper function to convert base64 back to Blob for playback (copied from VideoPlayerWrapper)
  const base64ToBlob = (base64: string, mimeType: string): Blob => {
    try {
      // First ensure we have a proper data URL with the correct format
      if (!base64.includes(',')) {
        throw new Error('Invalid base64 string format - missing comma separator');
      }
      
      // Extract the base64 part after the comma
      const base64Data = base64.split(',')[1];
      if (!base64Data) {
        throw new Error('Invalid base64 string - no data after comma');
      }
      
      // Decode the base64 string to binary
      const byteString = atob(base64Data);
      
      // Create an ArrayBuffer to hold the decoded data
      const ab = new ArrayBuffer(byteString.length);
      const ia = new Uint8Array(ab);
      
      // Copy the decoded binary data to the array buffer
      for (let i = 0; i < byteString.length; i++) {
        ia[i] = byteString.charCodeAt(i);
      }
      
      // Create and return a new Blob from the array buffer
      return new Blob([ab], { type: mimeType });
    } catch (error) {
      if (typeof window !== 'undefined') {
        console.error('Error converting base64 to Blob:', error);
      }
      throw error;
    }
  };
  
  // Start recording audio
  const startRecording = async () => {
    try {
      // Reset state
      chunksRef.current = [];
      setRecordedAudio(null);
      setElapsedTime(0);
      setRecordedBlob(null);
      setSerializedAudio(null);
      setDeserializedAudio(null);
      setAudioChunk(null);
      setDebugInfo({});
      
      // Request microphone access with high quality settings
      const stream = await navigator.mediaDevices.getUserMedia({
        audio: {
          echoCancellation: true,
          noiseSuppression: true,
          autoGainControl: true,
          channelCount: 1, // Mono for voice clarity
          sampleRate: 48000 // Higher sample rate for better quality
        }
      });
      
      // Find the best supported audio format
      let mimeType = '';
      const formats = [
        'audio/webm;codecs=opus',
        'audio/webm',
        'audio/mp4;codecs=opus',
        'audio/mp4',
        'audio/ogg;codecs=opus',
        'audio/ogg',
        'audio/wav'
      ];
      
      for (const format of formats) {
        if (MediaRecorder.isTypeSupported(format)) {
          mimeType = format;
          break;
        }
      }
      
      setRecordingFormat(mimeType || 'default format');
      if (typeof window !== 'undefined') {
        console.log('Using audio format:', mimeType || 'default');
      }
      
      // Create recorder with high quality settings
      const recorderOptions = {
        mimeType: mimeType || undefined,
        audioBitsPerSecond: 128000
      };
      
      const recorder = new MediaRecorder(stream, recorderOptions);
      mediaRecorderRef.current = recorder;
      
      // Handle data available event
      recorder.ondataavailable = (e) => {
        if (e.data.size > 0) {
          chunksRef.current.push(e.data);
        }
      };
      
      // Handle recording stop
      recorder.onstop = async () => {
        // Always clean up resources in finally block
        try {
          // Create audio blob from chunks
          const audioBlob = new Blob(chunksRef.current, { type: mimeType || 'audio/webm' });
          const audioUrl = URL.createObjectURL(audioBlob);
          setRecordedAudio(audioUrl);
          setRecordingDuration(elapsedTime);
          setRecordedBlob(audioBlob);
          
          if (typeof window !== 'undefined') {
            console.log('Recording stopped. Blob created:', {
              size: audioBlob.size,
              type: audioBlob.type,
              chunks: chunksRef.current.length
            });
          }
          
          // Use either the recordingStartTime if available, or fallback to current time
          const startTime = recordingStartTime || Date.now();
          if (typeof window !== 'undefined' && !recordingStartTime) {
            console.log('Warning: Recording start time was not set, using fallback time');
          }
          
          // Create an AudioChunk object similar to the one in the main app
          const chunk: AudioChunk = {
            blob: audioBlob,
            startTime: startTime,
            duration: elapsedTime * 1000, // Convert to ms
            videoTime: 0, // No video in this test, so set to 0
            mimeType: mimeType || 'audio/webm',
          };
          setAudioChunk(chunk);
          
          // Directly call our standalone serialization method
          try {
            if (typeof window !== 'undefined') {
              console.log('Calling direct serialization method for audio blob:', {
                size: audioBlob.size,
                type: audioBlob.type
              });
            }
            
            // Call the serializeAndDeserialize method to handle the entire process
            await serializeAndDeserialize(audioBlob, mimeType || 'audio/webm');
          } catch (serializationError) {
            if (typeof window !== 'undefined') {
              console.error('Error in serialization process:', serializationError);
            }
            setDebugInfo({
              serializationError: serializationError instanceof Error ? 
                serializationError.message : String(serializationError),
              stage: 'serialization_process'
            });
          }
        } catch (recordingError) {
          // Handle any errors in the main try block
          if (typeof window !== 'undefined') {
            console.error('Error processing recording:', recordingError);
          }
          setDebugInfo({
            error: recordingError instanceof Error ? 
              recordingError.message : String(recordingError),
            stage: 'recording_processing'
          });
        } finally {
          // Clean up resources
          stream.getTracks().forEach(track => track.stop());
          
          if (timerRef.current) {
            clearInterval(timerRef.current);
            timerRef.current = null;
          }
        }
      };
      
      // Start the recorder and timer
      const startTime = Date.now();
      setRecordingStartTime(startTime);
      
      // Log that we're setting the recording start time
      if (typeof window !== 'undefined') {
        console.log('Setting recording start time:', startTime);
      }
      
      recorder.start();
      setIsRecording(true);
      
      // Update elapsed time every second
      timerRef.current = setInterval(() => {
        setElapsedTime(prev => prev + 1);
      }, 1000);
    } catch (error) {
      if (typeof window !== 'undefined') {
        console.error('Error starting recording:', error);
        alert(`Could not start recording: ${error instanceof Error ? error.message : String(error)}`);
      }
    }
  };
  
  // Stop recording
  const stopRecording = () => {
    if (mediaRecorderRef.current && mediaRecorderRef.current.state !== 'inactive') {
      mediaRecorderRef.current.stop();
      setIsRecording(false);
    }
  };
  
  // Format seconds as MM:SS
  const formatTime = (seconds: number) => {
    const mins = Math.floor(seconds / 60);
    const secs = seconds % 60;
    return `${mins}:${secs < 10 ? '0' : ''}${secs}`;
  };
  
  // Direct serialization method (similar to serialization-test page)
  const serializeAndDeserialize = async (blob: Blob, mimeType: string) => {
    if (typeof window !== 'undefined') {
      console.log('Starting direct serializeAndDeserialize method with blob:', {
        size: blob.size,
        type: blob.type,
        mimeType: mimeType
      });
    }
    
    try {
      // Step 1: Convert blob to base64 string
      const startTime1 = performance.now();
      const base64String = await blobToBase64(blob);
      const endTime1 = performance.now();
      
      // Set serialized string state
      setSerializedAudio(base64String);
      
      if (typeof window !== 'undefined') {
        console.log('Base64 conversion complete:', {
          originalSize: blob.size,
          base64Length: base64String.length,
          timeMs: (endTime1 - startTime1).toFixed(2)
        });
      }
      
      // Step 2: Convert base64 back to blob
      if (typeof window !== 'undefined') {
        console.log('Converting base64 back to blob using MIME type:', mimeType || 'audio/webm');
      }
      
      const startTime2 = performance.now();
      const newBlob = base64ToBlob(base64String, mimeType || 'audio/webm');
      const endTime2 = performance.now();
      
      if (typeof window !== 'undefined') {
        console.log('Blob conversion complete:', {
          newSize: newBlob.size, 
          newType: newBlob.type,
          timeMs: (endTime2 - startTime2).toFixed(2)
        });
      }
      
      // Step 3: Create audio URL from new blob
      const deserializedUrl = URL.createObjectURL(newBlob);
      setDeserializedAudio(deserializedUrl);
      
      // Set debug info for display
      setDebugInfo({
        originalBlob: {
          size: blob.size,
          type: blob.type,
          mimeType: mimeType || 'audio/webm'
        },
        base64String: {
          length: base64String.length,
          preview: base64String.substring(0, 50) + '...',
          conversionTimeMs: (endTime1 - startTime1).toFixed(2)
        },
        deserializedBlob: {
          size: newBlob.size,
          type: newBlob.type,
          conversionTimeMs: (endTime2 - startTime2).toFixed(2)
        },
        comparison: {
          sizeMatch: blob.size === newBlob.size,
          typeMatch: blob.type === newBlob.type,
          diffBytes: Math.abs(blob.size - newBlob.size)
        }
      });
      
      return {
        base64String,
        deserializedBlob: newBlob,
        deserializedUrl
      };
    } catch (error) {
      if (typeof window !== 'undefined') {
        console.error('Serialization process failed:', error);
      }
      setDebugInfo(prev => ({
        ...prev,
        serializationError: error instanceof Error ? error.message : String(error)
      }));
      throw error;
    }
  };

  // Create JSON representation of the audio chunk
  const createJsonRepresentation = async () => {
    if (!audioChunk) return;
    
    try {
      // Create a deep copy and convert blob to base64 if needed
      const chunkCopy = {...audioChunk};
      
      if (chunkCopy.blob instanceof Blob) {
        // Convert blob to base64
        chunkCopy.blob = await blobToBase64(chunkCopy.blob);
      }
      
      // Remove url property
      delete chunkCopy.url;
      
      // Return JSON representation
      return JSON.stringify(chunkCopy, null, 2);
    } catch (error) {
      if (typeof window !== 'undefined') {
        console.error('Error creating JSON representation:', error);
      }
      return null;
    }
  };
  
  // Attempt to play an audio chunk programmatically
  const testAudioChunkPlayback = async (chunk: AudioChunk | null) => {
    if (!chunk) {
      console.error('No audio chunk to test');
      return false;
    }
    
    try {
      console.log('Testing audio chunk playback:', {
        blobType: chunk.blob instanceof Blob ? 'Blob' : typeof chunk.blob,
        startTime: chunk.startTime,
        duration: chunk.duration,
        videoTime: chunk.videoTime,
        mimeType: chunk.mimeType || 'unknown'
      });
      
      // Create audio element
      const audio = new Audio();
      
      // Set up event listeners
      return new Promise<boolean>((resolve) => {
        audio.onloadedmetadata = () => {
          console.log('Audio metadata loaded:', {
            duration: audio.duration,
            readyState: audio.readyState
          });
        };
        
        audio.oncanplay = () => {
          console.log('Audio can play now');
          audio.play().catch(error => {
            console.error('Failed to play audio:', error);
            resolve(false);
          });
        };
        
        audio.onplay = () => {
          console.log('Audio playback started');
        };
        
        audio.onended = () => {
          console.log('Audio playback completed successfully');
          resolve(true);
        };
        
        audio.onerror = (e) => {
          console.error('Audio playback error:', {
            error: audio.error?.code,
            message: audio.error?.message
          });
          resolve(false);
        };
        
        // Handle different blob types
        let audioUrl: string;
        if (chunk.url) {
          audioUrl = chunk.url;
        } else if (chunk.blob instanceof Blob) {
          audioUrl = URL.createObjectURL(chunk.blob);
        } else if (typeof chunk.blob === 'string' && chunk.blob.startsWith('data:')) {
          // Option 1: Convert data URL to blob first
          try {
            // Split the data URL into parts
            const parts = chunk.blob.split(',');
            if (parts.length !== 2) {
              throw new Error('Invalid data URL format');
            }
            
            // Extract MIME type
            const mimeMatch = parts[0].match(/:(.*?);/);
            const mime = mimeMatch ? mimeMatch[1] : chunk.mimeType || 'audio/webm';
            
            // Convert base64 to binary
            const binary = atob(parts[1]);
            
            // Create array buffer
            const arrayBuffer = new ArrayBuffer(binary.length);
            const uint8Array = new Uint8Array(arrayBuffer);
            
            for (let i = 0; i < binary.length; i++) {
              uint8Array[i] = binary.charCodeAt(i);
            }
            
            // Create blob and URL
            const newBlob = new Blob([uint8Array], { type: mime });
            audioUrl = URL.createObjectURL(newBlob);
            console.log('Converted data URL to blob for playback:', { 
              size: newBlob.size, 
              type: newBlob.type 
            });
          } catch (error) {
            console.error('Error processing data URL:', error);
            resolve(false);
            return;
          }
        } else {
          console.error('Invalid audio data format:', typeof chunk.blob);
          resolve(false);
          return;
        }
        
        // Set source and start loading
        audio.src = audioUrl;
        audio.load();
        
        // Set timeout for overall operation
        setTimeout(() => {
          console.warn('Audio test timed out after 10 seconds');
          resolve(false);
        }, 10000);
      });
    } catch (error) {
      console.error('Error testing audio chunk:', error);
      return false;
    }
  };
  
  // Test audio and video synchronization
  const testAudioVideoSync = async () => {
    if (!videoPlayerRef.current || !audioRef.current) {
      console.error('Video or audio element not available for sync test');
      return false;
    }
    
    if (!recordedAudio) {
      console.error('No recorded audio available to test');
      return false;
    }
    
    setSyncTestResult(null);
    
    try {
      console.log('Starting audio-video synchronization test');
      
      // Reset both players
      videoPlayerRef.current.currentTime = 0;
      audioRef.current.currentTime = 0;
      
      // Set up recording and measurement variables
      const videoStartTime = Date.now();
      let audioStartTime = 0;
      let syncDifference = 0;
      
      // Create promise for detecting synchronization
      return new Promise<{success: boolean; details: string}>((resolve) => {
        // Set up audio event handlers
        const audioPlayHandler = () => {
          audioStartTime = Date.now();
          syncDifference = audioStartTime - videoStartTime;
          
          console.log('Audio playback started in sync test:', {
            videoStartTime,
            audioStartTime,
            syncDifference: `${syncDifference}ms`
          });
        };
        
        const audioEndHandler = () => {
          // Clean up event listeners
          audioRef.current?.removeEventListener('play', audioPlayHandler);
          audioRef.current?.removeEventListener('ended', audioEndHandler);
          
          // Calculate results
          const success = syncDifference < 500; // Less than 500ms difference is considered good
          
          console.log('Sync test completed:', {
            syncDifference: `${syncDifference}ms`,
            success,
            threshold: '500ms'
          });
          
          resolve({
            success,
            details: `Audio started ${syncDifference}ms after video. ${
              success ? 'Synchronization is good!' : 'Synchronization needs improvement.'
            }`
          });
        };
        
        // Add event listeners to the audio element
        audioRef.current.addEventListener('play', audioPlayHandler);
        audioRef.current.addEventListener('ended', audioEndHandler);
        
        // Start playing the video, which should trigger the audio
        const videoPlayPromise = videoPlayerRef.current.play();
        
        videoPlayPromise.catch(error => {
          console.error('Video playback failed during sync test:', error);
          audioRef.current?.removeEventListener('play', audioPlayHandler);
          audioRef.current?.removeEventListener('ended', audioEndHandler);
          
          resolve({
            success: false,
            details: `Failed to play video: ${error.message}. Synchronization test aborted.`
          });
        });
        
        // Set timeout for overall operation
        setTimeout(() => {
          audioRef.current?.removeEventListener('play', audioPlayHandler);
          audioRef.current?.removeEventListener('ended', audioEndHandler);
          
          console.warn('Sync test timed out after 20 seconds');
          resolve({
            success: false,
            details: 'Synchronization test timed out. Check console for details.'
          });
        }, 20000);
      });
    } catch (error) {
      console.error('Error during sync test:', error);
      return {
        success: false,
        details: `Error during synchronization test: ${error instanceof Error ? error.message : String(error)}`
      };
    }
  };
  
  // Clean up on unmount
  useEffect(() => {
    return () => {
      if (timerRef.current) {
        clearInterval(timerRef.current);
      }
      if (recordedAudio) {
        URL.revokeObjectURL(recordedAudio);
      }
      if (deserializedAudio) {
        URL.revokeObjectURL(deserializedAudio);
      }
    };
  }, [recordedAudio, deserializedAudio]);
  
  return (
    <div className="p-8 max-w-4xl mx-auto">
      <div className="mb-6">
        <Link href="/" className="text-blue-500 hover:underline mb-4 inline-block">
          &larr; Back to main app
        </Link>
        <h1 className="text-3xl font-bold mb-4">Audio Recording Test</h1>
        <p className="text-gray-600 mb-4">
          This page provides a standalone audio recorder to test recording and playback functionality.
        </p>
        <div className="bg-amber-50 border-l-4 border-amber-500 p-4 text-amber-700">
          <h3 className="font-bold">Dual Playback Test</h3>
          <p>This enhanced test compares direct audio playback with JSON-serialized playback (mimicking the main app).</p>
        </div>
      </div>
      
      {/* Video player section */}
      <div className="mb-6">
        <h2 className="text-xl font-semibold mb-4">Video Player</h2>
        <div className="flex items-center justify-between mb-3 bg-white p-4 rounded-lg">
          <p className="text-sm text-gray-600">
            This video will synchronize with audio playback when testing recordings.
          </p>
          <div className="flex items-center">
            <label className="flex items-center cursor-pointer">
              <span className="mr-2 text-sm">Sync with Audio:</span>
              <div 
                className={`relative inline-block w-10 h-6 transition-colors duration-200 ease-in-out rounded-full ${syncWithVideo ? 'bg-green-500' : 'bg-gray-300'}`}
                onClick={() => setSyncWithVideo(!syncWithVideo)}
              >
                <span 
                  className={`absolute left-1 top-1 bottom-1 w-4 h-4 transition-transform duration-200 ease-in-out bg-white rounded-full ${syncWithVideo ? 'transform translate-x-4' : ''}`}
                ></span>
              </div>
            </label>
          </div>
        </div>
        
        <VideoPlayer 
          setVideoRef={(ref) => {
            videoPlayerRef.current = ref;
            
            // Add our custom event listeners for sync
            if (ref) {
              ref.addEventListener('play', () => setVideoPlaying(true));
              ref.addEventListener('pause', () => setVideoPlaying(false));
            }
          }}
        />
        
        <div className="bg-white p-3 rounded-lg mt-2">
          <div className="flex justify-between items-center mb-2">
            <div className="text-xs text-gray-500">
              {videoPlaying ? (
                <span className="text-green-500 flex items-center">
                  <span className="h-2 w-2 bg-green-500 rounded-full mr-1 animate-pulse"></span> 
                  Video is playing
                </span>
              ) : (
                <span>Video is paused</span>
              )}
            </div>
            {syncWithVideo && (
              <div className="text-blue-500 font-medium text-xs">
                ✓ Audio synchronization is enabled
              </div>
            )}
          </div>
          
          {recordedAudio && (
            <div className="mt-2">
              <button
                onClick={async () => {
                  setSyncTestResult(null);
                  const result = await testAudioVideoSync();
                  if (result) {
                    setSyncTestResult(result);
                  }
                }}
                className="bg-indigo-500 hover:bg-indigo-600 text-white text-sm py-1 px-3 rounded"
              >
                Test Audio-Video Sync
              </button>
              
              {syncTestResult && (
                <div className={`mt-2 p-2 text-xs rounded ${
                  syncTestResult.success ? 'bg-green-100 text-green-700' : 'bg-amber-100 text-amber-700'
                }`}>
                  {syncTestResult.success ? '✓ ' : '⚠ '}
                  {syncTestResult.details}
                </div>
              )}
            </div>
          )}
        </div>
      </div>
      
      <div className="bg-white p-6 rounded-lg shadow-md">
        <div className="mb-6">
          <h2 className="text-xl font-semibold mb-2">Audio Recorder</h2>
          <p className="text-sm text-gray-500 mb-4">
            Recording format: {recordingFormat || 'Not determined yet'}
          </p>
          
          <div className="flex items-center space-x-4 mb-4">
            {!isRecording ? (
              <button
                onClick={startRecording}
                className="flex items-center px-4 py-2 bg-red-500 text-white rounded-md hover:bg-red-600"
              >
                <span className="h-3 w-3 bg-white rounded-full mr-2"></span>
                Start Recording
              </button>
            ) : (
              <button
                onClick={stopRecording}
                className="flex items-center px-4 py-2 bg-gray-700 text-white rounded-md hover:bg-gray-800"
              >
                <span className="h-3 w-3 bg-white mr-2"></span>
                Stop Recording
              </button>
            )}
            <span className="text-gray-500">
              {isRecording ? `Recording: ${formatTime(elapsedTime)}` : ''}
            </span>
          </div>
          
          {isRecording && (
            <div className="flex items-center mb-4">
              <span className="h-3 w-3 bg-red-500 rounded-full animate-pulse mr-2"></span>
              <span className="text-red-500">Microphone active</span>
            </div>
          )}
        </div>
        
        {recordedAudio && (
          <div className="border-t pt-4">
            <h2 className="text-xl font-semibold mb-4">Audio Playback Comparison</h2>
            
            <div className="grid grid-cols-1 md:grid-cols-2 gap-6">
              {/* Direct Playback */}
              <div className="bg-gray-100 p-4 rounded-md">
                <h3 className="font-semibold text-lg mb-2 text-blue-700">1. Direct Playback</h3>
                <p className="text-sm text-gray-500 mb-2">
                  Direct blob URL playback (known to work)
                </p>
                
                <audio 
                  ref={audioRef}
                  src={recordedAudio} 
                  controls 
                  className="w-full mb-3"
                />
                
                <div className="text-xs text-gray-600 mb-2">
                  Recording length: {formatTime(recordingDuration)}
                </div>
                
                <div className="mt-2 flex flex-wrap gap-2">
                  <button
                    onClick={() => {
                      if (audioRef.current) {
                        audioRef.current.currentTime = 0;
                        
                        // If sync is enabled, start the video first
                        if (syncWithVideo && videoPlayerRef.current) {
                          // Reset the video to the beginning
                          videoPlayerRef.current.currentTime = 0;
                          
                          // Start playing the video
                          const playPromise = videoPlayerRef.current.play();
                          
                          // Video play is a promise that may be rejected (e.g., if user hasn't interacted with page)
                          playPromise.then(() => {
                            // Once video is playing, play the audio
                            audioRef.current?.play();
                            setVideoPlaying(true);
                          }).catch(err => {
                            console.warn('Video play failed, playing audio only:', err);
                            // Fall back to audio-only if video fails
                            audioRef.current?.play();
                          });
                        } else {
                          // Just play audio if sync is disabled
                          audioRef.current.play();
                        }
                      }
                    }}
                    className="px-3 py-1 bg-blue-500 text-white text-sm rounded hover:bg-blue-600"
                  >
                    Replay
                  </button>
                  
                  <button
                    onClick={() => {
                      const a = document.createElement('a');
                      a.href = recordedAudio;
                      a.download = `audio-recording-${new Date().toISOString()}.webm`;
                      document.body.appendChild(a);
                      a.click();
                      document.body.removeChild(a);
                    }}
                    className="px-3 py-1 bg-green-500 text-white text-sm rounded hover:bg-green-600"
                  >
                    Download
                  </button>
                </div>
              </div>
              
              {/* Serialized Playback */}
              <div className="bg-gray-100 p-4 rounded-md">
                <h3 className="font-semibold text-lg mb-2 text-purple-700">2. Serialized Playback</h3>
                <p className="text-sm text-gray-500 mb-2">
                  JSON serialized and deserialized (mimics main app)
                </p>
                
                {deserializedAudio ? (
                  <audio 
                    ref={audioSerializedRef}
                    src={deserializedAudio} 
                    controls 
                    className="w-full mb-3"
                  />
                ) : (
                  <div className="w-full h-12 bg-gray-200 flex items-center justify-center text-gray-500 mb-3">
                    Waiting for deserialization...
                  </div>
                )}
                
                <div className="text-xs text-gray-600 mb-2">
                  Conversion: Blob → base64 → Blob → URL
                </div>
                
                <div className="mt-2 flex flex-wrap gap-2">
                  <button
                    onClick={() => {
                      if (audioSerializedRef.current) {
                        audioSerializedRef.current.currentTime = 0;
                        
                        // If sync is enabled, start the video first
                        if (syncWithVideo && videoPlayerRef.current) {
                          // Reset the video to the beginning
                          videoPlayerRef.current.currentTime = 0;
                          
                          // Start playing the video
                          const playPromise = videoPlayerRef.current.play();
                          
                          // Video play is a promise that may be rejected (e.g., if user hasn't interacted with page)
                          playPromise.then(() => {
                            // Once video is playing, play the audio
                            audioSerializedRef.current?.play();
                            setVideoPlaying(true);
                          }).catch(err => {
                            console.warn('Video play failed, playing audio only:', err);
                            // Fall back to audio-only if video fails
                            audioSerializedRef.current?.play();
                          });
                        } else {
                          // Just play audio if sync is disabled
                          audioSerializedRef.current.play();
                        }
                      }
                    }}
                    disabled={!deserializedAudio}
                    className={`px-3 py-1 text-white text-sm rounded ${deserializedAudio ? 'bg-purple-500 hover:bg-purple-600' : 'bg-gray-400'}`}
                  >
                    Replay
                  </button>
                  
                  <button
                    onClick={async () => {
                      const jsonRepresentation = await createJsonRepresentation();
                      if (jsonRepresentation) {
                        const a = document.createElement('a');
                        const blob = new Blob([jsonRepresentation], { type: 'application/json' });
                        a.href = URL.createObjectURL(blob);
                        a.download = `audio-chunk-${new Date().toISOString()}.json`;
                        document.body.appendChild(a);
                        a.click();
                        document.body.removeChild(a);
                        URL.revokeObjectURL(a.href);
                      }
                    }}
                    disabled={!audioChunk}
                    className={`px-3 py-1 text-white text-sm rounded ${audioChunk ? 'bg-indigo-500 hover:bg-indigo-600' : 'bg-gray-400'}`}
                  >
                    Export JSON
                  </button>
                  
                  <button
                    onClick={async () => {
                      if (!audioChunk) return;
                      
                      setPlaybackTestRunning(true);
                      setPlaybackTestResult(null);
                      
                      try {
                        const success = await testAudioChunkPlayback(audioChunk);
                        setPlaybackTestResult({
                          success,
                          details: success 
                            ? "Playback test completed successfully!" 
                            : "Playback test failed. Check console for details."
                        });
                      } catch (error) {
                        setPlaybackTestResult({
                          success: false,
                          details: `Test error: ${error instanceof Error ? error.message : String(error)}`
                        });
                      } finally {
                        setPlaybackTestRunning(false);
                      }
                    }}
                    disabled={!audioChunk || playbackTestRunning}
                    className={`px-3 py-1 text-white text-sm rounded ${
                      !audioChunk || playbackTestRunning 
                        ? 'bg-gray-400' 
                        : 'bg-yellow-500 hover:bg-yellow-600'
                    }`}
                  >
                    {playbackTestRunning ? 'Testing...' : 'Test Playback'}
                  </button>
                </div>
                
                {playbackTestResult && (
                  <div className={`mt-3 p-2 text-xs rounded ${
                    playbackTestResult.success 
                      ? 'bg-green-100 text-green-700' 
                      : 'bg-red-100 text-red-700'
                  }`}>
                    <strong>{playbackTestResult.success ? 'Success:' : 'Error:'}</strong> {playbackTestResult.details}
                  </div>
                )}
                
                {debugInfo.serializationError && (
                  <div className="mt-3 p-2 bg-red-100 text-red-700 text-xs rounded">
                    <strong>Error:</strong> {debugInfo.serializationError}
                  </div>
                )}
              </div>
            </div>
            
            <div className="mt-4 flex justify-center">
              <button
                onClick={() => {
                  setRecordedAudio(null);
                  setRecordedBlob(null);
                  setSerializedAudio(null);
                  setDeserializedAudio(null);
                  setAudioChunk(null);
                  setDebugInfo({});
                  chunksRef.current = [];
                }}
                className="px-3 py-1 bg-gray-500 text-white text-sm rounded hover:bg-gray-600"
              >
                Clear All
              </button>
            </div>
          </div>
        )}
        
        {/* Serialization Debug Information */}
        {Object.keys(debugInfo).length > 0 && (
          <div className="mt-6 border-t pt-4">
            <h3 className="font-semibold text-lg mb-2">Debug Information</h3>
            
            <div className="bg-gray-800 text-green-300 p-4 rounded font-mono text-xs overflow-x-auto">
              <pre>
{`Audio Serialization Debug:
------------------------
Original Blob:
  Size: ${debugInfo.originalBlob?.size} bytes (${debugInfo.originalBlob?.size ? (debugInfo.originalBlob.size / 1024).toFixed(2) : '?'} KB)
  Type: ${debugInfo.originalBlob?.type}
  MIME: ${debugInfo.originalBlob?.mimeType}

Base64 Conversion:
  Length: ${debugInfo.base64String?.length} characters
  Time: ${debugInfo.base64String?.conversionTimeMs}ms
  Preview: ${debugInfo.base64String?.preview}

Deserialized Blob:
  Size: ${debugInfo.deserializedBlob?.size} bytes (${debugInfo.deserializedBlob?.size ? (debugInfo.deserializedBlob.size / 1024).toFixed(2) : '?'} KB)
  Type: ${debugInfo.deserializedBlob?.type}
  Time: ${debugInfo.deserializedBlob?.conversionTimeMs}ms

Comparison:
  Size Match: ${debugInfo.comparison?.sizeMatch ? '✅ Yes' : '❌ No'}
  Type Match: ${debugInfo.comparison?.typeMatch ? '✅ Yes' : '❌ No'}
  Size Difference: ${debugInfo.comparison?.diffBytes || 0} bytes
`}
              </pre>
            </div>
            
            {/* Add base64 string preview and download button */}
            {serializedAudio && (
              <div className="mt-4">
                <h4 className="font-semibold mb-2">Base64 String (first 100 chars)</h4>
                <div className="bg-gray-100 p-3 rounded overflow-x-auto">
                  <code className="text-xs break-all">
                    {serializedAudio.substring(0, 100)}...
                  </code>
                </div>
                <button
                  onClick={() => {
                    if (serializedAudio) {
                      const a = document.createElement('a');
                      const blob = new Blob([serializedAudio], { type: 'text/plain' });
                      a.href = URL.createObjectURL(blob);
                      a.download = `audio-base64-${new Date().toISOString()}.txt`;
                      document.body.appendChild(a);
                      a.click();
                      document.body.removeChild(a);
                      URL.revokeObjectURL(a.href);
                    }
                  }}
                  className="mt-2 px-3 py-1 bg-gray-500 text-white text-sm rounded hover:bg-gray-600"
                >
                  Download Base64 String
                </button>
              </div>
            )}
          </div>
        )}
        
        <div className="mt-8 border-t pt-4">
          <h3 className="font-semibold text-lg mb-2">Cross-Browser Compatibility</h3>
          
          <div className="bg-blue-50 p-4 rounded-md mb-4">
            <p className="text-sm text-blue-800 mb-2">
              This section helps diagnose audio recording and playback issues across different browsers.
              Below is a compatibility chart based on our testing.
            </p>
            
            <div className="overflow-x-auto">
              <table className="w-full text-xs border-collapse">
                <thead>
                  <tr className="bg-blue-100">
                    <th className="border border-blue-200 p-2">Browser</th>
                    <th className="border border-blue-200 p-2">Direct Recording</th>
                    <th className="border border-blue-200 p-2">Direct Playback</th>
                    <th className="border border-blue-200 p-2">Serialized Playback</th>
                    <th className="border border-blue-200 p-2">Preferred Format</th>
                    <th className="border border-blue-200 p-2">Notes</th>
                  </tr>
                </thead>
                <tbody>
                  <tr>
                    <td className="border border-blue-200 p-2">Chrome (Desktop)</td>
                    <td className="border border-blue-200 p-2 text-green-600">✓ Works</td>
                    <td className="border border-blue-200 p-2 text-green-600">✓ Works</td>
                    <td className="border border-blue-200 p-2 text-green-600">✓ Works</td>
                    <td className="border border-blue-200 p-2">audio/webm;codecs=opus</td>
                    <td className="border border-blue-200 p-2">Most reliable</td>
                  </tr>
                  <tr>
                    <td className="border border-blue-200 p-2">Firefox (Desktop)</td>
                    <td className="border border-blue-200 p-2 text-green-600">✓ Works</td>
                    <td className="border border-blue-200 p-2 text-green-600">✓ Works</td>
                    <td className="border border-blue-200 p-2 text-green-600">✓ Works</td>
                    <td className="border border-blue-200 p-2">audio/webm</td>
                    <td className="border border-blue-200 p-2">Works well</td>
                  </tr>
                  <tr>
                    <td className="border border-blue-200 p-2">Safari (Desktop)</td>
                    <td className="border border-blue-200 p-2 text-yellow-600">⚠ Variable</td>
                    <td className="border border-blue-200 p-2 text-green-600">✓ Works</td>
                    <td className="border border-blue-200 p-2 text-yellow-600">⚠ Variable</td>
                    <td className="border border-blue-200 p-2">audio/mp4</td>
                    <td className="border border-blue-200 p-2">May require explicit user interaction</td>
                  </tr>
                  <tr>
                    <td className="border border-blue-200 p-2">Chrome (Mobile)</td>
                    <td className="border border-blue-200 p-2 text-green-600">✓ Works</td>
                    <td className="border border-blue-200 p-2 text-green-600">✓ Works</td>
                    <td className="border border-blue-200 p-2 text-green-600">✓ Works</td>
                    <td className="border border-blue-200 p-2">audio/webm;codecs=opus</td>
                    <td className="border border-blue-200 p-2">Touch interaction required</td>
                  </tr>
                  <tr>
                    <td className="border border-blue-200 p-2">Safari (iOS)</td>
                    <td className="border border-blue-200 p-2 text-yellow-600">⚠ Variable</td>
                    <td className="border border-blue-200 p-2 text-green-600">✓ Works</td>
                    <td className="border border-blue-200 p-2 text-yellow-600">⚠ Variable</td>
                    <td className="border border-blue-200 p-2">audio/mp4</td>
                    <td className="border border-blue-200 p-2">Requires explicit user interaction</td>
                  </tr>
                </tbody>
              </table>
            </div>
          </div>
          
          <div className="text-xs text-gray-500">
            <h4 className="font-semibold mb-1">Your Browser Information:</h4>
            <ul className="list-disc pl-5 space-y-1">
              <li>
                <BrowserInfo />
              </li>
              <li>
                <MimeTypesInfo />
              </li>
            </ul>
          </div>
        </div>
      </div>
    </div>
  );
}
|| END ||


|| START ./app/layout.tsx ||

import type { Metadata } from "next";
import { Geist, Geist_Mono } from "next/font/google";
import "./globals.css";
import { AppProviders } from "@/src/contexts/AppProviders";
import { Navbar } from "@/src/components/Navbar";

const geistSans = Geist({
  variable: "--font-geist-sans",
  subsets: ["latin"],
});

const geistMono = Geist_Mono({
  variable: "--font-geist-mono",
  subsets: ["latin"],
});

export const metadata: Metadata = {
  title: "Golf Swing Analysis Tool",
  description: "Interactive tool for analyzing and annotating golf swing videos",
};

export default function RootLayout({
  children,
}: Readonly<{
  children: React.ReactNode;
}>) {
  return (
    <html lang="en">
      <body
        className={`${geistSans.variable} ${geistMono.variable} antialiased`}
      >
        <AppProviders>
          <Navbar />
          <main className="container mx-auto p-4">
            {children}
          </main>
        </AppProviders>
      </body>
    </html>
  );
}
|| END ||


|| START ./app/api/auth/register/route.ts ||

import { NextResponse } from "next/server";
import { createUser, findUserByEmail } from "@/src/lib/auth";

export async function POST(request: Request) {
  try {
    const { email, name, password } = await request.json();

    // Validate input
    if (!email || !name || !password) {
      return NextResponse.json(
        { message: "Missing required fields" },
        { status: 400 }
      );
    }

    // Check if user already exists
    const existingUser = await findUserByEmail(email);
    if (existingUser) {
      return NextResponse.json(
        { message: "User with this email already exists" },
        { status: 409 }
      );
    }

    // Create new user
    const user = await createUser(email, name, password);

    // Return success response without sensitive data
    return NextResponse.json(
      { 
        id: user.id, 
        email: user.email, 
        name: user.name,
        message: "User registered successfully" 
      },
      { status: 201 }
    );
  } catch (error) {
    console.error("Registration error:", error);
    return NextResponse.json(
      { message: "An error occurred during registration" },
      { status: 500 }
    );
  }
}

|| END ||


|| START ./app/api/auth/[...nextauth]/route.ts ||

import NextAuth from "next-auth";
import CredentialsProvider from "next-auth/providers/credentials";
import { compare } from "bcrypt";

// Import auth helpers
import { findUserByEmail } from "@/src/lib/auth";

export const authOptions = {
  providers: [
    CredentialsProvider({
      name: "Credentials",
      credentials: {
        email: { label: "Email", type: "email" },
        password: { label: "Password", type: "password" }
      },
      async authorize(credentials) {
        if (!credentials?.email || !credentials?.password) {
          return null;
        }
        
        const user = await findUserByEmail(credentials.email);
        if (!user) {
          return null;
        }
        
        const isPasswordValid = await compare(
          credentials.password,
          user.hashedPassword
        );
        
        if (!isPasswordValid) {
          return null;
        }
        
        return {
          id: user.id,
          email: user.email,
          name: user.name,
        };
      }
    }),
  ],
  callbacks: {
    async session({ session, token }: { session: any, token: any }) {
      if (token) {
        session.user.id = token.id;
      }
      return session;
    },
    async jwt({ token, user }: { token: any, user: any }) {
      if (user) {
        token.id = user.id;
      }
      return token;
    }
  },
  session: {
    strategy: "jwt",
  },
  pages: {
    signIn: "/auth/signin",
  },
};

const handler = NextAuth(authOptions);
export { handler as GET, handler as POST };

|| END ||


|| START ./app/api/videos/route.ts ||

import { NextResponse } from 'next/server';
import { CosmosClient } from '@azure/cosmos';

// Validate required environment variables
if (!process.env.COSMOS_ENDPOINT) {
  throw new Error('COSMOS_ENDPOINT environment variable is required');
}

if (!process.env.COSMOS_KEY) {
  throw new Error('COSMOS_KEY environment variable is required');
}

if (!process.env.COSMOS_DATABASE_ID) {
  throw new Error('COSMOS_DATABASE_ID environment variable is required');
}

if (!process.env.COSMOS_CONTAINER_ID) {
  throw new Error('COSMOS_CONTAINER_ID environment variable is required');
}

// Cosmos DB connection configuration
const endpoint = process.env.COSMOS_ENDPOINT;
const key = process.env.COSMOS_KEY;
const databaseId = process.env.COSMOS_DATABASE_ID;
const containerId = process.env.COSMOS_CONTAINER_ID;

// Initialize the Cosmos client
const client = new CosmosClient({ endpoint, key });
const database = client.database(databaseId);
const container = database.container(containerId);

export async function GET(request: Request) {
  const { searchParams } = new URL(request.url);
  const id = searchParams.get('id');
  const status = searchParams.get('status');

  try {
    let querySpec;
    
    if (id) {
      // Fetch specific video by ID
      querySpec = {
        query: "SELECT * FROM c WHERE c.id = @id",
        parameters: [{ name: "@id", value: id }]
      };
    } else if (status && status !== 'All') {
      // Filter by status
      querySpec = {
        query: "SELECT * FROM c WHERE c.status = @status",
        parameters: [{ name: "@status", value: status }]
      };
    } else {
      // Get all videos
      querySpec = {
        query: "SELECT * FROM c"
      };
    }

    const { resources: videos } = await container.items.query(querySpec).fetchAll();
    
    return NextResponse.json(videos, { status: 200 });
  } catch (error) {
    console.error('Error fetching videos from Cosmos DB:', error);
    return NextResponse.json(
      { error: 'Failed to fetch videos from database' },
      { status: 500 }
    );
  }
}

export async function POST(request: Request) {
  try {
    const video = await request.json();
    
    // Validate required fields
    if (!video.title || !video.videoUrl) {
      return NextResponse.json(
        { error: 'Missing required fields: title, videoUrl' },
        { status: 400 }
      );
    }
    
    // Ensure the video has an ID or generate one
    if (!video.id) {
      video.id = `video-${Date.now()}`;
    }
    
    // Add timestamp if not present
    if (!video.dateAdded) {
      video.dateAdded = new Date().toISOString().split('T')[0];
    }
    
    // Set default status if not provided
    if (!video.status) {
      video.status = 'Not Started';
    }
    
    const { resource: createdVideo } = await container.items.create(video);
    
    return NextResponse.json(createdVideo, { status: 201 });
  } catch (error) {
    console.error('Error creating video in Cosmos DB:', error);
    return NextResponse.json(
      { error: 'Failed to create video in database' },
      { status: 500 }
    );
  }
}

export async function PUT(request: Request) {
  try {
    const video = await request.json();
    
    if (!video.id) {
      return NextResponse.json(
        { error: 'Missing video ID' },
        { status: 400 }
      );
    }
    
    // Get the existing item to preserve the _rid, _self properties required by Cosmos DB
    const { resource: existingVideo } = await container.item(video.id, video.id).read();
    
    if (!existingVideo) {
      return NextResponse.json(
        { error: 'Video not found' },
        { status: 404 }
      );
    }
    
    // Update the status to "Completed" if a reviewSession is present and status isn't already "Archived"
    if (video.reviewSession && video.status !== "Archived") {
      video.status = "Completed";
    }
    
    const { resource: updatedVideo } = await container.item(video.id, video.id).replace(video);
    
    return NextResponse.json(updatedVideo, { status: 200 });
  } catch (error) {
    console.error('Error updating video in Cosmos DB:', error);
    return NextResponse.json(
      { error: 'Failed to update video in database' },
      { status: 500 }
    );
  }
}
|| END ||


|| START ./app/api/audio/[...path]/route.ts ||

import { NextRequest, NextResponse } from 'next/server';
import { BlobServiceClient } from '@azure/storage-blob';

// Azure Storage account configuration from environment variables
const connectionString = process.env.AZURE_STORAGE_CONNECTION_STRING || '';
const containerName = process.env.AZURE_STORAGE_CONTAINER_NAME || 'audio-recordings';

export async function GET(
  request: NextRequest,
  context: { params: { path: string[] } }
) {
  try {
    if (!connectionString) {
      return NextResponse.json(
        { error: 'Azure Storage connection string not configured' },
        { status: 500 }
      );
    }

    // Safely access params after awaiting
    const { params } = context;
    
    // Join the path parts to get the blob name
    const blobName = Array.isArray(params.path) ? params.path.join('/') : '';
    
    if (!blobName) {
      return NextResponse.json(
        { error: 'No blob path specified' },
        { status: 400 }
      );
    }

    console.log(`Proxying request for blob: ${blobName}`);
    
    // Initialize the BlobServiceClient
    const blobServiceClient = BlobServiceClient.fromConnectionString(connectionString);
    const containerClient = blobServiceClient.getContainerClient(containerName);
    const blobClient = containerClient.getBlobClient(blobName);
    
    // Get the blob properties to check if it exists
    const properties = await blobClient.getProperties();
    
    try {
      // Download the blob as a buffer instead of a stream
      const downloadResponse = await blobClient.downloadToBuffer();
      
      if (!downloadResponse || downloadResponse.length === 0) {
        return NextResponse.json(
          { error: 'Failed to download blob or empty blob' },
          { status: 500 }
        );
      }
      
      console.log(`Successfully downloaded blob: ${blobName}, size: ${downloadResponse.length} bytes`);
      
      // Create response with appropriate headers
      const response = new NextResponse(downloadResponse);
      response.headers.set('Content-Type', properties.contentType || 'audio/webm');
      response.headers.set('Content-Length', properties.contentLength?.toString() || downloadResponse.length.toString());
      response.headers.set('Accept-Ranges', 'bytes');
      response.headers.set('Cache-Control', 'max-age=3600');
      
      return response;
    } catch (downloadError) {
      console.error('Error downloading blob:', downloadError);
      
      // Alternative approach: download the blob directly
      try {
        console.log('Trying alternative download approach...');
        // Get a download URL with SAS token
        const downloadUrl = blobClient.url;
        
        // Fetch the blob directly from the URL
        const response = await fetch(downloadUrl);
        
        if (!response.ok) {
          throw new Error(`Failed to fetch blob: ${response.status} ${response.statusText}`);
        }
        
        // Get the data as an ArrayBuffer
        const arrayBuffer = await response.arrayBuffer();
        const buffer = Buffer.from(arrayBuffer);
        
        console.log(`Successfully downloaded blob using alternative method: ${blobName}, size: ${buffer.length} bytes`);
        
        // Create response with appropriate headers
        const nextResponse = new NextResponse(buffer);
        nextResponse.headers.set('Content-Type', properties.contentType || 'audio/webm');
        nextResponse.headers.set('Content-Length', buffer.length.toString());
        nextResponse.headers.set('Accept-Ranges', 'bytes');
        nextResponse.headers.set('Cache-Control', 'max-age=3600');
        
        return nextResponse;
      } catch (altError) {
        console.error('Alternative download method also failed:', altError);
        return NextResponse.json(
          { error: 'Failed to download blob using multiple methods' },
          { status: 500 }
        );
      }
    }
    
    // This code is unreachable - kept here in case we need it later
    return NextResponse.json(
      { error: 'Invalid code path reached' },
      { status: 500 }
    );
  } catch (error) {
    console.error('Error proxying blob:', error);
    return NextResponse.json(
      { error: 'Failed to proxy blob' },
      { status: 500 }
    );
  }
}
|| END ||


|| START ./app/api/audio/route.ts ||

import { NextRequest, NextResponse } from 'next/server';
import { uploadAudioBlob, ensureContainer } from '@/src/utils/azureStorage';

// Initialize the container when the server starts
ensureContainer().catch(err => {
  console.error('Failed to initialize Azure Storage container:', err);
});

export async function POST(request: NextRequest): Promise<NextResponse> {
  try {
    // Get the form data from the request
    const formData = await request.formData();
    const audioFile = formData.get('audio') as File;
    const sessionId = formData.get('sessionId') as string;
    
    if (!audioFile || !sessionId) {
      return NextResponse.json(
        { error: 'Missing required fields: audio, sessionId' },
        { status: 400 }
      );
    }
    
    // Convert File to Blob
    const audioBuffer = await audioFile.arrayBuffer();
    
    if (!audioBuffer) {
      console.error('Failed to get array buffer from file');
      return NextResponse.json(
        { error: 'Failed to process audio file' },
        { status: 500 }
      );
    }
    
    const audioBlob = new Blob([audioBuffer], { type: audioFile.type });
    
    // Upload the audio blob to Azure Storage
    const blobUrl = await uploadAudioBlob(audioBlob, sessionId);
    
    // Return the blob URL
    return NextResponse.json({ url: blobUrl }, { status: 200 });
  } catch (error) {
    console.error('Error uploading audio blob:', error);
    return NextResponse.json(
      { error: 'Failed to upload audio blob' },
      { status: 500 }
    );
  }
}
|| END ||


|| START ./app/page.tsx ||

'use client';

import Link from "next/link";
import VideoPlayerWrapper from "../src/components/VideoPlayerWrapper";
import { useState, useCallback, useEffect } from "react";
import { useSearchParams } from 'next/navigation';

export default function Home() {
  // Interface for review content configuration
  interface DataLabelingProperty {
    id: string;
    label: string;
  }
  
  interface KeyMetric {
    name: string;
    value: string | number;
  }

  interface ReviewContent {
    videoUrl: string;
    videoTitle?: string;
    videoDescription?: string;
    dataLabelingTitle: string;
    labelProperties: DataLabelingProperty[];
    keyMetricsTitle?: string;
    keyMetrics?: KeyMetric[];
  }
  
  // Get video ID from URL if present
  const searchParams = useSearchParams();
  const videoId = searchParams.get('videoId');
  
  // State for selected video
  const [contentToReview, setContentToReview] = useState<ReviewContent | null>(null);
  
  // Load video data if videoId is provided
  useEffect(() => {
    if (!videoId) {
      // Initialize with empty content if no video is selected
      setContentToReview(null);
      return;
    }
    
    const loadVideoData = async () => {
      try {
        // Fetch specific video by ID from Cosmos DB via API
        const response = await fetch(`/api/videos?id=${videoId}`);
        if (!response.ok) {
          throw new Error(`Failed to fetch video: ${response.status}`);
        }
        
        const videos = await response.json();
        
        // API returns an array even when querying by ID
        if (videos && videos.length > 0) {
          const selectedVideo = videos[0];
          
          // Convert video data to ReviewContent format
          const metricsArray = Object.entries(selectedVideo.metrics).map(([name, value]) => ({ name, value }));
          
          setContentToReview({
            videoUrl: selectedVideo.videoUrl,
            videoTitle: selectedVideo.title,
            videoDescription: selectedVideo.description,
            dataLabelingTitle: selectedVideo.dataLabelingTitle || "Animation Categories",
            labelProperties: selectedVideo.labelProperties || [
              { id: "artisticStyle", label: "Artistic Style" },
              { id: "characterDesign", label: "Character Design" },
              { id: "motionDynamics", label: "Motion Dynamics" },
              { id: "colorPalette", label: "Color Palette" },
              { id: "narrativeTechniques", label: "Narrative Techniques" },
            ],
            keyMetricsTitle: selectedVideo.keyMetricsTitle || "Swing Metrics",
            keyMetrics: metricsArray
          });
          
          // Check if this video has a saved review session
          if (selectedVideo.reviewSession) {
            console.log("Found saved review session:", selectedVideo.reviewSession);
            setSavedReviewSession(selectedVideo.reviewSession);
            
            // If status is "Completed", make session available for replay but don't auto-start
            if (selectedVideo.status === "Completed") {
              console.log("Completed review found, making session available for replay");
              window.__hasRecordedSession = true;
              window.__isCompletedVideo = true; // Mark as already completed
              const event = new CustomEvent('session-available');
              window.dispatchEvent(event);
            }
          }
        }
      } catch (error) {
        console.error("Error loading video:", error);
      }
    };
    
    loadVideoData();
  }, [videoId]);
  
  // State for star ratings during recording
  const [categories, setCategories] = useState<Record<string, number | null>>({});
  
  // State to track if we're in replay mode
  const [isReplayMode, setIsReplayMode] = useState(false);
  
  // State to track active recording
  const [isRecording, setIsRecording] = useState(false);
  
  // State for category list with ratings that will be shown during replay
  const [categoryList, setCategoryList] = useState<{name: string, rating: number}[]>([]);
  
  // Define window type with our custom properties
  declare global {
    interface Window {
      __videoPlayerWrapper?: {
        recordCategoryChange: (category: string, rating: number) => void;
        isRecording: boolean;
      };
      __hasRecordedSession?: boolean;
      __isCompletedVideo?: boolean;
      __sessionReady?: boolean;
      __isReplaying?: boolean;
    }
  }
  
  // State tracking
  const [hasRecordedSession, setHasRecordedSession] = useState(false);
  const [isCompletedVideo, setIsCompletedVideo] = useState(false);
  const [isSessionReady, setIsSessionReady] = useState(false);
  
  // Get formatted category label
  const getCategoryLabel = (category: string) => {
    return category.replace(/([A-Z])/g, ' $1').trim().replace(/^./, str => str.toUpperCase());
  };
  
  const handleCategoryChange = (category: string, rating: number) => {
    console.log(`RECORDING: Changing category ${category} to rating ${rating}`);
    
    const newCategories = {
      ...categories,
      [category]: rating,
    };
    
    console.log('RECORDING: New categories state:', newCategories);
    setCategories(newCategories);
    
    // Record the category change event in the orchestrator if we're recording
    if (typeof window !== 'undefined') {
      console.log(`RECORDING: Sending category event to orchestrator: ${category}=${rating}`);
      
      try {
        // Always send to record category changes, regardless of recording state
        // This ensures categories are saved properly even when not actively recording
        if (window.__videoPlayerWrapper?.recordCategoryChange) {
          window.__videoPlayerWrapper.recordCategoryChange(category, rating);
          console.log('RECORDING: Category event sent successfully');
        } else {
          console.warn('RECORDING: recordCategoryChange function not available');
        }
      } catch (error) {
        console.error('RECORDING: Error sending category event:', error);
      }
    } else {
      console.warn('RECORDING: Window not defined, cannot send category event');
    }
  };
  
  // Function to clear all categories (called when recording stops)
  const clearCategories = useCallback(() => {
    // Reset all categories to null (no rating)
    const resetCategories = Object.keys(categories).reduce((acc, key) => {
      acc[key] = null;
      return acc;
    }, {} as Record<string, number | null>);
    
    setCategories(resetCategories);
    
    // Also clear the category list for replay mode
    setCategoryList([]);
  }, [categories]);
  
  // Function to handle categories during replay
  const handleCategoryAddedDuringReplay = useCallback((categoryChanges: Record<string, number>) => {
    console.log('PARENT: Received categories for replay:', categoryChanges);
    
    // Debug log all entries
    Object.entries(categoryChanges).forEach(([key, value]) => {
      console.log(`PARENT: Category ${key} = ${value}`);
    });
    
    // Convert all rated categories to formatted objects with name and rating
    const ratedCategories = Object.entries(categoryChanges)
      .filter(([_, rating]) => rating !== null && rating > 0)
      .map(([categoryName, rating]) => {
        const label = getCategoryLabel(categoryName);
        console.log(`PARENT: Formatting category ${categoryName} to ${label} with rating ${rating}`);
        return {
          id: categoryName, // Keep the original ID
          name: label,
          rating: rating as number
        };
      });
    
    if (ratedCategories.length > 0) {
      console.log(`PARENT: Adding ${ratedCategories.length} categories to replay list:`, ratedCategories);
      
      // Force state update with a deep copy and force render with a callback
      const newList = [...ratedCategories];
      console.log('PARENT: Setting category list to:', newList);
      
      // Replace the entire list at once with a forced update - don't set replay mode here
      setCategoryList(newList);
      
      // Force UI update by using double setState in different ticks for React 18+ batching
      setTimeout(() => {
        setCategoryList(prevList => {
          console.log('PARENT: Forced update, category list is now:', prevList);
          return prevList; // Return same array but force an update
        });
        
        // Double-check in the next tick
        setTimeout(() => {
          console.log('PARENT: After update, category list should be:', newList);
        }, 100);
      }, 50);
    } else {
      console.log('PARENT: No rated categories found');
      setCategoryList([]);
    }
  }, []);
  
  // Function to handle replay mode change
  const handleReplayModeChange = useCallback((isReplay: boolean) => {
    console.log(`Setting replay mode to: ${isReplay}`);
    setIsReplayMode(isReplay);
    
    // Ensure the UI knows session is being replayed when mode changes
    if (isReplay && typeof window !== 'undefined') {
      // Update window state when entering replay mode
      window.__isReplaying = true;
    } else if (typeof window !== 'undefined') {
      window.__isReplaying = false;
    }
    
    // Clear the category list when entering/exiting replay mode
    if (isReplay) {
      setCategoryList([]);
    }
  }, []);
  
  // Function to update recording state (attached to global window object for access)
  useEffect(() => {
    // Check window object for recording state and session availability
    const checkState = () => {
      if (typeof window !== 'undefined') {
        // Check recording state
        if (window.__videoPlayerWrapper) {
          setIsRecording(!!window.__videoPlayerWrapper.isRecording);
        }
        
        // Check session availability
        const hasSession = !!window.__hasRecordedSession;
        if (hasSession !== hasRecordedSession) {
          console.log(`Session availability changed: ${hasSession}`);
          setHasRecordedSession(hasSession);
        }
        
        // Check if this is a completed video
        const isCompleted = !!window.__isCompletedVideo;
        if (isCompleted !== isCompletedVideo) {
          console.log(`Completed video status changed: ${isCompleted}`);
          setIsCompletedVideo(isCompleted);
        }
        
        // Check if session is ready for replay
        const sessionReady = !!window.__sessionReady;
        if (sessionReady !== isSessionReady) {
          console.log(`Session ready status changed: ${sessionReady}`);
          setIsSessionReady(sessionReady);
        }
        
        // Check if session is actively being replayed
        const isReplayActive = !!window.__isReplaying;
        if (isReplayActive !== isReplayMode) {
          console.log(`Replay active status changed: ${isReplayActive}`);
          setIsReplayMode(isReplayActive);
        }
      }
    };
    
    // Initial check
    checkState();
    
    // Set up interval to periodically check state
    const interval = setInterval(checkState, 300);
    
    // Listen for session availability events
    const handleSessionChange = () => {
      console.log('Received session change event');
      checkState();
    };
    
    const handleSessionReady = () => {
      console.log('Received session ready event');
      checkState();
    };
    
    window.addEventListener('session-available', handleSessionChange);
    window.addEventListener('session-ready', handleSessionReady);
    
    return () => {
      clearInterval(interval);
      window.removeEventListener('session-available', handleSessionChange);
      window.removeEventListener('session-ready', handleSessionReady);
    };
  }, [hasRecordedSession, isCompletedVideo, isSessionReady]);
  
  // Force client-side rendering for window access
  const [isClient, setIsClient] = useState(false);
  const [savedReviewSession, setSavedReviewSession] = useState(null);
  
  useEffect(() => {
    setIsClient(true);
  }, []);
  
  // Generate initial categories state from labelProperties when content is loaded
  useEffect(() => {
    if (contentToReview?.labelProperties) {
      const initialCats = contentToReview.labelProperties.reduce((acc, prop) => {
        acc[prop.id] = null; // null means no rating
        return acc;
      }, {} as Record<string, number | null>);
      setCategories(initialCats);
    }
  }, [contentToReview]);
  
  // Save the review session to Cosmos DB when a recording is completed
  const onSessionComplete = useCallback(async (session) => {
    console.log('Session complete:', session);
    
    // If this is a review of a specific video from the inbox, save the session to Cosmos DB
    if (videoId) {
      try {
        // First, get the current video data
        const response = await fetch(`/api/videos?id=${videoId}`);
        if (!response.ok) {
          throw new Error(`Failed to fetch video: ${response.status}`);
        }
        
        const videos = await response.json();
        if (videos && videos.length > 0) {
          const video = videos[0];
          
          // Update video with the session data and set status to "Completed"
          const updateResponse = await fetch('/api/videos', {
            method: 'PUT',
            headers: {
              'Content-Type': 'application/json',
            },
            body: JSON.stringify({
              ...video,
              status: "Completed",
              reviewSession: session
            })
          });
          
          if (updateResponse.ok) {
            console.log("Saved review session to Cosmos DB and updated status to Completed");
          } else {
            console.error("Failed to update video with review session:", await updateResponse.text());
          }
        }
      } catch (error) {
        console.error("Error saving review session:", error);
      }
    }
    
    // Update session availability
    if (typeof window !== 'undefined') {
      window.__hasRecordedSession = true;
      
      // Dispatch a custom event to notify about session availability
      const event = new CustomEvent('session-available');
      window.dispatchEvent(event);
    }
  }, [videoId]);

  return (
    <div className="min-h-screen p-4">
      <main className="max-w-4xl mx-auto">
        <div className="mb-4 flex flex-col sm:flex-row justify-between items-center">
          <div className="flex items-center gap-4 mt-4 sm:mt-0">
            <div className="flex space-x-2">
              <button
                onClick={() => document.getElementById(isClient && isRecording ? 'stopButton' : 'startRecordingButton')?.click()}
                disabled={isReplayMode}
                className={isReplayMode ? "bg-gray-300 text-gray-500 py-2 px-4 rounded-md" : isClient && isRecording ? "bg-gray-700 text-white py-2 px-4 rounded-md" : "bg-red-500 text-white py-2 px-4 rounded-md"}
              >
                {isClient && isRecording ? "Stop" : "Record"}
              </button>
              
              <button
                onClick={() => document.getElementById(isReplayMode ? 'stopButton' : 'startReplayButton')?.click()}
                disabled={isClient && 
                  (isRecording || 
                   (!hasRecordedSession && !isReplayMode) || 
                   (hasRecordedSession && !isSessionReady && !isReplayMode))}
                className={
                  isClient && 
                    (isRecording || 
                     (!hasRecordedSession && !isReplayMode) || 
                     (hasRecordedSession && !isSessionReady && !isReplayMode))
                    ? "bg-gray-300 text-gray-500 py-2 px-4 rounded-md" 
                    : isReplayMode 
                      ? "bg-yellow-500 text-white py-2 px-4 rounded-md" 
                      : "bg-green-600 text-white py-2 px-4 rounded-md"
                }
              >
                {isReplayMode 
                  ? "Stop Replay" 
                  : (hasRecordedSession && !isSessionReady) 
                    ? "Loading..." 
                    : "Replay Session"
                }
              </button>
            </div>
          </div>
        </div>
        
        {/* Display video title and description from URL parameter */}
        {videoId && contentToReview && (
          <div className="bg-blue-50 p-4 rounded-lg mb-4">
            <h2 className="text-xl font-semibold">{contentToReview.videoTitle}</h2>
            <p className="text-gray-600">{contentToReview.videoDescription}</p>
          </div>
        )}
        
        <div className="flex flex-col lg:flex-row gap-4">
          {/* Categories Section */}
          {contentToReview && <div className="lg:w-1/4">
            <div className="p-4 border rounded-lg bg-gray-50 h-full">
              <h2 className="text-xl font-semibold mb-3">{contentToReview.dataLabelingTitle}</h2>
              
              {/* Show rating stars during recording mode */}
              {!isReplayMode ? (
                <div className="space-y-3">
                  {contentToReview.labelProperties?.map((property) => (
                    <div key={property.id}>
                      <div className="mb-1">{property.label}</div>
                      <div className="flex items-center">
                        {[1, 2, 3, 4, 5].map((star) => (
                          <button
                            key={star}
                            type="button"
                            onClick={() => handleCategoryChange(property.id, star)}
                            className={categories[property.id] >= star ? "text-xl px-1 text-yellow-400" : "text-xl px-1 text-gray-300"}
                          >
                            ★
                          </button>
                        ))}
                      </div>
                    </div>
                  ))}
                </div>
              ) : (
                // Show ratings during replay mode
                <div>
                  {contentToReview && (
                    <>
                      <ul className="space-y-3">
                        {contentToReview.labelProperties?.map((property) => (
                          <li key={property.id}>
                            <div className="font-medium">{property.label}</div>
                            <div className="flex text-yellow-400 mt-1 text-base">
                              {[1, 2, 3, 4, 5].map((star) => {
                                // Find if we have a rating for this category from replay data
                                // First check by ID (most reliable)
                                const ratingCategoryById = categoryList.find(cat => 
                                  cat.id === property.id
                                );
                                
                                // Then check by name if ID match failed
                                const ratingCategoryByName = !ratingCategoryById ? categoryList.find(cat => 
                                  cat.name?.toLowerCase() === property.label.toLowerCase() ||
                                  cat.name?.toLowerCase().replace(/\s+/g, '') === property.label.toLowerCase().replace(/\s+/g, '')
                                ) : null;
                                
                                // Also check if we have a direct match in the categories object
                                const directRating = categories[property.id];
                                
                                // Use the first available rating
                                const rating = directRating || 
                                              ratingCategoryById?.rating || 
                                              ratingCategoryByName?.rating || 
                                              0;
                                
                                // Debug ratings for the first category
                                if (property.id === "setupAlignment") {
                                  console.log(`Rating for ${property.label}: directRating=${directRating}, byId=${ratingCategoryById?.rating}, byName=${ratingCategoryByName?.rating}, final=${rating}`);
                                }
                                
                                return (
                                  <span key={star} className={star <= rating ? "text-yellow-400" : "text-gray-300"}>
                                    ★
                                  </span>
                                );
                              })}
                            </div>
                          </li>
                        ))}
                      </ul>
                      <p className="text-xs text-gray-500 mt-2">
                        {categoryList.length > 0 ? `${categoryList.length} ${categoryList.length === 1 ? 'category' : 'categories'} rated` : 'No ratings yet'}
                      </p>
                    </>
                  )}
                </div>
              )}
            </div>
          </div>}
          
          {/* Video Player */}
          <div className="lg:w-1/2">
            {contentToReview ? <VideoPlayerWrapper 
              categories={categories}
              onCategoriesCleared={clearCategories}
              onCategoriesLoaded={handleCategoryAddedDuringReplay}
              onReplayModeChange={handleReplayModeChange}
              videoUrl={contentToReview.videoUrl}
              videoId={contentToReview.videoTitle?.replace(/\s+/g, '-').toLowerCase()}
              contentToReview={contentToReview}
              initialSession={savedReviewSession}
              onSessionComplete={onSessionComplete}
            /> : <div className="flex items-center justify-center h-64 bg-gray-100 rounded-lg">
              <p className="text-gray-500">Please select a video to review</p>
            </div>}
            
            {/* Session Data Controls */}
            {hasRecordedSession && (
              <div className="mt-4 p-4 border rounded-lg bg-gray-50">
                <h3 className="text-lg font-semibold mb-2">Recorded Session</h3>
                <div className="flex space-x-2">
                  <button
                    onClick={() => document.getElementById('downloadDataButton')?.click()}
                    disabled={isClient && (isRecording || !hasRecordedSession)}
                    className={isClient && (isRecording || !hasRecordedSession) ? "bg-gray-300 text-gray-500 py-2 px-4 rounded-md" : "bg-blue-500 text-white py-2 px-4 rounded-md"}
                  >
                    Download Data
                  </button>
                  
                  <label className="bg-purple-500 text-white py-2 px-4 rounded-md cursor-pointer inline-block">
                    Load Data
                    <input 
                      type="file" 
                      accept=".json" 
                      onChange={(e) => {
                        const fileInput = document.getElementById('fileUploadInput') as HTMLInputElement;
                        if (fileInput && e.target.files && e.target.files.length > 0) {
                          const dataTransfer = new DataTransfer();
                          dataTransfer.items.add(e.target.files[0]);
                          fileInput.files = dataTransfer.files;
                          const event = new Event('change', { bubbles: true });
                          fileInput.dispatchEvent(event);
                        }
                      }}
                      className="hidden"
                    />
                  </label>
                </div>
              </div>
            )}
          </div>
          
          {/* Key Metrics Section */}
          {contentToReview?.keyMetrics && contentToReview.keyMetrics.length > 0 && (
            <div className="lg:w-1/4">
              <div className="p-4 border rounded-lg bg-gray-50 h-full">
                <h2 className="text-xl font-semibold mb-3">{contentToReview.keyMetricsTitle || "Key Metrics"}</h2>
                <div className="flex flex-col gap-3">
                  {contentToReview.keyMetrics.map((metric, index) => (
                    <div key={index} className="p-2 bg-white rounded shadow-sm">
                      <span className="block text-xs text-gray-500">{metric.name}</span>
                      <span className="text-lg font-semibold">{metric.value}</span>
                    </div>
                  ))}
                </div>
              </div>
            </div>
          )}
        </div>
      </main>
    </div>
  );
}
|| END ||


|| START ./app/globals.css ||

@import "tailwindcss";

:root {
  --background: #ffffff;
  --foreground: #171717;
}

@theme inline {
  --color-background: var(--background);
  --color-foreground: var(--foreground);
  --font-sans: var(--font-geist-sans);
  --font-mono: var(--font-geist-mono);
}

@media (prefers-color-scheme: dark) {
  :root {
    --background: #0a0a0a;
    --foreground: #ededed;
  }
}

body {
  background: var(--background);
  color: var(--foreground);
  font-family: Arial, Helvetica, sans-serif;
}

|| END ||


|| START ./postcss.config.mjs ||

const config = {
  plugins: ["@tailwindcss/postcss"],
};

export default config;

|| END ||


|| START ./README.md ||

# Session Annotation Tool

A Next.js application for recording, annotating, and replaying video sessions with synchronized audio, video, drawing capabilities, and animation category tagging.

## Overview

This tool allows users to:
- Browse videos in an inbox with filtering and search capabilities
- Record synchronized audio while watching and interacting with videos
- Add visual annotations and drawings directly on the video
- Tag animations with specific categories (e.g., Artistic Style, Character Design)
- Capture all video player interactions (play, pause, seek, etc.)
- Replay entire sessions with perfect audio-video-annotation synchronization
- View selected animation categories during replay
- Save and load feedback sessions as JSON files
- Store video data in Azure Cosmos DB

## Installation & Setup

1. **Clone the repository**
   ```
   git clone https://github.com/shanedjones/cartoon-annotation-tool
   cd cartoon-annotation-tool
   ```

2. **Install dependencies**
   ```
   npm install
   ```

3. **Set up Azure Cosmos DB**
   - Create an Azure Cosmos DB account with SQL API
   - Create a database named `cartoon-db`
   - Create a container named `cartoons` with `/id` as the partition key
   - Copy your Cosmos DB endpoint and key from the Azure portal

4. **Set up Azure Blob Storage**
   - Create an Azure Storage account
   - Create a blob container named `audio-recordings`
   - Copy your storage connection string from the Azure portal

5. **Configure environment variables**
   - Create a `.env` file in the root directory:
   ```
   # Azure Cosmos DB Configuration
   COSMOS_ENDPOINT=https://your-cosmos-account.documents.azure.com:443/
   COSMOS_KEY=your-primary-key
   COSMOS_DATABASE_ID=cartoon-db
   COSMOS_CONTAINER_ID=cartoons
   
   # Azure Storage Configuration
   AZURE_STORAGE_CONNECTION_STRING=DefaultEndpointsProtocol=https;AccountName=your-account;AccountKey=your-key;EndpointSuffix=core.windows.net
   AZURE_STORAGE_CONTAINER_NAME=audio-recordings
   
   # Note: For Azure Storage, ensure that the storage account allows blob operations. 
   # Public access at container level is not required as the app will handle authentication.
   ```

5. **Seed the database**
   ```
   npm run seed-db
   ```

6. **Run the development server**
   ```
   npm run dev
   ```

7. **Build for production**
   ```
   npm run build
   npm start
   ```

## Key Components

### FeedbackOrchestrator
- Core orchestration component that coordinates all events
- Manages recording and replay synchronization
- Uses an audio-based timeline as the primary synchronization mechanism
- Handles all events based on relative time offsets (video, annotations, categories)
- Processes category events and provides them during replay
- Doesn't render any UI itself (headless component)

### VideoPlayerWrapper
- Container component that integrates the orchestrator with UI components
- Manages UI state for recording/replaying
- Handles file saving and loading, including category data
- Provides reset behavior for recording and replay
- Provides global access for category change recording
- Serves as the main entry point for the application

### Animation Categories
- Allows tagging of animation with predefined categories
- Categories can be selected/deselected during recording
- Categories are stored as both timeline events and session metadata
- During replay, all selected categories are shown in a list view

### VideoPlayer
- Customized video player with annotation capabilities
- Provides controls for play, pause, seek, volume, etc.
- Exposes imperative methods for controlling playback
- Forwards references to the annotation canvas

### AnnotationCanvas
- Canvas-based drawing component overlaid on the video
- Supports real-time drawing with color and width controls
- Handles both user-created and replayed annotations
- Carefully synchronizes with video timeline

### AudioRecorder
- Handles audio recording and playback
- Manages browser compatibility issues
- Provides error handling for permissions issues
- Serializes audio data for storage and replay

## Data Model

The application uses two main data structures:

1. **FeedbackSession**: Modern format used internally
   ```typescript
   interface FeedbackSession {
     id: string;
     videoId: string;
     startTime: number;
     endTime?: number;
     audioTrack: AudioTrack;
     events: TimelineEvent[];
     categories?: Record<string, number | null>; // Added to store category ratings (1-5 stars or null)
   }
   
   // Audio track containing all audio recording data
   interface AudioTrack {
     chunks: AudioChunk[];
     totalDuration: number;
   }
   
   // Timeline event - all synchronized to audio timeline
   interface TimelineEvent {
     id: string;
     type: 'video' | 'annotation' | 'marker' | 'category';
     timeOffset: number; // milliseconds from audio start
     duration?: number; // for events with duration
     payload: any; // specific data based on type
   }
   ```

2. **FeedbackData**: Legacy format maintained for backward compatibility
   ```typescript
   interface FeedbackData {
     sessionId: string;
     videoId: string;
     actions: RecordedAction[];
     startTime: number;
     endTime?: number;
     annotations?: DrawingPath[];
     audioChunks?: AudioChunk[];
   }
   ```

3. **Audio Data Structure**: Audio data is stored with metadata for playback
   ```typescript
   interface AudioChunk {
     blob: Blob | string;      // The audio data as Blob or string (for serialization)
     startTime: number;        // Relative to recording start
     duration: number;         // Length of audio chunk in ms
     videoTime: number;        // Video timestamp when this audio was recorded
     url?: string;             // URL for playback (created during replay)
     mimeType?: string;        // MIME type for proper playback
     blobUrl?: string;         // URL for the Azure Storage blob
   }
   ```

4. **Event Payloads**: Different event types have specific payload structures:
   ```typescript
   // Video event payload
   interface VideoEventPayload {
     action: 'play' | 'pause' | 'seek' | 'volume' | 'playbackRate';
     to?: number; // For seek, volume, and playbackRate events
   }
   
   // Annotation event payload
   interface AnnotationEventPayload {
     action: 'draw' | 'clear';
     path?: DrawingPath; // For draw events
   }
   
   // Category event payload
   interface CategoryEventPayload {
     category: string; // The category name (e.g., "artisticStyle")
     rating: number; // The star rating (1-5) or 0 to clear
   }
   
   // Marker event payload
   interface MarkerEventPayload {
     text: string; // The marker text
   }
   ```

## Key Features

### Recording Sessions
1. Audio is recorded using the MediaRecorder API
2. Video interactions (play, pause, seek) are captured as events
3. Drawing annotations are captured with timestamps
4. Animation category selections are recorded in real-time
5. All events are synchronized to a common timeline

### Replaying Sessions
1. Audio playback drives the main timeline
2. Video events are replayed at their recorded times
3. Annotations appear at their recorded timestamps
4. Animation categories selected during recording are displayed
5. All components reset properly when replay completes

### Animation Categories
1. Five key categories for animation analysis:
   - Artistic Style
   - Character Design
   - Motion Dynamics
   - Color Palette
   - Narrative Techniques
2. Each category can be rated with 1-5 stars during recording
3. Category ratings are visible during replay with star display
4. Category ratings are included in saved session data

### Serialization
- Sessions can be saved as JSON files
- Audio data is stored in Azure Blob Storage with URL references
- For backward compatibility, audio can also be serialized as base64 strings
- Files can be reloaded for later replay

## Project Structure

```
cartoon-annotation/
├── app/                  # Next.js app directory
│   ├── page.tsx          # Main application page
│   ├── layout.tsx        # App layout
│   ├── inbox/            # Video review inbox
│   │   └── page.tsx      # Inbox page component
│   └── api/              # Backend API routes
│       └── videos/       # Videos API
│           └── route.ts  # Cosmos DB CRUD operations
├── src/
│   ├── components/
│   │   ├── FeedbackOrchestrator.tsx   # Main coordination component
│   │   ├── VideoPlayerWrapper.tsx     # Container component
│   │   ├── VideoPlayer.tsx            # Custom video player
│   │   ├── AnnotationCanvas.tsx       # Drawing component
│   │   └── AudioRecorder.tsx          # Audio recording/playback
│   └── contexts/         # React contexts for state management
├── scripts/
│   └── seed-cosmos-db.js # Database seeding script
├── public/               # Static assets
└── package.json          # Dependencies and scripts
```

## Technical Details

- **Built with**: Next.js, React, TypeScript
- **Database**: Azure Cosmos DB (NoSQL)
- **Storage**: Azure Blob Storage for audio recordings
- **Backend API**: Next.js API routes
- **Audio**: Uses MediaRecorder API with format detection and Azure Storage integration
- **Drawing**: HTML5 Canvas for vector drawing
- **State Management**: React's Context and Refs for cross-component communication
- **Styling**: Tailwind CSS for responsive design

## Browser Compatibility

- Chrome (Desktop/Mobile): Full support
- Firefox (Desktop): Full support
- Safari (Desktop/iOS): Partial support - may require user interaction for audio playback
- Edge: Full support

## Potential Future Enhancements

- Visual timeline editor for post-recording edits
- Improved marker/comment system
- Video source selection
- Multiple annotation layers
- Custom animation categories
- Categorization analytics and reporting
- Category-based filtering during replay
- Export to video format
- Shared/collaborative sessions
- User authentication and role-based access
- Real-time collaboration features
- Advanced search and filtering for the inbox
- Integration with video streaming services
- Custom dashboard with analytics

## Known Issues

- Safari may require explicit user interaction before audio playback
- Large recordings with many annotations may experience performance issues
- Some mobile browsers have limited MediaRecorder support
|| END ||


|| START ./.gitignore ||

# See https://help.github.com/articles/ignoring-files/ for more about ignoring files.

# dependencies
/node_modules
/.pnp
.pnp.*
.yarn/*
!.yarn/patches
!.yarn/plugins
!.yarn/releases
!.yarn/versions

# testing
/coverage

# next.js
/.next/
/out/

# production
/build

# misc
.DS_Store
*.pem

# debug
npm-debug.log*
yarn-debug.log*
yarn-error.log*
.pnpm-debug.log*

# env files (can opt-in for committing if needed)
.env*

# vercel
.vercel

# typescript
*.tsbuildinfo
next-env.d.ts

cartoon-annotation-code-export.txt

|| END ||


|| START ./auth.md ||

# Authentication Recommendations

## Overview
For the cartoon annotation tool, a robust yet straightforward authentication system is recommended.

## Recommended Approach: NextAuth.js

NextAuth.js is ideal for this application because:
- Seamless integration with Next.js
- Support for multiple authentication providers
- Simple API with minimal configuration
- Built-in session management
- TypeScript support

## Implementation Steps

1. Install NextAuth.js:
```bash
npm install next-auth
```

2. Create an API route at `app/api/auth/[...nextauth]/route.ts`:
```typescript
import NextAuth from "next-auth";
import CredentialsProvider from "next-auth/providers/credentials";
import { compare } from "bcrypt";

// In a real app, replace this with your database logic
import { findUserByEmail } from "@/lib/auth"; 

export const authOptions = {
  providers: [
    CredentialsProvider({
      name: "Credentials",
      credentials: {
        email: { label: "Email", type: "email" },
        password: { label: "Password", type: "password" }
      },
      async authorize(credentials) {
        if (!credentials?.email || !credentials?.password) {
          return null;
        }
        
        const user = await findUserByEmail(credentials.email);
        if (!user) {
          return null;
        }
        
        const isPasswordValid = await compare(
          credentials.password,
          user.hashedPassword
        );
        
        if (!isPasswordValid) {
          return null;
        }
        
        return {
          id: user.id,
          email: user.email,
          name: user.name,
        };
      }
    }),
  ],
  callbacks: {
    async session({ session, token }) {
      if (token) {
        session.user.id = token.id;
      }
      return session;
    },
    async jwt({ token, user }) {
      if (user) {
        token.id = user.id;
      }
      return token;
    }
  },
  session: {
    strategy: "jwt",
  },
  pages: {
    signIn: "/auth/signin",
  },
};

const handler = NextAuth(authOptions);
export { handler as GET, handler as POST };
```

3. Create required environment variables in `.env.local`:
```
NEXTAUTH_URL=http://localhost:3000
NEXTAUTH_SECRET=your_random_secret_here
```

4. Create user authentication helpers in `src/lib/auth.ts`:
```typescript
import { hash } from "bcrypt";
import { CosmosClient } from "@azure/cosmos";

// Connect to your Cosmos DB
const client = new CosmosClient({
  endpoint: process.env.COSMOS_ENDPOINT,
  key: process.env.COSMOS_KEY,
});

const database = client.database("cartoon-db");
const container = database.container("users");

export async function findUserByEmail(email: string) {
  const querySpec = {
    query: "SELECT * FROM c WHERE c.email = @email",
    parameters: [
      {
        name: "@email",
        value: email
      }
    ]
  };
  
  const { resources } = await container.items.query(querySpec).fetchAll();
  return resources[0] || null;
}

export async function createUser(email: string, name: string, password: string) {
  const hashedPassword = await hash(password, 12);
  
  const newUser = {
    id: Date.now().toString(),
    email,
    name,
    hashedPassword,
    createdAt: new Date().toISOString()
  };
  
  const { resource } = await container.items.create(newUser);
  return resource;
}
```

5. Update the SessionContext to work with NextAuth:
```typescript
// src/contexts/SessionContext.tsx
import { createContext, useContext } from "react";
import { useSession, SessionProvider } from "next-auth/react";

export function AuthSessionProvider({ children }) {
  return <SessionProvider>{children}</SessionProvider>;
}

// Update AppProviders.tsx to include this provider
```

6. Create a sign-in page at `app/auth/signin/page.tsx`:
```typescript
'use client'
import { useState } from "react";
import { signIn } from "next-auth/react";
import { useRouter } from "next/navigation";

export default function SignIn() {
  const router = useRouter();
  const [email, setEmail] = useState("");
  const [password, setPassword] = useState("");
  const [error, setError] = useState("");
  
  const handleSubmit = async (e) => {
    e.preventDefault();
    setError("");
    
    try {
      const result = await signIn("credentials", {
        redirect: false,
        email,
        password,
      });
      
      if (result.error) {
        setError("Invalid email or password");
        return;
      }
      
      router.push("/");
    } catch (error) {
      setError("An error occurred. Please try again.");
    }
  };
  
  return (
    <div className="flex min-h-full flex-col justify-center py-12 sm:px-6 lg:px-8">
      <div className="sm:mx-auto sm:w-full sm:max-w-md">
        <h2 className="mt-6 text-center text-3xl font-bold tracking-tight text-gray-900">
          Sign in to your account
        </h2>
      </div>

      <div className="mt-8 sm:mx-auto sm:w-full sm:max-w-md">
        <div className="bg-white py-8 px-4 shadow sm:rounded-lg sm:px-10">
          <form className="space-y-6" onSubmit={handleSubmit}>
            {error && (
              <div className="text-red-500 text-sm">{error}</div>
            )}
            <div>
              <label htmlFor="email" className="block text-sm font-medium text-gray-700">
                Email address
              </label>
              <div className="mt-1">
                <input
                  id="email"
                  name="email"
                  type="email"
                  autoComplete="email"
                  required
                  value={email}
                  onChange={(e) => setEmail(e.target.value)}
                  className="block w-full appearance-none rounded-md border border-gray-300 px-3 py-2 shadow-sm focus:border-indigo-500 focus:outline-none focus:ring-indigo-500 sm:text-sm"
                />
              </div>
            </div>

            <div>
              <label htmlFor="password" className="block text-sm font-medium text-gray-700">
                Password
              </label>
              <div className="mt-1">
                <input
                  id="password"
                  name="password"
                  type="password"
                  autoComplete="current-password"
                  required
                  value={password}
                  onChange={(e) => setPassword(e.target.value)}
                  className="block w-full appearance-none rounded-md border border-gray-300 px-3 py-2 shadow-sm focus:border-indigo-500 focus:outline-none focus:ring-indigo-500 sm:text-sm"
                />
              </div>
            </div>

            <div>
              <button
                type="submit"
                className="flex w-full justify-center rounded-md border border-transparent bg-indigo-600 py-2 px-4 text-sm font-medium text-white shadow-sm hover:bg-indigo-700 focus:outline-none focus:ring-2 focus:ring-indigo-500 focus:ring-offset-2"
              >
                Sign in
              </button>
            </div>
          </form>
        </div>
      </div>
    </div>
  );
}
```

7. Add authentication checks to protected routes:
```typescript
// For client components
'use client'
import { useSession } from "next-auth/react";
import { redirect } from "next/navigation";

export default function ProtectedPage() {
  const { status } = useSession({
    required: true,
    onUnauthenticated() {
      redirect("/auth/signin");
    },
  });

  if (status === "loading") {
    return <div>Loading...</div>;
  }

  return <div>Protected content</div>;
}
```

## Database Integration
Extend the authentication system to store user-specific annotation data in the existing Cosmos DB backend:

1. Associate annotations with user IDs
2. Add permission checks for annotation editing
3. Implement user-specific views in the inbox

## Considerations
- Use dedicated middleware for route protection
- Implement role-based access control if needed
- Install additional dependencies: `npm install bcrypt @types/bcrypt`
- Consider adding user registration functionality
|| END ||


|| START ./scripts/seed-cosmos-db.js ||

// Seed script for Cosmos DB
// Usage: node scripts/seed-cosmos-db.js

require('dotenv').config();
const { CosmosClient } = require('@azure/cosmos');

// Cosmos DB connection configuration
const endpoint = process.env.COSMOS_ENDPOINT;
const key = process.env.COSMOS_KEY;
const databaseId = process.env.COSMOS_DATABASE_ID || '';
const containerId = process.env.COSMOS_CONTAINER_ID || '';

// Check for required environment variables
if (!endpoint || !key) {
  console.error('Error: COSMOS_ENDPOINT and COSMOS_KEY environment variables are required.');
  console.error('Please create a .env file with these values or set them in your environment.');
  process.exit(1);
}

// Initialize the Cosmos client
const client = new CosmosClient({ endpoint, key });

// Sample video data to seed
const videoData = [
  {
    id: "video-001",
    title: "Michael Johnson - Driver Full Swing",
    description: "Please review the stance throughout the swing and check if the club path is correct on the downswing.",
    thumbnailUrl: "https://img.freepik.com/free-photo/full-shot-man-playing-golf_23-2149354970.jpg",
    videoUrl: "https://cartoonannotationsta.blob.core.windows.net/videos/downTheLine.mp4",
    duration: "0:38",
    dateAdded: "2024-01-15",
    status: "Not Started",
    tags: [
      "DR",
      "HCP 5"
    ],
    dataLabelingTitle: "Swing Analysis",
    labelProperties: [
      { id: "setupAlignment", label: "Setup & Alignment" },
      { id: "takeawayPath", label: "Takeaway Path" },
      { id: "backswingPosition", label: "Backswing Position" },
      { id: "downswingTransition", label: "Downswing Transition" },
      { id: "impactPosition", label: "Impact Position" }
    ],
    keyMetricsTitle: "Driver Metrics",
    metrics: {
      "Club Head Speed": "112 mph",
      "Smash Factor": 1.48,
      "Launch Angle": "13.5°",
      "Spin Rate": "2340 rpm",
      "Carry Distance": "285 yards",
      "Clubface Angle": "0.2° open",
      "Attack Angle": "-1.2°",
      "Handicap": "+2.5"
    }
  },
  {
    id: "video-002",
    title: "Sarah Williams - 7 Iron Full Strength",
    description: "Can you review if the weight transfer is correct? Pay attention to the transition at the top of the backswing.",
    thumbnailUrl: "https://images.unsplash.com/photo-1535131749006-b7f58c99034b",
    videoUrl: "https://commondatastorage.googleapis.com/gtv-videos-bucket/sample/ForBiggerBlazes.mp4",
    duration: "0:45",
    dateAdded: "2024-02-10",
    status: "Not Started",
    tags: [
      "7I",
      "HCP 12"
    ],
    dataLabelingTitle: "Iron Technique",
    labelProperties: [
      { id: "ballPosition", label: "Ball Position" },
      { id: "wristHinge", label: "Wrist Hinge" },
      { id: "weightTransfer", label: "Weight Transfer" },
      { id: "followThrough", label: "Follow Through" },
      { id: "divotPattern", label: "Divot Pattern" }
    ],
    keyMetricsTitle: "Iron Shot Metrics",
    metrics: {
      "Club Head Speed": "87 mph",
      "Smash Factor": 1.38,
      "Dynamic Loft": "32.5°",
      "Shaft Lean": "7.2°",
      "Spin Rate": "6540 rpm",
      "Carry Distance": "165 yards",
      "Dispersion": "4.5 yards",
      "Handicap": "5"
    }
  },
  {
    id: "video-003",
    title: "Robert Chen - SW Bunker Shot",
    description: "Check if sand entry point is correct and provide feedback on the follow-through technique.",
    thumbnailUrl: "https://images.unsplash.com/photo-1592919505780-303950717480",
    videoUrl: "https://commondatastorage.googleapis.com/gtv-videos-bucket/sample/ForBiggerEscapes.mp4",
    duration: "0:52",
    dateAdded: "2024-03-05",
    status: "Not Started",
    tags: [
      "SW",
      "HCP 18"
    ],
    dataLabelingTitle: "Sand Technique",
    labelProperties: [
      { id: "stanceWidth", label: "Stance Width" },
      { id: "clubFacePosition", label: "Clubface Position" },
      { id: "sandEntry", label: "Sand Entry" },
      { id: "followThroughLength", label: "Follow Through Length" },
      { id: "bodyRotation", label: "Body Rotation" }
    ],
    keyMetricsTitle: "Bunker Shot Metrics",
    metrics: {
      "Club": "58° wedge",
      "Sand Type": "Fluffy",
      "Entry Point": "1.5 inches behind ball",
      "Swing Speed": "67 mph",
      "Shot Height": "High",
      "Spin Rate": "8650 rpm",
      "Carry Distance": "24 yards",
      "Roll Out": "4 feet"
    }
  },
  {
    id: "video-004",
    title: "Emma Davis - Putter 8-foot Break",
    description: "Please review the stroke path and face alignment at impact. Is the tempo consistent through the stroke?",
    thumbnailUrl: "https://images.unsplash.com/photo-1611374243147-44a702c2e414",
    videoUrl: "https://commondatastorage.googleapis.com/gtv-videos-bucket/sample/ForBiggerFun.mp4",
    duration: "0:41",
    dateAdded: "2024-01-28",
    status: "Not Started",
    tags: [
      "PT",
      "HCP 20"
    ],
    dataLabelingTitle: "Putting Elements",
    labelProperties: [
      { id: "gripPressure", label: "Grip Pressure" },
      { id: "strokePath", label: "Stroke Path" },
      { id: "faceAngle", label: "Face Angle" },
      { id: "tempo", label: "Tempo" },
      { id: "acceleration", label: "Acceleration" }
    ],
    keyMetricsTitle: "Putting Metrics",
    metrics: {
      "Stroke Type": "Slight Arc",
      "Tempo Ratio": "2:1 (back:through)",
      "Face Rotation": "0.3°",
      "Path Deviation": "0.2° inside",
      "Impact Position": "Center",
      "Ball Speed": "6.8 feet/second",
      "Distance Control": "97% accuracy",
      "Make Percentage": "88% from 10 feet"
    }
  },
  {
    id: "video-005",
    title: "James Rodriguez - PW Half Strength",
    description: "Can you check if hand position at impact is correct? Also review if loft presentation is appropriate for a half swing.",
    thumbnailUrl: "https://images.unsplash.com/photo-1576690234871-28f376265de8",
    videoUrl: "https://commondatastorage.googleapis.com/gtv-videos-bucket/sample/ForBiggerJoyrides.mp4",
    duration: "0:37",
    dateAdded: "2024-02-18",
    status: "Not Started",
    tags: [
      "PW",
      "HCP 7"
    ],
    dataLabelingTitle: "Pitch Shot Technique",
    labelProperties: [
      { id: "handPosition", label: "Hand Position" },
      { id: "wristAction", label: "Wrist Action" },
      { id: "lowPointControl", label: "Low Point Control" },
      { id: "faceControl", label: "Face Control" },
      { id: "loftPresentation", label: "Loft Presentation" }
    ],
    keyMetricsTitle: "Pitch Shot Metrics",
    metrics: {
      "Club": "56° wedge",
      "Carry Distance": "38 yards",
      "Spin Rate": "7340 rpm",
      "Launch Angle": "32°",
      "Landing Angle": "55°",
      "Apex Height": "18 feet",
      "Roll Out": "2.5 feet",
      "Shot Dispersion": "3.8 feet"
    }
  }
];

async function main() {
  try {
    // Create database if it doesn't exist
    console.log(`Checking for database: ${databaseId}...`);
    const { database } = await client.databases.createIfNotExists({ id: databaseId });
    console.log(`Database: ${database.id} ${database.id === databaseId ? 'exists' : 'created'}`);

    // Create container if it doesn't exist
    console.log(`Checking for container: ${containerId}...`);
    const { container } = await database.containers.createIfNotExists({ 
      id: containerId,
      partitionKey: { paths: ["/id"] } 
    });
    console.log(`Container: ${container.id} ${container.id === containerId ? 'exists' : 'created'}`);

    // Check if the container is already populated
    const { resources: existingItems } = await container.items.query({
      query: "SELECT VALUE COUNT(1) FROM c"
    }).fetchAll();

    const itemCount = existingItems[0];
    if (itemCount > 0) {
      console.log(`Container already contains ${itemCount} items. Do you want to clear and reseed? (yes/no)`);
      // In a real script, you would add user input here
      // For this example, we'll assume "yes" and continue
      console.log('Proceeding with clear and reseed...');
      
      // Delete existing items
      console.log('Deleting existing items...');
      const { resources: allItems } = await container.items.query({
        query: "SELECT c.id FROM c"
      }).fetchAll();
      
      for (const item of allItems) {
        await container.item(item.id, item.id).delete();
        console.log(`Deleted item: ${item.id}`);
      }
    }

    // Insert sample data
    console.log('Inserting sample video data...');
    for (const video of videoData) {
      const { resource: createdItem } = await container.items.create(video);
      console.log(`Created item: ${createdItem.id}`);
    }

    console.log('Seed data inserted successfully');
  } catch (error) {
    console.error('Error seeding Cosmos DB:', error);
  }
}

main()
  .then(() => console.log('Seeding completed successfully'))
  .catch(error => console.error('Seeding failed:', error));
|| END ||


|| START ./scripts/seed-users.js ||

// Load environment variables
require('dotenv').config({ path: '.env' });

const bcrypt = require('bcrypt');
const { CosmosClient } = require('@azure/cosmos');

// Cosmos DB connection
const client = new CosmosClient({
  endpoint: process.env.COSMOS_ENDPOINT,
  key: process.env.COSMOS_KEY,
});

// Validate environment variables
if (!process.env.COSMOS_DATABASE_ID) {
  throw new Error('COSMOS_DATABASE_ID environment variable is required');
}

if (!process.env.COSMOS_USERS_CONTAINER_ID) {
  throw new Error('COSMOS_USERS_CONTAINER_ID environment variable is required');
}

// Database and container names
const databaseId = process.env.COSMOS_DATABASE_ID;
const containerId = process.env.COSMOS_USERS_CONTAINER_ID;

console.log(`Using database: ${databaseId}`);
console.log(`Using container: ${containerId}`);

async function main() {
  try {
    console.log('Starting user database setup...');

    // Get or create database
    const { database } = await client.databases.createIfNotExists({ id: databaseId });
    console.log(`Database ${databaseId} ready`);

    // Get or create container
    const { container } = await database.containers.createIfNotExists({
      id: containerId,
      partitionKey: {
        paths: ['/id']
      }
    });
    console.log(`Container ${containerId} ready`);

    // Check if admin user already exists
    const querySpec = {
      query: "SELECT * FROM c WHERE c.email = @email",
      parameters: [
        {
          name: "@email",
          value: "admin@example.com"
        }
      ]
    };

    const { resources: existingUsers } = await container.items.query(querySpec).fetchAll();
    
    if (existingUsers.length > 0) {
      console.log('Admin user already exists.');
      return;
    }

    // Create admin user
    const hashedPassword = await bcrypt.hash('password123', 12);
    const adminUser = {
      id: Date.now().toString(),
      email: 'admin@example.com',
      name: 'Admin User',
      hashedPassword,
      createdAt: new Date().toISOString(),
      isAdmin: true
    };

    await container.items.create(adminUser);
    console.log('Admin user created successfully.');

    console.log('\nLogin credentials for testing:');
    console.log('Email: admin@example.com');
    console.log('Password: password123');
    console.log('\nIMPORTANT: Change these credentials in production!');

  } catch (error) {
    console.error('Error setting up user database:', error);
  }
}

main()
  .then(() => console.log('Database setup complete'))
  .catch(err => console.error('Database setup failed:', err));
|| END ||


|| START ./next.config.ts ||

import type { NextConfig } from "next";

const nextConfig: NextConfig = {
  /* config options here */
  env: {
    COSMOS_ENDPOINT: process.env.COSMOS_ENDPOINT,
    COSMOS_KEY: process.env.COSMOS_KEY,
    COSMOS_DATABASE_ID: process.env.COSMOS_DATABASE_ID,
    COSMOS_CONTAINER_ID: process.env.COSMOS_CONTAINER_ID,
    AZURE_STORAGE_CONNECTION_STRING: process.env.AZURE_STORAGE_CONNECTION_STRING,
    AZURE_STORAGE_CONTAINER_NAME: process.env.AZURE_STORAGE_CONTAINER_NAME,
  },
};

export default nextConfig;

|| END ||


|| START ./src/types/next-auth.d.ts ||

import "next-auth";

declare module "next-auth" {
  /**
   * Returned by `useSession`, `getSession` and received as a prop on the `SessionProvider` React Context
   */
  interface Session {
    user: {
      /** The user's unique identifier */
      id: string;
      /** The user's name */
      name?: string | null;
      /** The user's email address */
      email?: string | null;
      /** The user's profile image */
      image?: string | null;
    };
  }
}
|| END ||


|| START ./src/types/common.ts ||

/**
 * Common type definitions used across multiple components
 */

/**
 * Represents a 2D point with x,y coordinates
 */
export interface Point {
  x: number;
  y: number;
}

/**
 * Represents a unique identifier for various entities
 */
export type EntityId = string;

/**
 * Represents a timestamp in milliseconds since epoch
 */
export type Timestamp = number;

/**
 * Represents a duration in milliseconds
 */
export type Duration = number;

/**
 * Represents a time in the global timeline in milliseconds
 */
export type TimelinePosition = number;

/**
 * Represents a time in the video in milliseconds
 */
export type VideoPosition = number;

/**
 * Represents an RGB color in string format (e.g. "#ff0000")
 */
export type Color = string;

/**
 * Represents the drawing tool width in pixels
 */
export type StrokeWidth = number;

/**
 * Represents the aspect ratio calculation for the video and canvas
 */
export interface Dimensions {
  width: number;
  height: number;
}

/**
 * Basic key-value mapping with string keys
 */
export type Dictionary<T> = Record<string, T>;

/**
 * Categories and ratings mapping
 */
export type CategoryRatings = Dictionary<number | null>;
|| END ||


|| START ./src/types/ui.ts ||

/**
 * Type definitions for UI components and structures
 */
import { EntityId, Dictionary } from './common';

/**
 * Data labeling property for review
 */
export interface DataLabelingProperty {
  /** Unique identifier for the property */
  id: string;
  /** Display label for the property */
  label: string;
}

/**
 * Key metric for display
 */
export interface KeyMetric {
  /** Name of the metric */
  name: string;
  /** Value of the metric (string or number) */
  value: string | number;
}

/**
 * Content to be reviewed
 */
export interface ReviewContent {
  /** URL to the video being reviewed */
  videoUrl: string;
  /** Title of the video */
  videoTitle?: string;
  /** Description of the video */
  videoDescription?: string;
  /** Title for the data labeling section */
  dataLabelingTitle: string;
  /** Properties that can be labeled */
  labelProperties: DataLabelingProperty[];
  /** Title for the key metrics section */
  keyMetricsTitle?: string;
  /** Metrics to display */
  keyMetrics?: KeyMetric[];
}

/**
 * Properties for the VideoPlayerWrapper component
 */
export interface VideoPlayerWrapperProps {
  /** Category ratings */
  categories?: Dictionary<number | null>;
  /** Callback when categories are cleared */
  onCategoriesCleared?: () => void;
  /** Callback when categories are loaded */
  onCategoriesLoaded?: (categories: Dictionary<number>) => void;
  /** Callback when replay mode changes */
  onReplayModeChange?: (isReplay: boolean) => void;
  /** URL to the video */
  videoUrl?: string;
  /** Identifier for the video */
  videoId?: string;
  /** Content to be reviewed */
  contentToReview?: any;
}

/**
 * Properties for the star rating component
 */
export interface StarRatingProps {
  /** Category identifier */
  categoryId: string;
  /** Current rating value */
  value: number | null;
  /** Callback when rating changes */
  onChange: (categoryId: string, value: number) => void;
  /** Whether the rating is disabled */
  disabled?: boolean;
  /** Maximum rating value */
  maxRating?: number;
}

/**
 * Global window extensions
 */
export interface GlobalExtensions {
  /** VideoPlayerWrapper access */
  __videoPlayerWrapper?: {
    /** Method to record a category change */
    recordCategoryChange: (category: string, rating: number) => void;
    /** Whether recording is active */
    isRecording: boolean;
  };
  /** Global timeline position in milliseconds */
  __globalTimePosition?: number;
  /** Time when canvas was last cleared */
  __lastClearTime?: number;
  /** Whether a recorded session is available */
  __hasRecordedSession?: boolean;
}
|| END ||


|| START ./src/types/index.ts ||

/**
 * Central exports for all type definitions
 */

// Common types
export * from './common';

// Annotation types
export * from './annotation';

// Media types
export * from './media';

// Timeline types
export * from './timeline';

// UI types
export * from './ui';
|| END ||


|| START ./src/types/timeline.ts ||

/**
 * Type definitions for timeline and event recording/playback
 */
import { EntityId, TimelinePosition, VideoPosition, Timestamp, Duration, Dictionary } from './common';
import { DrawingPath } from './annotation';
import { AudioTrack, AudioChunk } from './media';

/**
 * Types of actions that can be recorded
 */
export type ActionType = 'play' | 'pause' | 'seek' | 'playbackRate' | 'keyboardShortcut' | 'annotation' | 'audio';

/**
 * Types of timeline events
 */
export type TimelineEventType = 'video' | 'annotation' | 'marker' | 'category';

/**
 * Base interface for all payload types
 */
export interface BasePayload {
  [key: string]: any;
}

/**
 * Video action payload
 */
export interface VideoPayload extends BasePayload {
  action: string;
  from?: number;
  to?: number;
  globalTimeOffset?: TimelinePosition;
}

/**
 * Annotation action payload
 */
export interface AnnotationPayload extends BasePayload {
  action: 'draw' | 'clear';
  path?: DrawingPath;
  clear?: boolean;
  globalTimeOffset?: TimelinePosition;
}

/**
 * Marker action payload
 */
export interface MarkerPayload extends BasePayload {
  text: string;
}

/**
 * Category action payload
 */
export interface CategoryPayload extends BasePayload {
  category: string;
  rating: number;
}

/**
 * Discriminated union of all possible payload types
 */
export type TimelinePayload = 
  | { type: 'video'; payload: VideoPayload }
  | { type: 'annotation'; payload: AnnotationPayload }
  | { type: 'marker'; payload: MarkerPayload }
  | { type: 'category'; payload: CategoryPayload };

/**
 * Timeline event - represents an action on the global timeline
 */
export interface TimelineEvent {
  /** Unique identifier for the event */
  id: EntityId;
  /** Type of the event */
  type: TimelineEventType;
  /** Time offset in milliseconds from the start of recording */
  timeOffset: TimelinePosition;
  /** Duration in milliseconds (for events with duration) */
  duration?: Duration;
  /** Event-specific data */
  payload: any; // This should ideally use the discriminated union
}

/**
 * Recorded action from the video player
 */
export interface RecordedAction {
  /** Type of action */
  type: ActionType;
  /** Time in milliseconds since recording started */
  timestamp: TimelinePosition;
  /** Current time in the video when action occurred */
  videoTime: VideoPosition;
  /** Additional details specific to the action */
  details?: {
    [key: string]: any;
  };
}

/**
 * Legacy feedback data structure
 */
export interface FeedbackData {
  /** Unique session identifier */
  sessionId: EntityId;
  /** Identifier for the video */
  videoId: EntityId;
  /** Array of recorded actions */
  actions: RecordedAction[];
  /** Timestamp when recording started */
  startTime: Timestamp;
  /** Timestamp when recording ended */
  endTime?: Timestamp;
  /** Annotations created during the session */
  annotations?: DrawingPath[];
  /** Audio recordings created during the session */
  audioChunks?: AudioChunk[];
}

/**
 * Modern feedback session structure
 */
export interface FeedbackSession {
  /** Unique session identifier */
  id: EntityId;
  /** Identifier for the video */
  videoId: EntityId;
  /** Timestamp when recording started */
  startTime: Timestamp;
  /** Timestamp when recording ended */
  endTime?: Timestamp;
  /** Audio track containing all audio data */
  audioTrack: AudioTrack;
  /** Timeline events */
  events: TimelineEvent[];
  /** Category ratings */
  categories?: Dictionary<boolean>;
}

/**
 * Properties for the feedback orchestrator
 */
export interface FeedbackOrchestratorProps {
  /** Reference to the video element */
  videoElementRef: React.RefObject<HTMLVideoElement>;
  /** Reference to the canvas element */
  canvasRef: React.RefObject<any>;
  /** Method to draw an annotation */
  drawAnnotation: (path: DrawingPath) => void;
  /** Method to clear annotations */
  clearAnnotations: () => void;
  /** Callback when audio is recorded */
  onAudioRecorded: (audioTrack: AudioTrack) => void;
  /** Callback when a session is completed */
  onSessionComplete: (session: FeedbackSession) => void;
  /** Initial session for replay */
  initialSession?: FeedbackSession | null;
  /** Current operation mode */
  mode: 'record' | 'replay';
  /** Callback when categories are loaded during replay */
  onCategoriesLoaded?: (categories: Dictionary<boolean>) => void;
}
|| END ||


|| START ./src/types/environment.d.ts ||

declare namespace NodeJS {
  interface ProcessEnv {
    NEXTAUTH_URL: string;
    NEXTAUTH_SECRET: string;
    COSMOS_ENDPOINT: string;
    COSMOS_KEY: string;
  }
}

declare module "next-auth" {
  interface Session {
    user: {
      id: string;
      name?: string | null;
      email?: string | null;
      image?: string | null;
    };
  }
}
|| END ||


|| START ./src/types/media.ts ||

/**
 * Type definitions for media (audio/video) handling
 */
import { Timestamp, VideoPosition, EntityId, TimelinePosition, Duration } from './common';

/**
 * Represents a chunk of recorded audio
 */
export interface AudioChunk {
  /** Audio data as Blob (in memory) or string (serialized) */
  blob: Blob | string;
  /** Start time relative to recording start */
  startTime: Timestamp;
  /** Length of audio chunk in ms */
  duration: Duration;
  /** Video position when this audio was recorded */
  videoTime: VideoPosition;
  /** URL for playback (created during replay) */
  url?: string;
  /** MIME type for proper playback */
  mimeType?: string;
}

/**
 * Audio track containing all audio recording data
 */
export interface AudioTrack {
  /** Array of audio chunks that make up the recording */
  chunks: AudioChunk[];
  /** Total duration of all audio in ms */
  totalDuration: Duration;
}

/**
 * Properties for the AudioRecorder component
 */
export interface AudioRecorderProps {
  /** Whether recording is active */
  isRecording: boolean;
  /** Whether replay is active */
  isReplaying: boolean;
  /** Current video playback position in seconds */
  currentVideoTime: number;
  /** Callback when an audio chunk is recorded */
  onAudioChunk?: (chunk: AudioChunk) => void;
  /** Audio chunks to replay */
  replayAudioChunks?: AudioChunk[];
}

/**
 * State for video playback
 */
export interface VideoState {
  /** Whether video is currently playing */
  isPlaying: boolean;
  /** Current playback position in seconds */
  currentTime: number;
  /** Total video duration in seconds */
  duration: number;
  /** Current playback speed (1.0 = normal) */
  playbackRate: number;
  /** Video dimensions */
  dimensions: {
    width: number;
    height: number;
  };
  /** Whether the video is muted */
  isMuted: boolean;
  /** Current volume level (0-1) */
  volume: number;
}

/**
 * Properties for the VideoPlayer component
 */
export interface VideoPlayerProps {
  /** Whether recording is active */
  isRecording?: boolean;
  /** Whether replay is active */
  isReplaying?: boolean;
  /** Callback for when a recordable action occurs */
  onRecordAction?: (action: RecordedAction) => void;
  /** Callback to set the video element reference */
  setVideoRef?: (ref: HTMLVideoElement | null) => void;
  /** Annotations to display during replay */
  replayAnnotations?: any[]; // Replace with proper type
  /** Callback when a new annotation is added */
  onAnnotationAdded?: (annotation: any) => void; // Replace with proper type
  /** URL of the video to play */
  videoUrl?: string;
}

/**
 * Methods exposed by the VideoPlayer via forwardRef
 */
export interface VideoPlayerRef {
  /** Reference to the video element */
  video: HTMLVideoElement | null;
  /** Reference to the annotation canvas */
  annotationCanvas: any; // Replace with proper type
  /** Method to add an annotation programmatically */
  handleManualAnnotation: (path: any) => void; // Replace with proper type
  /** Method to clear all annotations */
  clearAllAnnotations: () => void;
}
|| END ||


|| START ./src/types/annotation.ts ||

/**
 * Type definitions for annotations and drawing functionality
 */
import { Point, Color, StrokeWidth, TimelinePosition, VideoPosition, Timestamp, EntityId } from './common';

/**
 * Represents a drawing path for annotations
 */
export interface DrawingPath {
  /** Array of points that make up the drawing path */
  points: Point[];
  /** Color of the drawing stroke */
  color: Color;
  /** Width of the drawing stroke in pixels */
  width: StrokeWidth;
  /** Local timestamp when the annotation was created */
  timestamp: Timestamp;
  /** Time position in the video when this annotation was created (in ms) */
  videoTime?: VideoPosition;
  /** The global timeline position when this annotation was created (in ms) */
  globalTimeOffset?: TimelinePosition;
  /** Explicit timeOffset property added by FeedbackOrchestrator */
  timeOffset?: TimelinePosition;
  /** Optional ID for the drawing path */
  id?: EntityId;
}

/**
 * Properties required for the AnnotationCanvas component
 */
export interface AnnotationCanvasProps {
  /** Width of the canvas in pixels */
  width: number;
  /** Height of the canvas in pixels */
  height: number;
  /** Whether drawing is enabled */
  isEnabled: boolean;
  /** Current playback position of the video in seconds */
  currentTime: number;
  /** Whether recording is active */
  isRecording?: boolean;
  /** Whether replay is active */
  isReplaying?: boolean;
  /** Callback for when a new annotation is drawn */
  onAnnotationAdded?: (path: DrawingPath) => void;
  /** Annotations to display during replay */
  replayAnnotations?: DrawingPath[];
  /** Current drawing tool color */
  toolColor?: Color;
  /** Current drawing tool width */
  toolWidth?: StrokeWidth;
  /** Flag to clear the canvas */
  clearCanvas?: boolean;
  /** Callback when canvas clearing is complete */
  onClearComplete?: () => void;
}

/**
 * Represents the state of the annotation tool
 */
export interface AnnotationState {
  /** Whether drawing mode is active */
  isDrawingEnabled: boolean;
  /** Currently selected color */
  currentColor: Color;
  /** Currently selected stroke width */
  currentWidth: StrokeWidth;
  /** All drawing paths created in the current session */
  paths: DrawingPath[];
  /** Time of last canvas clear operation in global timeline */
  lastClearTime: TimelinePosition;
}

/**
 * Methods exposed by the AnnotationCanvas via forwardRef
 */
export interface AnnotationCanvasRef {
  /** Reference to the canvas element */
  canvasRef: React.RefObject<HTMLCanvasElement>;
  /** All drawing paths */
  allDrawings: DrawingPath[];
  /** Get the canvas context */
  getContext: () => CanvasRenderingContext2D | null;
  /** Clear all drawings from the canvas */
  clearCanvasDrawings: () => void;
  /** Programmatically add an annotation */
  handleManualAnnotation: (path: DrawingPath) => void;
}
|| END ||


|| START ./src/contexts/SessionContext.tsx ||

'use client';

import { createContext, useContext, useReducer, useMemo, useCallback, ReactNode, useEffect } from 'react';
import { FeedbackSession, TimelineEvent, AudioTrack, TimelinePosition, EntityId, CategoryRatings, Dictionary } from '../types';
import { v4 as uuidv4 } from 'uuid';

/**
 * Session State Interface
 */
interface SessionState {
  // Current session data
  session: FeedbackSession | null;
  // Whether a recording/replay session is active
  isActive: boolean;
  // Current mode
  mode: 'record' | 'replay' | 'idle';
  // Whether a recorded session is available
  hasRecordedSession: boolean;
  // Category ratings
  categories: CategoryRatings;
}

/**
 * Session Action Types
 */
type SessionActionType = 
  | { type: 'START_RECORDING'; payload: { videoId: string } }
  | { type: 'STOP_RECORDING'; payload: { endTime: number; audioTrack: AudioTrack; events: TimelineEvent[] } }
  | { type: 'START_REPLAY' }
  | { type: 'STOP_REPLAY' }
  | { type: 'LOAD_SESSION'; payload: { session: FeedbackSession } }
  | { type: 'SET_CATEGORY'; payload: { category: string; rating: number | null } }
  | { type: 'CLEAR_CATEGORIES' }
  | { type: 'SET_CATEGORIES'; payload: { categories: CategoryRatings } }
  | { type: 'RESET' };

/**
 * Session Context Interface
 */
interface SessionContextType {
  // Current state
  state: SessionState;
  // Action dispatchers
  startRecording: (videoId: string) => void;
  stopRecording: (endTime: number, audioTrack: AudioTrack, events: TimelineEvent[]) => void;
  startReplay: () => void;
  stopReplay: () => void;
  loadSession: (session: FeedbackSession) => void;
  setCategory: (category: string, rating: number | null) => void;
  clearCategories: () => void;
  setCategories: (categories: CategoryRatings) => void;
  reset: () => void;
  // Helper functions
  saveSessionToFile: () => Promise<void>;
  prepareSessionForExport: () => Promise<FeedbackSession>;
}

// Initial state
const initialState: SessionState = {
  session: null,
  isActive: false,
  mode: 'idle',
  hasRecordedSession: false,
  categories: {},
};

// Session reducer
function sessionReducer(state: SessionState, action: SessionActionType): SessionState {
  switch (action.type) {
    case 'START_RECORDING': {
      const startTime = Date.now();
      const sessionId = uuidv4();
      
      return {
        ...state,
        isActive: true,
        mode: 'record',
        session: {
          id: sessionId,
          videoId: action.payload.videoId,
          startTime,
          audioTrack: { chunks: [], totalDuration: 0 },
          events: [],
          categories: {}
        },
      };
    }

    case 'STOP_RECORDING': {
      if (!state.session) return state;
      
      const updatedSession: FeedbackSession = {
        ...state.session,
        endTime: action.payload.endTime,
        audioTrack: action.payload.audioTrack,
        events: action.payload.events,
        categories: convertCategoryRatingsToDict(state.categories),
      };
      
      return {
        ...state,
        isActive: false,
        mode: 'idle',
        session: updatedSession,
        hasRecordedSession: true,
      };
    }

    case 'START_REPLAY':
      return {
        ...state,
        isActive: true,
        mode: 'replay',
      };

    case 'STOP_REPLAY':
      return {
        ...state,
        isActive: false,
        mode: 'idle',
      };

    case 'LOAD_SESSION':
      return {
        ...state,
        session: action.payload.session,
        hasRecordedSession: true,
        categories: convertDictToCategoryRatings(action.payload.session.categories || {}),
      };

    case 'SET_CATEGORY': {
      const { category, rating } = action.payload;
      return {
        ...state,
        categories: {
          ...state.categories,
          [category]: rating,
        },
      };
    }

    case 'CLEAR_CATEGORIES':
      return {
        ...state,
        categories: {},
      };

    case 'SET_CATEGORIES':
      return {
        ...state,
        categories: action.payload.categories,
      };

    case 'RESET':
      return initialState;

    default:
      return state;
  }
}

// Helper function to convert category ratings to dictionary
function convertCategoryRatingsToDict(ratings: CategoryRatings): Dictionary<boolean> {
  const result: Dictionary<boolean> = {};
  
  Object.entries(ratings).forEach(([category, rating]) => {
    // Only include categories with non-null values
    if (rating !== null) {
      result[category] = true;
    }
  });
  
  return result;
}

// Helper function to convert dictionary to category ratings
function convertDictToCategoryRatings(dict: Dictionary<boolean>): CategoryRatings {
  const result: CategoryRatings = {};
  
  Object.entries(dict).forEach(([category, value]) => {
    // Convert boolean values to ratings
    if (value === true) {
      result[category] = 5; // Default to highest rating
    } else {
      result[category] = null;
    }
  });
  
  return result;
}

// Create the context
const SessionContext = createContext<SessionContextType | null>(null);

/**
 * Session Provider Props
 */
interface SessionProviderProps {
  children: ReactNode;
}

/**
 * Session Provider Component
 */
export function SessionProvider({ children }: SessionProviderProps) {
  const [state, dispatch] = useReducer(sessionReducer, initialState);

  // Memoized action creators
  const actions = useMemo(() => ({
    startRecording: (videoId: string) => 
      dispatch({ type: 'START_RECORDING', payload: { videoId } }),
    
    stopRecording: (endTime: number, audioTrack: AudioTrack, events: TimelineEvent[]) => 
      dispatch({ type: 'STOP_RECORDING', payload: { endTime, audioTrack, events } }),
    
    startReplay: () => dispatch({ type: 'START_REPLAY' }),
    
    stopReplay: () => dispatch({ type: 'STOP_REPLAY' }),
    
    loadSession: (session: FeedbackSession) => 
      dispatch({ type: 'LOAD_SESSION', payload: { session } }),
    
    setCategory: (category: string, rating: number | null) => 
      dispatch({ type: 'SET_CATEGORY', payload: { category, rating } }),
    
    clearCategories: () => dispatch({ type: 'CLEAR_CATEGORIES' }),
    
    setCategories: (categories: CategoryRatings) => 
      dispatch({ type: 'SET_CATEGORIES', payload: { categories } }),
    
    reset: () => dispatch({ type: 'RESET' }),
  }), []);

  // Helper function to save session as a JSON file
  const saveSessionToFile = useCallback(async () => {
    if (!state.session) {
      throw new Error('No session to save');
    }
    
    try {
      // In a real implementation, this would include serializing audio data
      const sessionData = await prepareSessionForExport();
      
      const dataStr = JSON.stringify(sessionData, null, 2);
      const dataUri = 'data:application/json;charset=utf-8,'+ encodeURIComponent(dataStr);
      
      const a = document.createElement('a');
      a.setAttribute('href', dataUri);
      a.setAttribute('download', `feedback-session-${state.session.id}.json`);
      document.body.appendChild(a);
      a.click();
      document.body.removeChild(a);
      
      return Promise.resolve();
    } catch (error) {
      console.error('Error saving session:', error);
      return Promise.reject(error);
    }
  }, [state.session]);

  // Helper function to prepare session for export
  const prepareSessionForExport = useCallback(async () => {
    if (!state.session) {
      throw new Error('No session to export');
    }
    
    // In a real implementation, this would include serializing audio data
    // For now, we just return a copy of the session
    return {
      ...state.session,
      categories: convertCategoryRatingsToDict(state.categories),
    };
  }, [state.session, state.categories]);

  // Update global window variables for backwards compatibility
  useEffect(() => {
    if (typeof window !== 'undefined') {
      window.__hasRecordedSession = state.hasRecordedSession;
      
      // Setup category change recording for compatibility
      window.__videoPlayerWrapper = {
        recordCategoryChange: (category, rating) => {
          actions.setCategory(category, rating);
        },
        isRecording: state.mode === 'record' && state.isActive,
      };
    }
  }, [state.hasRecordedSession, state.mode, state.isActive, actions]);

  // Provide the context value
  const contextValue = useMemo(() => ({
    state,
    ...actions,
    saveSessionToFile,
    prepareSessionForExport,
  }), [state, actions, saveSessionToFile, prepareSessionForExport]);

  return (
    <SessionContext.Provider value={contextValue}>
      {children}
    </SessionContext.Provider>
  );
}

/**
 * Custom hook to use the session context
 */
export function useSession() {
  const context = useContext(SessionContext);
  if (!context) {
    throw new Error('useSession must be used within a SessionProvider');
  }
  return context;
}

/**
 * Custom hook to manage categories
 */
export function useCategories() {
  const { state, setCategory, clearCategories, setCategories } = useSession();
  return {
    categories: state.categories,
    setCategory,
    clearCategories,
    setCategories,
  };
}

/**
 * Custom hook to check if a session is available
 */
export function useSessionAvailability() {
  const { state } = useSession();
  return {
    hasRecordedSession: state.hasRecordedSession,
    isActive: state.isActive,
    mode: state.mode,
  };
}
|| END ||


|| START ./src/contexts/TimelineContext.tsx ||

'use client';

import { createContext, useContext, useReducer, useMemo, useCallback, ReactNode, useEffect, useRef } from 'react';
import { TimelinePosition, TimelineEvent, FeedbackSession } from '../types';

/**
 * Timeline State Interface
 */
interface TimelineState {
  // Current position in the global timeline (in ms)
  currentPosition: TimelinePosition;
  // Total duration of the timeline (in ms)
  totalDuration: TimelinePosition;
  // Whether the timeline is currently active (recording or replaying)
  isActive: boolean;
  // Current mode of operation
  mode: 'record' | 'replay' | 'idle';
  // Events in the timeline
  events: TimelineEvent[];
  // Current session ID
  sessionId: string | null;
  // Start time of the current recording session
  recordingStartTime: number | null;
  // Progress percentage (0-100) for replay
  replayProgress: number;
  // Last time annotations were cleared (in ms)
  lastClearTime: TimelinePosition;
}

/**
 * Timeline Action Types
 */
type TimelineActionType = 
  | { type: 'START_RECORDING' }
  | { type: 'STOP_RECORDING' }
  | { type: 'START_REPLAY'; payload: { events: TimelineEvent[], totalDuration: TimelinePosition } }
  | { type: 'STOP_REPLAY' }
  | { type: 'UPDATE_POSITION'; payload: { position: TimelinePosition } }
  | { type: 'UPDATE_PROGRESS'; payload: { progress: number } }
  | { type: 'ADD_EVENT'; payload: { event: TimelineEvent } }
  | { type: 'LOAD_SESSION'; payload: { session: FeedbackSession } }
  | { type: 'UPDATE_CLEAR_TIME'; payload: { clearTime: TimelinePosition } }
  | { type: 'RESET_TIMELINE_POSITION' }
  | { type: 'RESET' };

/**
 * Timeline Context Interface
 */
interface TimelineContextType {
  // Current state
  state: TimelineState;
  // Action dispatchers
  startRecording: () => void;
  stopRecording: () => void;
  startReplay: (events: TimelineEvent[], totalDuration: TimelinePosition) => void;
  stopReplay: () => void;
  updatePosition: (position: TimelinePosition) => void;
  updateProgress: (progress: number) => void;
  addEvent: (event: TimelineEvent) => void;
  loadSession: (session: FeedbackSession) => void;
  updateClearTime: (clearTime: TimelinePosition) => void;
  resetTimelinePosition: () => void;
  reset: () => void;
}

// Initial state
const initialState: TimelineState = {
  currentPosition: 0,
  totalDuration: 0,
  isActive: false,
  mode: 'idle',
  events: [],
  sessionId: null,
  recordingStartTime: null,
  replayProgress: 0,
  lastClearTime: 0,
};

// Timeline reducer
function timelineReducer(state: TimelineState, action: TimelineActionType): TimelineState {
  switch (action.type) {
    case 'START_RECORDING':
      return {
        ...state,
        isActive: true,
        mode: 'record',
        currentPosition: 0,
        recordingStartTime: Date.now(),
        events: [],
        sessionId: `session-${Date.now()}`,
        replayProgress: 0,
        lastClearTime: 0,
      };

    case 'STOP_RECORDING':
      return {
        ...state,
        isActive: false,
        mode: 'idle',
        currentPosition: 0,
      };

    case 'START_REPLAY':
      return {
        ...state,
        isActive: true,
        mode: 'replay',
        currentPosition: 0,
        events: action.payload.events,
        totalDuration: action.payload.totalDuration,
        replayProgress: 0,
        lastClearTime: 0,
      };

    case 'STOP_REPLAY':
      return {
        ...state,
        isActive: false,
        mode: 'idle',
        currentPosition: 0,
        replayProgress: 0,
      };

    case 'UPDATE_POSITION':
      return {
        ...state,
        currentPosition: action.payload.position,
      };

    case 'UPDATE_PROGRESS':
      return {
        ...state,
        replayProgress: action.payload.progress,
      };

    case 'ADD_EVENT':
      return {
        ...state,
        events: [...state.events, action.payload.event],
      };

    case 'LOAD_SESSION':
      return {
        ...state,
        events: action.payload.session.events,
        sessionId: action.payload.session.id,
        totalDuration: action.payload.session.audioTrack.totalDuration,
      };
      
    case 'UPDATE_CLEAR_TIME':
      return {
        ...state,
        lastClearTime: action.payload.clearTime,
      };
      
    case 'RESET_TIMELINE_POSITION':
      return {
        ...state,
        currentPosition: 0,
        lastClearTime: 0,
      };

    case 'RESET':
      return initialState;

    default:
      return state;
  }
}

// Create the context
const TimelineContext = createContext<TimelineContextType | null>(null);

/**
 * Timeline Provider Props
 */
interface TimelineProviderProps {
  children: ReactNode;
}

/**
 * Timeline Provider Component
 */
export function TimelineProvider({ children }: TimelineProviderProps) {
  const [state, dispatch] = useReducer(timelineReducer, initialState);

  // Memoized action creators
  const actions = useMemo(() => ({
    startRecording: () => dispatch({ type: 'START_RECORDING' }),
    stopRecording: () => dispatch({ type: 'STOP_RECORDING' }),
    startReplay: (events: TimelineEvent[], totalDuration: TimelinePosition) => 
      dispatch({ type: 'START_REPLAY', payload: { events, totalDuration } }),
    stopReplay: () => dispatch({ type: 'STOP_REPLAY' }),
    updatePosition: (position: TimelinePosition) => 
      dispatch({ type: 'UPDATE_POSITION', payload: { position } }),
    updateProgress: (progress: number) => 
      dispatch({ type: 'UPDATE_PROGRESS', payload: { progress } }),
    addEvent: (event: TimelineEvent) => 
      dispatch({ type: 'ADD_EVENT', payload: { event } }),
    loadSession: (session: FeedbackSession) => 
      dispatch({ type: 'LOAD_SESSION', payload: { session } }),
    updateClearTime: (clearTime: TimelinePosition) => 
      dispatch({ type: 'UPDATE_CLEAR_TIME', payload: { clearTime } }),
    resetTimelinePosition: () => 
      dispatch({ type: 'RESET_TIMELINE_POSITION' }),
    reset: () => dispatch({ type: 'RESET' }),
  }), []);

  // Synchronize with window globals when needed for backward compatibility
  useEffect(() => {
    if (typeof window !== 'undefined') {
      window.__globalTimePosition = state.currentPosition;
    }
  }, [state.currentPosition]);
  
  // Synchronize lastClearTime with window global
  useEffect(() => {
    if (typeof window !== 'undefined') {
      window.__lastClearTime = state.lastClearTime;
      
      // Add development-only logging
      if (process.env.NODE_ENV === 'development') {
        console.log(`Syncing lastClearTime to window.__lastClearTime: ${state.lastClearTime}ms`);
      }
    }
  }, [state.lastClearTime]);

  // Provide the context value
  const contextValue = useMemo(() => ({
    state,
    ...actions,
  }), [state, actions]);

  return (
    <TimelineContext.Provider value={contextValue}>
      {children}
    </TimelineContext.Provider>
  );
}

/**
 * Custom hook to use the timeline context
 */
export function useTimeline() {
  const context = useContext(TimelineContext);
  if (!context) {
    throw new Error('useTimeline must be used within a TimelineProvider');
  }
  return context;
}

/**
 * Custom hook to get the current timeline position
 */
export function useTimelinePosition() {
  const { state } = useTimeline();
  return state.currentPosition;
}

/**
 * Custom hook to calculate the current time relative to recording start
 */
export function useRecordingTime() {
  const { state } = useTimeline();
  
  if (!state.recordingStartTime || state.mode !== 'record') {
    return 0;
  }
  
  return Date.now() - state.recordingStartTime;
}

/**
 * Custom hook to track events
 */
export function useTimelineEvents() {
  const { state, addEvent } = useTimeline();
  return {
    events: state.events,
    addEvent,
  };
}

/**
 * Custom hook to access and update lastClearTime
 */
export function useLastClearTime() {
  const { state, updateClearTime } = useTimeline();
  return {
    lastClearTime: state.lastClearTime,
    updateClearTime,
  };
}
|| END ||


|| START ./src/contexts/AuthContext.tsx ||

'use client';

import { createContext, useContext, ReactNode } from 'react';
import { SessionProvider, useSession, signIn, signOut } from 'next-auth/react';
import { useRouter } from 'next/navigation';

/**
 * Auth Session Provider Component
 */
export function AuthSessionProvider({ children }: { children: ReactNode }) {
  return <SessionProvider>{children}</SessionProvider>;
}

/**
 * Auth Context Interface
 */
interface AuthContextType {
  user: any;
  status: "loading" | "authenticated" | "unauthenticated";
  signin: (email: string, password: string) => Promise<{ success: boolean; error?: string }>;
  signout: () => Promise<void>;
}

// Create the context
const AuthContext = createContext<AuthContextType | null>(null);

/**
 * Auth Provider Component
 */
export function AuthProvider({ children }: { children: ReactNode }) {
  const { data: session, status } = useSession();
  const router = useRouter();

  // Sign in function
  const signin = async (email: string, password: string) => {
    try {
      const result = await signIn('credentials', {
        redirect: false,
        email,
        password,
      });

      if (result?.error) {
        return { success: false, error: 'Invalid email or password' };
      }

      return { success: true };
    } catch (error) {
      return { success: false, error: 'An error occurred. Please try again.' };
    }
  };

  // Sign out function
  const signout = async () => {
    await signOut({ redirect: false });
    router.push('/auth/signin');
  };

  // Context value
  const contextValue = {
    user: session?.user,
    status,
    signin,
    signout,
  };

  return (
    <AuthContext.Provider value={contextValue}>
      {children}
    </AuthContext.Provider>
  );
}

/**
 * Custom hook to use the auth context
 */
export function useAuth() {
  const context = useContext(AuthContext);
  if (!context) {
    throw new Error('useAuth must be used within an AuthProvider');
  }
  return context;
}

/**
 * Custom hook to check if a user is authenticated and redirect if not
 */
export function useRequireAuth() {
  const { status } = useSession();
  const router = useRouter();

  if (status === 'unauthenticated') {
    router.push('/auth/signin');
    return { status: 'redirecting' };
  }

  return { status };
}

|| END ||


|| START ./src/contexts/AppProviders.tsx ||

'use client';

import { ReactNode } from 'react';
import { TimelineProvider } from './TimelineContext';
import { AnnotationProvider } from './AnnotationContext';
import { VideoProvider } from './VideoContext';
import { SessionProvider } from './SessionContext';
import { AuthSessionProvider, AuthProvider } from './AuthContext';

interface AppProvidersProps {
  children: ReactNode;
  initialVideoUrl?: string;
}

/**
 * Combines all context providers in the correct nesting order
 */
export function AppProviders({ children, initialVideoUrl = '' }: AppProvidersProps) {
  return (
    <AuthSessionProvider>
      <AuthProvider>
        <SessionProvider>
          <TimelineProvider>
            <AnnotationProvider>
              <VideoProvider initialUrl={initialVideoUrl}>
                {children}
              </VideoProvider>
            </AnnotationProvider>
          </TimelineProvider>
        </SessionProvider>
      </AuthProvider>
    </AuthSessionProvider>
  );
}

/**
 * Export individual providers and hooks for direct imports
 */
export * from './TimelineContext';
export * from './AnnotationContext';
export * from './VideoContext';
export * from './SessionContext';
export * from './AuthContext';
|| END ||


|| START ./src/contexts/AnnotationContext.tsx ||

'use client';

import { createContext, useContext, useReducer, useMemo, useCallback, ReactNode, useEffect } from 'react';
import { DrawingPath, TimelinePosition, Color, StrokeWidth } from '../types';

/**
 * Annotation State Interface
 */
interface AnnotationState {
  // All annotations created in the current session
  annotations: DrawingPath[];
  // Time of the last clear operation in global timeline
  lastClearTime: TimelinePosition;
  // Currently selected color
  currentColor: Color;
  // Currently selected stroke width
  currentWidth: StrokeWidth;
  // Whether annotation is enabled
  isEnabled: boolean;
  // Whether annotation is in replay mode
  isReplaying: boolean;
}

/**
 * Annotation Action Types
 */
type AnnotationActionType = 
  | { type: 'ADD_ANNOTATION'; payload: { annotation: DrawingPath } }
  | { type: 'CLEAR_ANNOTATIONS'; payload: { clearTime: TimelinePosition } }
  | { type: 'SET_ANNOTATIONS'; payload: { annotations: DrawingPath[] } }
  | { type: 'SET_COLOR'; payload: { color: Color } }
  | { type: 'SET_WIDTH'; payload: { width: StrokeWidth } }
  | { type: 'SET_ENABLED'; payload: { enabled: boolean } }
  | { type: 'SET_REPLAYING'; payload: { replaying: boolean } }
  | { type: 'RESET' };

/**
 * Annotation Context Interface
 */
interface AnnotationContextType {
  // Current state
  state: AnnotationState;
  // Action dispatchers
  addAnnotation: (annotation: DrawingPath) => void;
  clearAnnotations: (clearTime: TimelinePosition) => void;
  setAnnotations: (annotations: DrawingPath[]) => void;
  setColor: (color: Color) => void;
  setWidth: (width: StrokeWidth) => void;
  setEnabled: (enabled: boolean) => void;
  setReplaying: (replaying: boolean) => void;
  reset: () => void;
  // Helper functions
  isAnnotationVisibleAtTime: (annotation: DrawingPath, timePosition: TimelinePosition) => boolean;
  getVisibleAnnotations: (timePosition: TimelinePosition) => DrawingPath[];
}

// Initial state
const initialState: AnnotationState = {
  annotations: [],
  lastClearTime: 0,
  currentColor: '#ff0000', // Default red
  currentWidth: 3, // Default medium width
  isEnabled: true,
  isReplaying: false,
};

// Annotation reducer
function annotationReducer(state: AnnotationState, action: AnnotationActionType): AnnotationState {
  switch (action.type) {
    case 'ADD_ANNOTATION':
      return {
        ...state,
        annotations: [...state.annotations, action.payload.annotation],
      };

    case 'CLEAR_ANNOTATIONS':
      return {
        ...state,
        lastClearTime: action.payload.clearTime,
      };

    case 'SET_ANNOTATIONS':
      return {
        ...state,
        annotations: action.payload.annotations,
      };

    case 'SET_COLOR':
      return {
        ...state,
        currentColor: action.payload.color,
      };

    case 'SET_WIDTH':
      return {
        ...state,
        currentWidth: action.payload.width,
      };

    case 'SET_ENABLED':
      return {
        ...state,
        isEnabled: action.payload.enabled,
      };

    case 'SET_REPLAYING':
      return {
        ...state,
        isReplaying: action.payload.replaying,
      };

    case 'RESET':
      return initialState;

    default:
      return state;
  }
}

// Create the context
const AnnotationContext = createContext<AnnotationContextType | null>(null);

/**
 * Annotation Provider Props
 */
interface AnnotationProviderProps {
  children: ReactNode;
}

/**
 * Annotation Provider Component
 */
export function AnnotationProvider({ children }: AnnotationProviderProps) {
  const [state, dispatch] = useReducer(annotationReducer, initialState);

  // Memoized action creators
  const actions = useMemo(() => ({
    addAnnotation: (annotation: DrawingPath) => 
      dispatch({ type: 'ADD_ANNOTATION', payload: { annotation } }),
    
    clearAnnotations: (clearTime: TimelinePosition) => 
      dispatch({ type: 'CLEAR_ANNOTATIONS', payload: { clearTime } }),
    
    setAnnotations: (annotations: DrawingPath[]) => 
      dispatch({ type: 'SET_ANNOTATIONS', payload: { annotations } }),
    
    setColor: (color: Color) => 
      dispatch({ type: 'SET_COLOR', payload: { color } }),
    
    setWidth: (width: StrokeWidth) => 
      dispatch({ type: 'SET_WIDTH', payload: { width } }),
    
    setEnabled: (enabled: boolean) => 
      dispatch({ type: 'SET_ENABLED', payload: { enabled } }),
    
    setReplaying: (replaying: boolean) => 
      dispatch({ type: 'SET_REPLAYING', payload: { replaying } }),
    
    reset: () => dispatch({ type: 'RESET' }),
  }), []);

  // Helper function to determine if an annotation should be visible at a given time
  const isAnnotationVisibleAtTime = useCallback(
    (annotation: DrawingPath, timePosition: TimelinePosition): boolean => {
      // First priority: check globalTimeOffset
      if ('globalTimeOffset' in annotation) {
        const globalTimeOffset = annotation.globalTimeOffset as TimelinePosition;
        
        // Skip annotations that were drawn before the last clear
        if (globalTimeOffset <= state.lastClearTime) {
          return false;
        }
        
        // Show annotation if it was created before or at the current time
        return globalTimeOffset <= timePosition;
      }
      
      // Second priority: check timeOffset
      if ('timeOffset' in annotation) {
        const timeOffset = annotation.timeOffset as TimelinePosition;
        
        // Skip annotations drawn before the last clear
        if (timeOffset <= state.lastClearTime) {
          return false;
        }
        
        return timeOffset <= timePosition;
      }
      
      // Fallback to videoTime for legacy support
      if (annotation.videoTime !== undefined) {
        return true; // Simplified for now, would need video position context for full implementation
      }
      
      // Last resort: use timestamp
      return annotation.timestamp <= timePosition;
    },
    [state.lastClearTime]
  );

  // Get all annotations that should be visible at a given time
  const getVisibleAnnotations = useCallback(
    (timePosition: TimelinePosition): DrawingPath[] => {
      return state.annotations.filter(annotation => 
        isAnnotationVisibleAtTime(annotation, timePosition)
      );
    },
    [state.annotations, isAnnotationVisibleAtTime]
  );

  // Synchronize with window globals when needed for backward compatibility
  useEffect(() => {
    if (typeof window !== 'undefined') {
      window.__lastClearTime = state.lastClearTime;
    }
  }, [state.lastClearTime]);

  // Provide the context value
  const contextValue = useMemo(() => ({
    state,
    ...actions,
    isAnnotationVisibleAtTime,
    getVisibleAnnotations,
  }), [state, actions, isAnnotationVisibleAtTime, getVisibleAnnotations]);

  return (
    <AnnotationContext.Provider value={contextValue}>
      {children}
    </AnnotationContext.Provider>
  );
}

/**
 * Custom hook to use the annotation context
 */
export function useAnnotation() {
  const context = useContext(AnnotationContext);
  if (!context) {
    throw new Error('useAnnotation must be used within an AnnotationProvider');
  }
  return context;
}

/**
 * Custom hook to get current drawing options
 */
export function useDrawingOptions() {
  const { state, setColor, setWidth } = useAnnotation();
  return {
    color: state.currentColor,
    width: state.currentWidth,
    setColor,
    setWidth,
  };
}

/**
 * Custom hook to manage annotations
 */
export function useAnnotationManager() {
  const { 
    state, 
    addAnnotation, 
    clearAnnotations, 
    setAnnotations, 
    getVisibleAnnotations 
  } = useAnnotation();
  
  return {
    annotations: state.annotations,
    lastClearTime: state.lastClearTime,
    isEnabled: state.isEnabled,
    isReplaying: state.isReplaying,
    addAnnotation,
    clearAnnotations,
    setAnnotations,
    getVisibleAnnotations,
  };
}
|| END ||


|| START ./src/contexts/index.ts ||

/**
 * Central exports for all contexts
 */
export * from './AppProviders';
export * from './TimelineContext';
export * from './AnnotationContext';
export * from './VideoContext';
export * from './SessionContext';
|| END ||


|| START ./src/contexts/VideoContext.tsx ||

'use client';

import { createContext, useContext, useReducer, useMemo, useCallback, ReactNode, useRef, MutableRefObject, useEffect, useState } from 'react';
import { VideoState, Dimensions } from '../types';
import { cacheVideo, getFromCache, clearVideoCache, getVideoCacheStats } from '../utils/videoCache';

/**
 * Video Context State Interface
 */
interface VideoContextState {
  // Video playback state
  playback: VideoState;
  // Original Video URL
  videoUrl: string;
  // Cached Blob URL (if available)
  cachedUrl: string | null;
  // Loading state
  isLoading: boolean;
  // Reference to the video element
  videoRef: MutableRefObject<HTMLVideoElement | null>;
  // Whether the video is ready to play
  isReady: boolean;
  // Cache statistics
  cacheStats: {
    count: number;
    totalSize: number;
  };
}

/**
 * Video Action Types
 */
type VideoActionType = 
  | { type: 'PLAY' }
  | { type: 'PAUSE' }
  | { type: 'SEEK'; payload: { time: number } }
  | { type: 'UPDATE_TIME'; payload: { time: number } }
  | { type: 'UPDATE_DURATION'; payload: { duration: number } }
  | { type: 'SET_PLAYBACK_RATE'; payload: { rate: number } }
  | { type: 'SET_DIMENSIONS'; payload: { dimensions: Dimensions } }
  | { type: 'SET_MUTED'; payload: { muted: boolean } }
  | { type: 'SET_VOLUME'; payload: { volume: number } }
  | { type: 'SET_VIDEO_URL'; payload: { url: string } }
  | { type: 'SET_CACHED_URL'; payload: { url: string | null } }
  | { type: 'SET_LOADING'; payload: { isLoading: boolean } }
  | { type: 'SET_READY'; payload: { ready: boolean } }
  | { type: 'SET_CACHE_STATS'; payload: { stats: { count: number; totalSize: number } } }
  | { type: 'RESET' };

/**
 * Video Context Interface
 */
interface VideoContextType {
  // Current state
  state: VideoContextState;
  // Action dispatchers
  play: () => void;
  pause: () => void;
  seek: (time: number) => void;
  updateTime: (time: number) => void;
  updateDuration: (duration: number) => void;
  setPlaybackRate: (rate: number) => void;
  setDimensions: (dimensions: Dimensions) => void;
  setMuted: (muted: boolean) => void;
  setVolume: (volume: number) => void;
  setVideoUrl: (url: string) => void;
  setReady: (ready: boolean) => void;
  resetCache: () => Promise<void>;
  reset: () => void;
  // Helper methods
  getFormattedTime: (time: number) => string;
  togglePlay: () => void;
  getFormattedCacheSize: () => string;
}

// Format time helper function (mm:ss)
function formatTime(time: number): string {
  if (!time || isNaN(time) || time < 0) {
    return '0:00';
  }
  const minutes = Math.floor(time / 60);
  const seconds = Math.floor(time % 60);
  return `${minutes}:${seconds < 10 ? '0' : ''}${seconds}`;
}

// Format file size helper (bytes to KB, MB, GB)
function formatFileSize(bytes: number): string {
  if (bytes === 0) return '0 Bytes';
  
  const sizes = ['Bytes', 'KB', 'MB', 'GB'];
  const i = Math.floor(Math.log(bytes) / Math.log(1024));
  
  return parseFloat((bytes / Math.pow(1024, i)).toFixed(2)) + ' ' + sizes[i];
}

// Initial video state
const initialVideoState: VideoState = {
  isPlaying: false,
  currentTime: 0,
  duration: 0,
  playbackRate: 1,
  dimensions: {
    width: 0,
    height: 0,
  },
  isMuted: false,
  volume: 1,
};

// Initial context state
const initialState: VideoContextState = {
  playback: initialVideoState,
  videoUrl: '',
  cachedUrl: null,
  isLoading: false,
  videoRef: { current: null },
  isReady: false,
  cacheStats: {
    count: 0,
    totalSize: 0,
  },
};

// Video reducer
function videoReducer(state: VideoContextState, action: VideoActionType): VideoContextState {
  switch (action.type) {
    case 'PLAY':
      return {
        ...state,
        playback: {
          ...state.playback,
          isPlaying: true,
        },
      };

    case 'PAUSE':
      return {
        ...state,
        playback: {
          ...state.playback,
          isPlaying: false,
        },
      };

    case 'SEEK':
      return {
        ...state,
        playback: {
          ...state.playback,
          currentTime: action.payload.time,
        },
      };

    case 'UPDATE_TIME':
      return {
        ...state,
        playback: {
          ...state.playback,
          currentTime: action.payload.time,
        },
      };

    case 'UPDATE_DURATION':
      return {
        ...state,
        playback: {
          ...state.playback,
          duration: action.payload.duration,
        },
      };

    case 'SET_PLAYBACK_RATE':
      return {
        ...state,
        playback: {
          ...state.playback,
          playbackRate: action.payload.rate,
        },
      };

    case 'SET_DIMENSIONS':
      return {
        ...state,
        playback: {
          ...state.playback,
          dimensions: action.payload.dimensions,
        },
      };

    case 'SET_MUTED':
      return {
        ...state,
        playback: {
          ...state.playback,
          isMuted: action.payload.muted,
        },
      };

    case 'SET_VOLUME':
      return {
        ...state,
        playback: {
          ...state.playback,
          volume: action.payload.volume,
        },
      };

    case 'SET_VIDEO_URL':
      return {
        ...state,
        videoUrl: action.payload.url,
        // Reset cached URL when setting a new video URL
        cachedUrl: null,
        isReady: false,
      };
      
    case 'SET_CACHED_URL':
      return {
        ...state,
        cachedUrl: action.payload.url,
      };
      
    case 'SET_LOADING':
      return {
        ...state,
        isLoading: action.payload.isLoading,
      };

    case 'SET_READY':
      return {
        ...state,
        isReady: action.payload.ready,
      };
      
    case 'SET_CACHE_STATS':
      return {
        ...state,
        cacheStats: action.payload.stats,
      };

    case 'RESET':
      return {
        ...initialState,
        videoRef: state.videoRef, // Keep the reference
        cacheStats: state.cacheStats, // Keep cache stats
      };

    default:
      return state;
  }
}

// Create the context
const VideoContext = createContext<VideoContextType | null>(null);

/**
 * Video Provider Props
 */
interface VideoProviderProps {
  children: ReactNode;
  initialUrl?: string;
}

/**
 * Video Provider Component
 */
export function VideoProvider({ children, initialUrl = '' }: VideoProviderProps) {
  const videoRef = useRef<HTMLVideoElement | null>(null);
  
  const initialContextState = {
    ...initialState,
    videoUrl: initialUrl,
    videoRef,
  };
  
  const [state, dispatch] = useReducer(videoReducer, initialContextState);

  // We'll use this ref to track if we're currently loading a specific URL
  const loadingUrlRef = useRef<string | null>(null);
  
  // Attempt to load the video from cache when the URL changes
  useEffect(() => {
    // Ignore empty URLs or server-side rendering
    if (!state.videoUrl || typeof window === 'undefined') return;
    
    // If we're already loading this specific URL, don't start another load
    if (loadingUrlRef.current === state.videoUrl) {
      console.log('Already loading this URL, skipping duplicate request:', state.videoUrl);
      return;
    }
    
    // If we're already loading and it's a different URL, cancel the old load and start a new one
    if (state.isLoading) {
      console.log('Switching to load new URL:', state.videoUrl);
    }
    
    // Start loading this URL
    const loadVideoFromCacheOrNetwork = async () => {
      try {
        // Mark we're loading this specific URL
        loadingUrlRef.current = state.videoUrl;
        dispatch({ type: 'SET_LOADING', payload: { isLoading: true } });
        console.log('Starting to load video:', state.videoUrl);
        
        // Check if this is a local file URL or blob URL (which we shouldn't try to cache)
        const isLocalOrBlobUrl = 
          state.videoUrl.startsWith('blob:') || 
          state.videoUrl.startsWith('data:') ||
          state.videoUrl.startsWith('file:');
          
        if (isLocalOrBlobUrl) {
          console.log('Using local URL directly, skipping cache:', state.videoUrl);
          dispatch({ type: 'SET_CACHED_URL', payload: { url: state.videoUrl } });
          dispatch({ type: 'SET_LOADING', payload: { isLoading: false } });
          return;
        }
        
        // First try to get from cache
        const cachedVideo = await getFromCache(state.videoUrl);
        
        // Make sure we're still loading the same URL (user might have changed it while we were loading)
        if (loadingUrlRef.current !== state.videoUrl) {
          console.log('URL changed during cache lookup, aborting load of:', state.videoUrl);
          return;
        }
        
        if (cachedVideo) {
          console.log('Found video in cache:', state.videoUrl);
          dispatch({ type: 'SET_CACHED_URL', payload: { url: cachedVideo.blobUrl } });
          dispatch({ type: 'SET_LOADING', payload: { isLoading: false } });
          return;
        }
        
        // Check if it's a cross-origin URL
        let isCrossOrigin = false;
        try {
          if (typeof window !== 'undefined') {
            const videoUrlObj = new URL(state.videoUrl, window.location.href);
            isCrossOrigin = videoUrlObj.origin !== window.location.origin;
          }
        } catch (e) {
          console.warn('Error parsing URL, assuming same-origin:', e);
        }
        
        if (isCrossOrigin) {
          console.log('Cross-origin video, may have CORS issues:', state.videoUrl);
        }
        
        // Not in cache, need to download and cache
        console.log('Video not in cache, downloading:', state.videoUrl);
        const blobUrl = await cacheVideo(state.videoUrl);
        
        // Make sure we're still loading the same URL
        if (loadingUrlRef.current !== state.videoUrl) {
          console.log('URL changed during fetch, aborting load of:', state.videoUrl);
          return;
        }
        
        // Update cached URL
        dispatch({ type: 'SET_CACHED_URL', payload: { url: blobUrl } });
        
        // Update cache stats after caching
        const stats = await getVideoCacheStats();
        dispatch({ type: 'SET_CACHE_STATS', payload: { stats } });
      } catch (error) {
        console.error('Error loading video:', error);
        // Fallback to using original URL
        dispatch({ type: 'SET_CACHED_URL', payload: { url: state.videoUrl } });
      } finally {
        // Only clear loading state if we're still loading the same URL
        if (loadingUrlRef.current === state.videoUrl) {
          loadingUrlRef.current = null;
          dispatch({ type: 'SET_LOADING', payload: { isLoading: false } });
        }
      }
    };
    
    loadVideoFromCacheOrNetwork();
  }, [state.videoUrl]);
  
  // Update cache stats periodically
  useEffect(() => {
    if (typeof window === 'undefined') return;
    
    const updateStats = async () => {
      const stats = await getVideoCacheStats();
      dispatch({ type: 'SET_CACHE_STATS', payload: { stats } });
    };
    
    // Update stats when component mounts
    updateStats();
    
    // Update stats every 5 minutes
    const interval = setInterval(updateStats, 5 * 60 * 1000);
    
    return () => {
      clearInterval(interval);
    };
  }, []);

  // Memoized action creators
  const actions = useMemo(() => ({
    play: () => {
      if (state.videoRef.current) {
        state.videoRef.current.play();
      }
      dispatch({ type: 'PLAY' });
    },
    
    pause: () => {
      if (state.videoRef.current) {
        state.videoRef.current.pause();
      }
      dispatch({ type: 'PAUSE' });
    },
    
    seek: (time: number) => {
      if (state.videoRef.current) {
        state.videoRef.current.currentTime = time;
      }
      dispatch({ type: 'SEEK', payload: { time } });
    },
    
    updateTime: (time: number) => 
      dispatch({ type: 'UPDATE_TIME', payload: { time } }),
    
    updateDuration: (duration: number) => 
      dispatch({ type: 'UPDATE_DURATION', payload: { duration } }),
    
    setPlaybackRate: (rate: number) => {
      if (state.videoRef.current) {
        state.videoRef.current.playbackRate = rate;
      }
      dispatch({ type: 'SET_PLAYBACK_RATE', payload: { rate } });
    },
    
    setDimensions: (dimensions: Dimensions) => 
      dispatch({ type: 'SET_DIMENSIONS', payload: { dimensions } }),
    
    setMuted: (muted: boolean) => {
      if (state.videoRef.current) {
        state.videoRef.current.muted = muted;
      }
      dispatch({ type: 'SET_MUTED', payload: { muted } });
    },
    
    setVolume: (volume: number) => {
      if (state.videoRef.current) {
        state.videoRef.current.volume = volume;
      }
      dispatch({ type: 'SET_VOLUME', payload: { volume } });
    },
    
    setVideoUrl: (url: string) => {
      if (url === state.videoUrl) return;
      dispatch({ type: 'SET_VIDEO_URL', payload: { url } });
    },
    
    setReady: (ready: boolean) => 
      dispatch({ type: 'SET_READY', payload: { ready } }),
    
    resetCache: async () => {
      try {
        await clearVideoCache();
        const stats = await getVideoCacheStats();
        dispatch({ type: 'SET_CACHE_STATS', payload: { stats } });
      } catch (error) {
        console.error('Error clearing video cache:', error);
      }
    },
    
    reset: () => dispatch({ type: 'RESET' }),
  }), [state.videoRef, state.videoUrl]);

  // Helper to format time (mm:ss)
  const getFormattedTime = useCallback((time: number) => formatTime(time), []);
  
  // Helper to format cache size
  const getFormattedCacheSize = useCallback(() => {
    return formatFileSize(state.cacheStats.totalSize);
  }, [state.cacheStats.totalSize]);

  // Toggle play/pause
  const togglePlay = useCallback(() => {
    if (state.playback.isPlaying) {
      actions.pause();
    } else {
      actions.play();
    }
  }, [state.playback.isPlaying, actions]);

  // Provide the context value
  const contextValue = useMemo(() => ({
    state,
    ...actions,
    getFormattedTime,
    getFormattedCacheSize,
    togglePlay,
  }), [state, actions, getFormattedTime, getFormattedCacheSize, togglePlay]);

  return (
    <VideoContext.Provider value={contextValue}>
      {children}
    </VideoContext.Provider>
  );
}

/**
 * Custom hook to use the video context
 */
export function useVideo() {
  const context = useContext(VideoContext);
  if (!context) {
    throw new Error('useVideo must be used within a VideoProvider');
  }
  return context;
}

/**
 * Custom hook to get video playback controls
 */
export function useVideoControls() {
  const { state, play, pause, seek, setPlaybackRate, togglePlay } = useVideo();
  return {
    isPlaying: state.playback.isPlaying,
    currentTime: state.playback.currentTime,
    duration: state.playback.duration,
    playbackRate: state.playback.playbackRate,
    play,
    pause,
    seek,
    setPlaybackRate,
    togglePlay,
  };
}

/**
 * Custom hook to get video dimensions
 */
export function useVideoDimensions() {
  const { state, setDimensions } = useVideo();
  return {
    dimensions: state.playback.dimensions,
    setDimensions,
  };
}

/**
 * Custom hook to get video source URL (either cached or original)
 */
export function useVideoSource() {
  const { state } = useVideo();
  return {
    videoUrl: state.videoUrl,
    cachedUrl: state.cachedUrl,
    isLoading: state.isLoading,
    effectiveUrl: state.cachedUrl || state.videoUrl,
    cacheStats: state.cacheStats,
  };
}
|| END ||


|| START ./src/utils/videoCache.ts ||

'use client';

// Constants for the IndexedDB database
const DB_NAME = 'VideoCache';
const DB_VERSION = 1;
const VIDEO_STORE = 'videos';

// Interface for cached video metadata
interface VideoCacheItem {
  url: string;
  blobUrl: string;
  size: number;
  lastAccessed: number;
  mimeType: string;
}

// Type definition for cache entries to be stored in IndexedDB
type VideoCacheEntry = {
  url: string;
  blob: Blob;
  blobUrl?: string;
  lastAccessed: number;
};

// Max cache size in bytes (100MB)
const MAX_CACHE_SIZE = 100 * 1024 * 1024;

/**
 * Initialize the IndexedDB database for video caching
 */
function initializeDB(): Promise<IDBDatabase> {
  return new Promise((resolve, reject) => {
    if (!('indexedDB' in window)) {
      reject(new Error('IndexedDB is not supported in this browser'));
      return;
    }

    const request = indexedDB.open(DB_NAME, DB_VERSION);

    request.onerror = (event) => {
      console.error('Video Cache DB error:', event);
      reject(new Error('Failed to open video cache database'));
    };

    request.onupgradeneeded = (event) => {
      const db = (event.target as IDBOpenDBRequest).result;
      
      // Create the videos object store with URL as the key path
      if (!db.objectStoreNames.contains(VIDEO_STORE)) {
        const store = db.createObjectStore(VIDEO_STORE, { keyPath: 'url' });
        store.createIndex('lastAccessed', 'lastAccessed', { unique: false });
      }
    };

    request.onsuccess = (event) => {
      const db = (event.target as IDBOpenDBRequest).result;
      resolve(db);
    };
  });
}

/**
 * Cache a video by URL, storing it in IndexedDB
 * @param url The URL of the video to cache
 * @returns Promise that resolves to a Blob URL for the cached video
 */
export async function cacheVideo(url: string): Promise<string> {
  try {
    // Check if already in cache
    const existingItem = await getFromCache(url);
    if (existingItem) {
      console.log('Video already in cache:', url);
      // Update last accessed time
      await updateLastAccessed(url);
      return existingItem.blobUrl;
    }
    
    // Check if it's likely a cross-origin URL before attempting fetch
    let isCrossOrigin = false;
    try {
      if (typeof window !== 'undefined') {
        const videoUrlObj = new URL(url, window.location.href);
        isCrossOrigin = videoUrlObj.origin !== window.location.origin;
      }
    } catch (e) {
      console.warn('Error parsing URL, assuming same-origin:', e);
    }
    
    // Skip fetch for cross-origin URLs to avoid CORS errors
    if (isCrossOrigin) {
      console.log('Cross-origin video detected, skipping fetch:', url);
      
      // Create a cache entry with the original URL to prevent future fetch attempts
      await storeInCache({
        url,
        blob: new Blob([], { type: 'video/mp4' }), // Empty placeholder blob
        blobUrl: url, // Use original URL
        lastAccessed: Date.now(),
      });
      
      return url; // Return original URL directly
    }
    
    // For same-origin URLs, attempt to fetch
    console.log('Fetching same-origin video to cache:', url);
    try {
      // Try direct fetch
      const response = await fetch(url, { 
        mode: 'cors',
        credentials: 'same-origin'
      });
      
      if (!response.ok) {
        throw new Error(`Failed to fetch video: ${response.status}`);
      }
      
      const blob = await response.blob();
      const blobUrl = URL.createObjectURL(blob);
      
      // Check if we need to make room in the cache
      await ensureCacheSpace(blob.size);
      
      // Store in IndexedDB
      await storeInCache({
        url,
        blob,
        blobUrl,
        lastAccessed: Date.now(),
      });
      
      console.log('Video cached successfully:', url);
      return blobUrl;
    } catch (fetchError) {
      console.error('Error with direct fetch, using original URL:', fetchError);
      
      // Create a minimal cache entry with the original URL
      // This way we don't try to fetch it again in the future
      await storeInCache({
        url,
        blob: new Blob([], { type: 'video/mp4' }), // Empty placeholder blob
        blobUrl: url, // Use original URL
        lastAccessed: Date.now(),
      });
      
      return url; // Return original URL as fallback
    }
  } catch (error) {
    console.error('Error caching video:', error);
    // Return the original URL as fallback
    return url;
  }
}

/**
 * Get a video from the cache by URL
 * @param url The URL of the video to retrieve
 * @returns Promise that resolves to the cache item or null if not found
 */
export async function getFromCache(url: string): Promise<VideoCacheItem | null> {
  try {
    const db = await initializeDB();
    
    return new Promise((resolve, reject) => {
      const transaction = db.transaction([VIDEO_STORE], 'readonly');
      const store = transaction.objectStore(VIDEO_STORE);
      const request = store.get(url);
      
      request.onerror = () => {
        reject(new Error('Failed to retrieve video from cache'));
      };
      
      request.onsuccess = () => {
        const result = request.result as VideoCacheEntry | undefined;
        
        if (result) {
          resolve({
            url: result.url,
            blobUrl: result.blobUrl || URL.createObjectURL(result.blob),
            size: result.blob.size,
            lastAccessed: result.lastAccessed,
            mimeType: result.blob.type,
          });
        } else {
          resolve(null);
        }
      };
    });
  } catch (error) {
    console.error('Error getting video from cache:', error);
    return null;
  }
}

/**
 * Store a video in the cache
 * @param entry The cache entry to store
 */
async function storeInCache(entry: VideoCacheEntry): Promise<void> {
  try {
    const db = await initializeDB();
    
    return new Promise((resolve, reject) => {
      const transaction = db.transaction([VIDEO_STORE], 'readwrite');
      const store = transaction.objectStore(VIDEO_STORE);
      const request = store.put(entry);
      
      request.onerror = () => {
        reject(new Error('Failed to store video in cache'));
      };
      
      request.onsuccess = () => {
        resolve();
      };
    });
  } catch (error) {
    console.error('Error storing video in cache:', error);
    throw error;
  }
}

/**
 * Update the last accessed time for a cached video
 * @param url The URL of the video to update
 */
async function updateLastAccessed(url: string): Promise<void> {
  try {
    const db = await initializeDB();
    
    return new Promise((resolve, reject) => {
      const transaction = db.transaction([VIDEO_STORE], 'readwrite');
      const store = transaction.objectStore(VIDEO_STORE);
      const request = store.get(url);
      
      request.onerror = () => {
        reject(new Error('Failed to retrieve video for updating last accessed time'));
      };
      
      request.onsuccess = () => {
        const result = request.result as VideoCacheEntry | undefined;
        
        if (result) {
          result.lastAccessed = Date.now();
          const updateRequest = store.put(result);
          
          updateRequest.onerror = () => {
            reject(new Error('Failed to update last accessed time'));
          };
          
          updateRequest.onsuccess = () => {
            resolve();
          };
        } else {
          resolve(); // Item not found, nothing to update
        }
      };
    });
  } catch (error) {
    console.error('Error updating last accessed time:', error);
  }
}

/**
 * Get the total size of all cached videos
 * @returns Promise that resolves to the total size in bytes
 */
async function getCacheTotalSize(): Promise<number> {
  try {
    const db = await initializeDB();
    
    return new Promise((resolve, reject) => {
      const transaction = db.transaction([VIDEO_STORE], 'readonly');
      const store = transaction.objectStore(VIDEO_STORE);
      const request = store.getAll();
      
      request.onerror = () => {
        reject(new Error('Failed to retrieve cache contents'));
      };
      
      request.onsuccess = () => {
        const entries = request.result as VideoCacheEntry[];
        const totalSize = entries.reduce((total, entry) => total + (entry.blob?.size || 0), 0);
        resolve(totalSize);
      };
    });
  } catch (error) {
    console.error('Error calculating cache size:', error);
    return 0;
  }
}

/**
 * Ensure there's enough space in the cache for a new video
 * @param newItemSize The size of the new video in bytes
 */
async function ensureCacheSpace(newItemSize: number): Promise<void> {
  try {
    // Get current cache size
    const currentSize = await getCacheTotalSize();
    
    // Check if adding the new item would exceed the max cache size
    if (currentSize + newItemSize > MAX_CACHE_SIZE) {
      console.log('Cache cleanup needed. Current size:', currentSize, 'New item size:', newItemSize);
      await removeOldestItems(currentSize + newItemSize - MAX_CACHE_SIZE);
    }
  } catch (error) {
    console.error('Error ensuring cache space:', error);
  }
}

/**
 * Remove the oldest items from the cache until enough space is freed
 * @param bytesToFree The number of bytes to free
 */
async function removeOldestItems(bytesToFree: number): Promise<void> {
  try {
    const db = await initializeDB();
    
    return new Promise((resolve, reject) => {
      const transaction = db.transaction([VIDEO_STORE], 'readwrite');
      const store = transaction.objectStore(VIDEO_STORE);
      const index = store.index('lastAccessed');
      const request = index.openCursor();
      
      let freedBytes = 0;
      
      request.onerror = () => {
        reject(new Error('Failed to open cursor for removing old items'));
      };
      
      request.onsuccess = (event) => {
        const cursor = (event.target as IDBRequest<IDBCursorWithValue>).result;
        
        if (cursor && freedBytes < bytesToFree) {
          const entry = cursor.value as VideoCacheEntry;
          freedBytes += entry.blob?.size || 0;
          
          // Revoke the blob URL to prevent memory leaks
          if (entry.blobUrl && entry.blobUrl !== entry.url) {
            // Only revoke if it's a blob URL, not the original URL
            URL.revokeObjectURL(entry.blobUrl);
          }
          
          const deleteRequest = cursor.delete();
          deleteRequest.onerror = () => {
            console.error('Failed to delete old cache item:', entry.url);
          };
          
          cursor.continue();
        } else {
          console.log(`Freed ${freedBytes} bytes from cache`);
          resolve();
        }
      };
    });
  } catch (error) {
    console.error('Error removing oldest items from cache:', error);
  }
}

/**
 * Clear all videos from the cache
 */
export async function clearVideoCache(): Promise<void> {
  try {
    const db = await initializeDB();
    
    return new Promise((resolve, reject) => {
      const transaction = db.transaction([VIDEO_STORE], 'readwrite');
      const store = transaction.objectStore(VIDEO_STORE);
      
      // First get all entries to revoke blob URLs
      const getAllRequest = store.getAll();
      
      getAllRequest.onerror = () => {
        reject(new Error('Failed to retrieve cache entries for clearing'));
      };
      
      getAllRequest.onsuccess = () => {
        const entries = getAllRequest.result as VideoCacheEntry[];
        
        // Revoke all blob URLs to prevent memory leaks
        entries.forEach(entry => {
          if (entry.blobUrl && entry.blobUrl !== entry.url) {
            // Only revoke if it's a blob URL, not the original URL
            URL.revokeObjectURL(entry.blobUrl);
          }
        });
        
        // Clear the store
        const clearRequest = store.clear();
        
        clearRequest.onerror = () => {
          reject(new Error('Failed to clear video cache'));
        };
        
        clearRequest.onsuccess = () => {
          console.log('Video cache cleared successfully');
          resolve();
        };
      };
    });
  } catch (error) {
    console.error('Error clearing video cache:', error);
  }
}

/**
 * Get cache statistics
 * @returns Promise that resolves to cache statistics
 */
export async function getVideoCacheStats(): Promise<{ count: number; totalSize: number }> {
  try {
    const db = await initializeDB();
    
    return new Promise((resolve, reject) => {
      const transaction = db.transaction([VIDEO_STORE], 'readonly');
      const store = transaction.objectStore(VIDEO_STORE);
      const countRequest = store.count();
      
      countRequest.onerror = () => {
        reject(new Error('Failed to count cache entries'));
      };
      
      countRequest.onsuccess = () => {
        const count = countRequest.result;
        
        // Get all entries to calculate total size
        const getAllRequest = store.getAll();
        
        getAllRequest.onerror = () => {
          reject(new Error('Failed to retrieve cache entries for stats'));
        };
        
        getAllRequest.onsuccess = () => {
          const entries = getAllRequest.result as VideoCacheEntry[];
          const totalSize = entries.reduce((total, entry) => total + (entry.blob?.size || 0), 0);
          
          resolve({
            count,
            totalSize,
          });
        };
      };
    });
  } catch (error) {
    console.error('Error getting cache stats:', error);
    return { count: 0, totalSize: 0 };
  }
}
|| END ||


|| START ./src/utils/audioStorage.ts ||

/**
 * Client-side utilities for managing audio storage
 */

/**
 * Upload an audio blob to Azure Storage
 * @param audioBlob The audio blob to upload
 * @param sessionId The session ID to associate with the audio
 * @returns The URL of the uploaded blob
 */
export const uploadAudioToStorage = async (audioBlob: Blob, sessionId: string): Promise<string> => {
  try {
    if (!audioBlob || !(audioBlob instanceof Blob) || audioBlob.size === 0) {
      throw new Error('Invalid audio blob provided for upload');
    }
    
    console.log(`Preparing to upload audio blob: size=${audioBlob.size}, type=${audioBlob.type}`);
    
    const formData = new FormData();
    formData.append('audio', new File([audioBlob], 'audio.webm', { type: audioBlob.type || 'audio/webm' }));
    formData.append('sessionId', sessionId);
    
    console.log('Sending audio blob to API endpoint');
    
    const response = await fetch('/api/audio', {
      method: 'POST',
      body: formData,
    });
    
    if (!response.ok) {
      let errorMessage = response.statusText;
      try {
        const errorData = await response.json();
        errorMessage = errorData.error || errorMessage;
      } catch (e) {
        // Ignore JSON parsing errors and use statusText
      }
      throw new Error(`Failed to upload audio: ${errorMessage} (status: ${response.status})`);
    }
    
    const data = await response.json();
    
    if (!data.url) {
      throw new Error('No URL returned from audio upload API');
    }
    
    console.log('Successfully uploaded audio blob, received URL:', data.url);
    return data.url;
  } catch (error) {
    console.error('Error uploading audio to storage:', error);
    throw error;
  }
};

/**
 * Download an audio blob from a URL
 * @param url The URL of the audio blob to download
 * @returns The downloaded audio blob
 */
export const downloadAudioFromUrl = async (url: string): Promise<Blob> => {
  try {
    const response = await fetch(url);
    
    if (!response.ok) {
      throw new Error(`Failed to download audio: ${response.statusText}`);
    }
    
    return await response.blob();
  } catch (error) {
    console.error('Error downloading audio from URL:', error);
    throw error;
  }
};
|| END ||


|| START ./src/utils/azureStorage.ts ||

import { BlobServiceClient, ContainerClient } from '@azure/storage-blob';
import { v4 as uuidv4 } from 'uuid';

// Azure Storage account configuration
const connectionString = process.env.AZURE_STORAGE_CONNECTION_STRING || '';
const containerName = process.env.AZURE_STORAGE_CONTAINER_NAME || 'audio-recordings';

// Initialize the BlobServiceClient
let blobServiceClient: BlobServiceClient;
let containerClient: ContainerClient;

// Initialize the storage client
const initializeStorageClient = () => {
  if (!connectionString) {
    throw new Error('Azure Storage connection string is not configured');
  }
  
  if (!blobServiceClient) {
    blobServiceClient = BlobServiceClient.fromConnectionString(connectionString);
    containerClient = blobServiceClient.getContainerClient(containerName);
  }
  
  return { blobServiceClient, containerClient };
};

// Create the container if it doesn't exist
export const ensureContainer = async (): Promise<void> => {
  try {
    const { containerClient } = initializeStorageClient();
    // Create container without specifying public access level
    await containerClient.createIfNotExists();
    console.log(`Container "${containerName}" is ready`);
  } catch (error) {
    console.error('Error ensuring container exists:', error);
    throw error;
  }
};

// Upload a blob to storage
export const uploadAudioBlob = async (
  audioBlob: Blob,
  sessionId: string
): Promise<string> => {
  try {
    if (!audioBlob || !(audioBlob instanceof Blob)) {
      throw new Error('Invalid audio blob provided');
    }
    
    const { containerClient } = initializeStorageClient();
    
    // Generate a unique filename
    const blobName = `${sessionId}/${uuidv4()}.webm`;
    
    // Get a block blob client
    const blockBlobClient = containerClient.getBlockBlobClient(blobName);
    
    // Convert Blob to Buffer or ArrayBuffer for uploadData
    const arrayBuffer = await audioBlob.arrayBuffer();
    
    if (!arrayBuffer) {
      throw new Error('Failed to convert blob to array buffer');
    }
    
    // Upload the blob
    const uploadOptions = {
      blobHTTPHeaders: {
        blobContentType: audioBlob.type || 'audio/webm',
      }
    };
    
    await blockBlobClient.uploadData(arrayBuffer, uploadOptions);
    console.log(`Audio blob uploaded: ${blobName}`);
    
    // Create a Shared Access Signature (SAS) URL with read permissions that expires in 24 hours
    const expiresOn = new Date();
    expiresOn.setDate(expiresOn.getDate() + 1); // 1 day from now
    
    const sasUrl = blockBlobClient.url;
    console.log(`Generated SAS URL for blob: ${sasUrl}`);
    
    // Return the URL for the uploaded blob
    return sasUrl;
  } catch (error) {
    console.error('Error uploading audio blob:', error);
    throw error;
  }
};

// Download a blob from storage
export const downloadAudioBlob = async (blobUrl: string): Promise<Blob> => {
  try {
    // Extract the blob name from the URL
    const url = new URL(blobUrl);
    const pathParts = url.pathname.split('/');
    const blobName = pathParts.slice(2).join('/'); // Skip the first two segments (/container/blob)
    
    const { containerClient } = initializeStorageClient();
    const blockBlobClient = containerClient.getBlockBlobClient(blobName);
    
    // Download the blob
    const downloadResponse = await blockBlobClient.download();
    
    // Convert the stream to a blob
    const data = await streamToBlob(downloadResponse.readableStreamBody);
    return data;
  } catch (error) {
    console.error('Error downloading audio blob:', error);
    throw error;
  }
};

// Helper function to convert a stream to a blob
async function streamToBlob(stream: NodeJS.ReadableStream | null): Promise<Blob> {
  if (!stream) {
    throw new Error('No stream provided');
  }
  
  // Create a wrapper to work in both browser and Node.js environments
  const reader = stream.getReader ? stream : new ReadableStream({
    start(controller) {
      stream.on('data', (chunk) => {
        controller.enqueue(chunk);
      });
      stream.on('end', () => {
        controller.close();
      });
      stream.on('error', (err) => {
        controller.error(err);
      });
    }
  }).getReader();
  
  // Read all chunks
  const chunks: Uint8Array[] = [];
  
  // @ts-ignore - Property 'read' does not exist on type 'ReadableStreamReader<any>'
  async function readChunks() {
    let data;
    while (!(data = await reader.read()).done) {
      chunks.push(new Uint8Array(data.value));
    }
  }
  
  await readChunks();
  
  // Concatenate chunks into a single Uint8Array
  let totalLength = 0;
  for (const chunk of chunks) {
    totalLength += chunk.length;
  }
  
  const result = new Uint8Array(totalLength);
  let offset = 0;
  for (const chunk of chunks) {
    result.set(chunk, offset);
    offset += chunk.length;
  }
  
  // Create a Blob from the Uint8Array
  return new Blob([result]);
}
|| END ||


|| START ./src/components/Navbar.tsx ||

'use client';

import { usePathname } from 'next/navigation';
import Link from 'next/link';
import { useAuth } from '../contexts/AuthContext';

export function Navbar() {
  const pathname = usePathname();
  const { user, status, signout } = useAuth();
  
  // Check if the path is active
  const isActive = (path: string) => pathname === path;

  // Skip rendering on auth pages
  if (pathname.startsWith('/auth/')) {
    return null;
  }

  return (
    <nav className="bg-gray-800 text-white shadow-md">
      <div className="max-w-7xl mx-auto px-4">
        <div className="flex justify-between h-16">
          <div className="flex items-center">
            <Link href="/" className="flex-shrink-0 flex items-center">
              <span className="text-xl font-bold">Golf Swing Analysis</span>
            </Link>
            <div className="ml-10 flex items-baseline space-x-4">
              <Link 
                href="/inbox" 
                className={`px-3 py-2 rounded-md text-sm font-medium ${isActive('/inbox') ? 'bg-gray-900 text-white' : 'text-gray-300 hover:bg-gray-700 hover:text-white'}`}
              >
                Inbox
              </Link>
            </div>
          </div>
          <div className="flex items-center">
            {status === 'authenticated' && user ? (
              <div className="flex items-center space-x-4">
                <span className="text-sm font-medium">{user.name || user.email}</span>
                <button
                  onClick={() => signout()}
                  className="px-3 py-2 rounded-md text-sm font-medium text-gray-300 hover:bg-gray-700 hover:text-white"
                >
                  Sign out
                </button>
              </div>
            ) : (
              <div className="flex items-center space-x-4">
                <Link 
                  href="/auth/signin" 
                  className="px-3 py-2 rounded-md text-sm font-medium text-gray-300 hover:bg-gray-700 hover:text-white"
                >
                  Sign in
                </Link>
                <Link 
                  href="/auth/register" 
                  className="px-3 py-2 rounded-md text-sm font-medium bg-indigo-600 text-white hover:bg-indigo-700"
                >
                  Register
                </Link>
              </div>
            )}
          </div>
        </div>
      </div>
    </nav>
  );
}
|| END ||


|| START ./src/components/VideoPlayerWrapper.tsx ||

'use client';

import { useState, useCallback, useRef, useEffect } from 'react';
import dynamic from 'next/dynamic';
import { v4 as uuidv4 } from 'uuid';
import type { RecordedAction, FeedbackData } from './VideoPlayer';
import type { DrawingPath } from './AnnotationCanvas';
import AudioRecorder from './AudioRecorder';
import FeedbackOrchestrator, { FeedbackSession, AudioTrack, TimelineEvent } from './FeedbackOrchestrator';
import { AppProviders } from '../contexts/AppProviders';
import { useVideoSource, useVideo } from '../contexts/VideoContext';

// Import the AudioChunk type from the AudioRecorder component
import type { AudioChunk } from './AudioRecorder';

// Dynamically import the VideoPlayer with no SSR
const VideoPlayer = dynamic(() => import('./VideoPlayer'), { ssr: false });

// Helper function to convert Blob to base64 for storage
const blobToBase64 = (blob: Blob): Promise<string> => {
  return new Promise((resolve, reject) => {
    const reader = new FileReader();
    reader.onload = () => {
      const dataUrl = reader.result as string;
      resolve(dataUrl);
    };
    reader.onerror = reject;
    reader.readAsDataURL(blob);
  });
};

// Format category name from camelCase to readable format
const getCategoryLabel = (category: string): string => {
  return category.replace(/([A-Z])/g, ' $1').trim().replace(/^./, str => str.toUpperCase());
};

// Enhanced helper function to convert base64 back to Blob for playback
const base64ToBlob = (base64: string, mimeType: string): Blob => {
  try {
    // Validate input parameters
    if (!base64 || typeof base64 !== 'string') {
      console.error('Invalid base64 input: not a string or empty', typeof base64);
      throw new Error('Invalid base64 input: not a string or empty');
    }
    
    if (!mimeType || typeof mimeType !== 'string') {
      console.warn('Invalid or missing MIME type, using default: audio/webm');
      mimeType = 'audio/webm'; // Fallback to default
    }
    
    // First ensure we have a proper data URL with the correct format
    if (!base64.startsWith('data:')) {
      console.error('Invalid base64 string format - missing data: prefix');
      console.debug('String starts with:', base64.substring(0, Math.min(20, base64.length)));
      throw new Error('Invalid base64 string format - missing data: prefix');
    }
    
    if (!base64.includes(',')) {
      console.error('Invalid base64 string format - missing comma separator');
      throw new Error('Invalid base64 string format - missing comma separator');
    }
    
    // Extract the base64 part after the comma
    const base64Data = base64.split(',')[1];
    if (!base64Data) {
      console.error('Invalid base64 string - no data after comma');
      throw new Error('Invalid base64 string - no data after comma');
    }
    
    // Get actual MIME type from the data URL if present
    const headerPart = base64.split(',')[0];
    const mimeMatch = headerPart.match(/^data:(.*?)(;base64)?$/);
    if (mimeMatch && mimeMatch[1]) {
      // If the data URL contains a MIME type, use it instead of the provided mimeType
      console.log(`Using MIME type from data URL (${mimeMatch[1]}) instead of provided type (${mimeType})`);
      mimeType = mimeMatch[1];
    }
    
    try {
      // Decode the base64 string to binary with error handling
      const byteString = atob(base64Data);
      
      // Create an ArrayBuffer to hold the decoded data
      const ab = new ArrayBuffer(byteString.length);
      const ia = new Uint8Array(ab);
      
      // Copy the decoded binary data to the array buffer
      for (let i = 0; i < byteString.length; i++) {
        ia[i] = byteString.charCodeAt(i);
      }
      
      // Create and return a new Blob from the array buffer
      const blob = new Blob([ab], { type: mimeType });
      
      // Validate created blob
      if (blob.size === 0) {
        console.warn('Created an empty blob from base64 data, possible data corruption');
      } else {
        console.log(`Successfully converted base64 to Blob: size=${blob.size}, type=${blob.type}`);
      }
      
      return blob;
    } catch (binaryError) {
      console.error('Error processing binary data:', binaryError);
      throw new Error(`Failed to process binary data: ${binaryError instanceof Error ? binaryError.message : String(binaryError)}`);
    }
  } catch (error) {
    console.error('Error converting base64 to Blob:', error);
    throw error;
  }
};

// Helper function to prepare audio chunks for saving to JSON
const prepareAudioChunksForSave = async (chunks: AudioChunk[]): Promise<any[]> => {
  if (!chunks || chunks.length === 0) {
    console.log('No audio chunks to prepare for save');
    return [];
  }
  
  console.log(`Preparing ${chunks.length} audio chunks for save...`);
  
  // Create a deep copy of the chunks
  return Promise.all(chunks.map(async (chunk, index) => {
    try {
      console.log(`Processing chunk ${index} for save, blob type:`, 
        chunk.blob instanceof Blob ? 'Blob object' : typeof chunk.blob);
      
      // Only convert if it's a Blob and not already a string
      if (chunk.blob instanceof Blob) {
        console.log(`Chunk ${index}: Converting Blob to base64, size: ${chunk.blob.size}, type: ${chunk.blob.type}`);
        
        // Convert Blob to base64 string for storage
        const base64 = await blobToBase64(chunk.blob);
        
        // Log length of base64 string for debugging
        console.log(`Chunk ${index}: Base64 conversion complete, string length: ${base64.length}`);
        
        // Save with MIME type and other properties
        return {
          ...chunk,
          blob: base64, // Replace Blob with base64 string
          mimeType: chunk.mimeType || chunk.blob.type, // Ensure we save the mime type
          url: undefined, // Remove URL property if it exists
          blobUrl: chunk.blobUrl // Keep the Azure Storage blob URL if it exists
        };
      } else if (typeof chunk.blob === 'string' && chunk.blob.startsWith('data:')) {
        // Already a data URL, verify it's properly formatted
        console.log(`Chunk ${index}: Already a data URL, length: ${chunk.blob.length}`);
        
        // Verify data URL format
        const parts = chunk.blob.split(',');
        if (parts.length !== 2) {
          console.warn(`Chunk ${index}: Invalid data URL format - wrong number of parts`);
        }
        
        // Return as is, but ensure all properties are set
        return {
          ...chunk,
          mimeType: chunk.mimeType || 'audio/webm', // Ensure MIME type is set
          url: undefined, // Remove URL property if it exists
          blobUrl: chunk.blobUrl // Keep the Azure Storage blob URL if it exists
        };
      } else {
        console.warn(`Chunk ${index}: Unknown blob format: ${typeof chunk.blob}`);
        
        // Return with minimal valid properties
        return {
          ...chunk,
          blob: typeof chunk.blob === 'string' ? chunk.blob : '', // Keep string or use empty string
          mimeType: chunk.mimeType || 'audio/webm', // Ensure MIME type is set
          url: undefined, // Remove URL property if it exists
          blobUrl: chunk.blobUrl // Keep the Azure Storage blob URL if it exists
        };
      }
    } catch (error) {
      console.error(`Error converting audio chunk ${index} for storage:`, error);
      return null;
    }
  })).then(results => {
    const validResults = results.filter(Boolean); // Remove any failed conversions
    console.log(`Successfully prepared ${validResults.length} of ${chunks.length} audio chunks for save`);
    return validResults;
  });
};

// Helper function to restore audio chunks when loading saved data
const restoreAudioChunks = (savedChunks: any[]): AudioChunk[] => {
  if (!savedChunks || savedChunks.length === 0) {
    console.log('No audio chunks to restore');
    return [];
  }
  
  console.log(`Restoring ${savedChunks.length} audio chunks...`);
  
  return savedChunks.map((savedChunk, index) => {
    try {
      // If blob is already a Blob object, just return the chunk as is
      if (savedChunk.blob instanceof Blob) {
        console.log(`Chunk ${index}: Already a Blob object`);
        return savedChunk;
      }
      
      // If blob is a string (data URL), validate and keep as a string for compatibility
      if (typeof savedChunk.blob === 'string') {
        if (savedChunk.blob.startsWith('data:')) {
          console.log(`Chunk ${index}: Found data URL, keeping as string for AudioRecorder component`);
          
          // Try to validate the data URL format
          try {
            const dataUrlParts = savedChunk.blob.split(',');
            if (dataUrlParts.length !== 2) {
              console.warn(`Chunk ${index}: Invalid data URL format - wrong number of parts`);
            }
            // Check if the mime type part is valid
            const mimeMatch = dataUrlParts[0].match(/:(.*?);/);
            if (!mimeMatch) {
              console.warn(`Chunk ${index}: Data URL has no valid MIME type`);
            }
          } catch (validationError) {
            console.warn(`Chunk ${index}: Error validating data URL:`, validationError);
          }
          
          // Ensure all required properties are present
          return {
            ...savedChunk,
            blob: savedChunk.blob, // Keep the data URL as is
            mimeType: savedChunk.mimeType || 'audio/webm', // Set default MIME type if missing
            startTime: savedChunk.startTime || 0,
            duration: savedChunk.duration || 0,
            videoTime: savedChunk.videoTime || 0,
            blobUrl: savedChunk.blobUrl // Keep the Azure Storage blob URL if it exists
          };
        } else {
          console.warn(`Chunk ${index}: String blob doesn't start with 'data:' prefix: ${savedChunk.blob.substring(0, 20)}...`);
        }
      }
      
      console.warn(`Unknown blob format in chunk ${index}:`, typeof savedChunk.blob);
      // Log more details to aid debugging
      if (typeof savedChunk.blob === 'string') {
        console.info(`Chunk ${index} string length: ${savedChunk.blob.length}, starts with: ${savedChunk.blob.substring(0, 30)}...`);
      } else if (savedChunk.blob === null) {
        console.warn(`Chunk ${index}: Blob is null`);
      } else if (savedChunk.blob === undefined) {
        console.warn(`Chunk ${index}: Blob is undefined`);
      }
      
      // Return a simplified chunk as a fallback (audio won't play but won't crash either)
      return {
        ...savedChunk,
        blob: savedChunk.blob || '', // Keep as is even if invalid
        mimeType: savedChunk.mimeType || 'audio/webm',
        startTime: savedChunk.startTime || 0,
        duration: savedChunk.duration || 0,
        videoTime: savedChunk.videoTime || 0,
        blobUrl: savedChunk.blobUrl // Keep the Azure Storage blob URL if it exists
      };
    } catch (error) {
      console.error(`Error restoring audio chunk ${index}:`, error);
      return null;
    }
  }).filter(Boolean as any); // Remove any failed conversions
};

// Convert the legacy FeedbackData to the new FeedbackSession format
const convertLegacyDataToSession = (legacyData: FeedbackData): FeedbackSession => {
  // Create a new FeedbackSession from the legacy data
  const audioTrack: AudioTrack = {
    chunks: legacyData.audioChunks || [],
    totalDuration: legacyData.audioChunks?.reduce((total, chunk) => total + chunk.duration, 0) || 0
  };
  
  // Convert actions to timeline events
  const events: TimelineEvent[] = legacyData.actions.map(action => {
    return {
      id: uuidv4(),
      type: action.type === 'annotation' ? 'annotation' : 'video',
      timeOffset: action.timestamp,
      duration: action.type === 'audio' ? action.details?.duration : undefined,
      payload: 
        action.type === 'annotation' 
          ? { action: action.details?.clear ? 'clear' : 'draw', path: action.details?.path } 
          : { action: action.type, ...action.details }
    };
  });
  
  return {
    id: legacyData.sessionId || uuidv4(),
    videoId: legacyData.videoId,
    startTime: legacyData.startTime,
    endTime: legacyData.endTime,
    audioTrack,
    events
  };
};

// Convert FeedbackSession to legacy FeedbackData format for compatibility
const convertSessionToLegacyData = (session: FeedbackSession): FeedbackData => {
  // Create a new FeedbackData object
  const legacyData: FeedbackData = {
    sessionId: session.id,
    videoId: session.videoId,
    startTime: session.startTime,
    endTime: session.endTime,
    actions: [],
    audioChunks: session.audioTrack.chunks,
    annotations: []
  };
  
  // Collect all annotation paths
  const annotations: DrawingPath[] = [];
  
  // Convert timeline events to legacy actions
  session.events.forEach(event => {
    if (event.type === 'video') {
      const action: RecordedAction = {
        type: event.payload.action,
        timestamp: event.timeOffset,
        videoTime: 0, // Will need proper conversion
        details: { ...event.payload }
      };
      delete action.details.action;
      legacyData.actions.push(action);
    } 
    else if (event.type === 'annotation') {
      const action: RecordedAction = {
        type: 'annotation',
        timestamp: event.timeOffset,
        videoTime: 0, // Will need proper conversion
        details: event.payload.action === 'clear' 
          ? { clear: true } 
          : { path: event.payload.path }
      };
      legacyData.actions.push(action);
      
      // Also collect annotation paths for backwards compatibility
      if (event.payload.action === 'draw' && event.payload.path) {
        // Add timing metadata to the annotation for proper replay
        const pathWithTiming = {
          ...event.payload.path,
          timeOffset: event.timeOffset,
          globalTimeOffset: event.timeOffset,
          videoTime: event.timeOffset,
          tool: event.payload.path.tool || 'freehand' // Ensure tool type is preserved
        };
        annotations.push(pathWithTiming);
      }
    }
    else if (event.type === 'marker') {
      // Skip markers, as they don't have a direct equivalent in the legacy format
      console.log('Skipping marker event in legacy conversion:', event.payload.text);
    }
  });
  
  // Add annotations
  legacyData.annotations = annotations;
  
  return legacyData;
};

interface VideoPlayerWrapperProps {
  categories?: Record<string, number | null>;
  onCategoriesCleared?: () => void;
  onCategoriesLoaded?: (categories: Record<string, number>) => void;
  onReplayModeChange?: (isReplay: boolean) => void;
  videoUrl?: string;
  videoId?: string;
  contentToReview?: any; // Allow passing the full content object for display
  initialSession?: any; // Allow passing an initial session from Cosmos DB
  onSessionComplete?: (session: FeedbackSession) => void; // Callback when session is complete
}

export default function VideoPlayerWrapper({ 
  categories = {}, 
  onCategoriesCleared,
  onCategoriesLoaded,
  onReplayModeChange,
  videoUrl,
  videoId = 'sample-video',
  contentToReview,
  initialSession,
  onSessionComplete
}: VideoPlayerWrapperProps) {
  // Wrap in try/catch in case we're not in a provider
  let videoContext;
  try {
    videoContext = useVideo();
  } catch (error) {
    console.warn('VideoContext not available:', error);
    videoContext = { setVideoUrl: () => {}, state: {} };
  }
  
  // Set video URL in context when videoUrl prop changes
  useEffect(() => {
    if (videoUrl && videoContext.setVideoUrl) {
      console.log('VideoPlayerWrapper: Setting video URL in context:', videoUrl);
      videoContext.setVideoUrl(videoUrl);
    }
  }, [videoUrl, videoContext]);
  // Log categories passed from parent on every render
  console.log('VideoPlayerWrapper received categories:', categories);
  const [mode, setMode] = useState<'record' | 'replay'>('record');
  const [isActive, setIsActive] = useState(false);
  const [currentSession, setCurrentSession] = useState<FeedbackSession | null>(initialSession || null);
  const [feedbackData, setFeedbackData] = useState<FeedbackData>({
    sessionId: '',
    videoId: videoId,
    actions: [],
    startTime: 0,
    annotations: [],
    audioChunks: [],
  });
  
  // References
  const videoRef = useRef<HTMLVideoElement | null>(null);
  const canvasRef = useRef<HTMLCanvasElement | null>(null);
  const orchestratorRef = useRef<any>(null);
  const annotationCanvasComponentRef = useRef<any>(null);
  
  // Function to set the video reference from the child component
  const setVideoElementRef = useCallback((el: HTMLVideoElement | null) => {
    videoRef.current = el;
  }, []);
  
  // Start recording
  const startRecording = useCallback(() => {
    setMode('record');
    
    // Clear any existing annotations before starting
    if (annotationCanvasComponentRef.current) {
      console.log('Clearing annotations before starting new recording');
      if (annotationCanvasComponentRef.current.clearCanvasDrawings) {
        annotationCanvasComponentRef.current.clearCanvasDrawings();
      }
    }
    
    // Reset video to beginning if needed
    if (videoRef.current) {
      console.log('Resetting video position before starting recording');
      videoRef.current.currentTime = 0;
    }
    
    if (orchestratorRef.current) {
      orchestratorRef.current.startRecordingSession();
      setIsActive(true);
    }
  }, []);
  
  // Stop recording
  const stopRecording = useCallback(() => {
    if (orchestratorRef.current) {
      console.log('Stopping recording session');
      
      // End recording session
      orchestratorRef.current.endRecordingSession();
      setIsActive(false);
      
      // Reset video to beginning
      if (videoRef.current) {
        console.log('Resetting video position to start');
        videoRef.current.currentTime = 0;
        
        // If it's playing, pause it
        if (!videoRef.current.paused) {
          videoRef.current.pause();
        }
      }
      
      // Clear annotations
      if (annotationCanvasComponentRef.current) {
        console.log('Clearing annotations after recording stopped');
        if (annotationCanvasComponentRef.current.clearCanvasDrawings) {
          annotationCanvasComponentRef.current.clearCanvasDrawings();
        }
      }
      
      // Call the onCategoriesCleared callback if it exists
      if (onCategoriesCleared) {
        onCategoriesCleared();
      }
      
      // Make sure session availability is updated immediately
      if (typeof window !== 'undefined') {
        window.__hasRecordedSession = true;
        console.log('Session available flag set to true after stopping recording');
        
        // Dispatch a custom event to notify about session availability
        window.dispatchEvent(new Event('session-available'));
      }
    }
  }, [onCategoriesCleared, currentSession]);
  
  // Start replaying the recorded session with optimized state changes
  const startReplay = useCallback(() => {
    // Prepare everything before changing state
    // This prevents multiple re-renders
    
    // Prepare canvas
    if (annotationCanvasComponentRef.current && annotationCanvasComponentRef.current.clearCanvasDrawings) {
      console.log('Clearing annotations before starting replay');
      annotationCanvasComponentRef.current.clearCanvasDrawings();
    }
    
    // Prepare video if available
    if (videoRef.current) {
      console.log('Resetting video position before starting replay');
      videoRef.current.currentTime = 0;
    }
    
    if (orchestratorRef.current && currentSession) {
      // First, handle categories clearing if needed
      if (onCategoriesCleared) {
        console.log('Clearing categories before replay');
        onCategoriesCleared();
      }
      
      console.log('Replaying session with categories:', currentSession.categories);
      
      // Load the session if needed
      if (typeof window !== 'undefined' && !window.__sessionReady) {
        console.log('Loading session data');
        orchestratorRef.current.loadSession(currentSession);
      }
      
      // Now batch the state updates to reduce re-renders
      try {
        // Batch state updates together to minimize renders
        console.log('Batching state updates for replay');
        setMode('replay');
        setIsActive(true);
        
        // Use setTimeout to ensure state updates have completed
        // before starting the replay, which prevents race conditions
        setTimeout(() => {
          if (orchestratorRef.current) {
            console.log('Starting replay after state updates');
            orchestratorRef.current.startReplay();
            
            // Set the completed review status if needed
            if (typeof window !== 'undefined' && window.__isCompletedVideo) {
              console.log('Starting replay of completed review');
            }
          }
        }, 0);
      } catch (error) {
        console.error('Error in replay state updates:', error);
      }
    } else {
      alert('No recorded session to replay. Record a session first.');
    }
  }, [currentSession, onCategoriesCleared]);
  
  // Stop replay
  const stopReplay = useCallback(() => {
    if (orchestratorRef.current) {
      orchestratorRef.current.stopReplay();
      setIsActive(false);
      
      // Reset video to beginning
      if (videoRef.current) {
        console.log('Resetting video position to start after replay');
        videoRef.current.currentTime = 0;
        
        // If it's playing, pause it
        if (!videoRef.current.paused) {
          videoRef.current.pause();
        }
      }
      
      // Clear annotations
      if (annotationCanvasComponentRef.current) {
        console.log('Clearing annotations after replay stopped');
        if (annotationCanvasComponentRef.current.clearCanvasDrawings) {
          annotationCanvasComponentRef.current.clearCanvasDrawings();
        }
      }
    }
  }, []);
  
  // Handle session completion
  const handleSessionComplete = useCallback((session: FeedbackSession) => {
    // First create a deep copy of the session to avoid any referential issues
    const sessionCopy = JSON.parse(JSON.stringify(session));
    
    // Deep copy the categories to ensure they're not references
    const categoriesCopy = JSON.parse(JSON.stringify(categories));
    
    // Log the actual values we're trying to save
    console.log('Current categories to save:', categoriesCopy);
    
    // Log which categories are true (selected)
    const selectedCategories = Object.entries(categoriesCopy)
      .filter(([_, value]) => value)
      .map(([key]) => key);
    console.log('Selected categories:', selectedCategories);
    
    // Add the categories to the session
    const sessionWithCategories = {
      ...sessionCopy,
      categories: categoriesCopy
    };
    
    setCurrentSession(sessionWithCategories);
    
    // Also update legacy feedbackData for compatibility
    const legacyData = convertSessionToLegacyData(sessionWithCategories);
    setFeedbackData(legacyData);
    
    // Update session availability flag immediately
    if (typeof window !== 'undefined') {
      window.__hasRecordedSession = true;
      console.log('Session available flag set to true');
      
      // Dispatch a custom event to notify about session availability
      window.dispatchEvent(new Event('session-available'));
    }
    
    console.log('Session completed with categories:', sessionWithCategories);
    
    // Call the parent's onSessionComplete callback if provided
    if (onSessionComplete) {
      onSessionComplete(sessionWithCategories);
    }
  }, [categories, onSessionComplete]);
  
  // Handle audio recording completed
  const handleAudioRecorded = useCallback((audioTrack: AudioTrack) => {
    console.log('Audio recorded:', audioTrack);
  }, []);
  
  // Draw annotation
  const drawAnnotation = useCallback((path: DrawingPath) => {
    // Only update the video time if it's not already set (for new annotations, not replayed ones)
    if (videoRef.current && !path.videoTime) {
      path.videoTime = videoRef.current.currentTime * 1000;
      
      console.log('Setting videoTime for new annotation:', path.videoTime);
    }
    
    // Pass annotation to the annotation canvas component via the VideoPlayer
    if (annotationCanvasComponentRef.current) {
      // Log the attempt to draw to aid debugging
      console.log('Drawing annotation via handleManualAnnotation:', {
        pathPoints: path.points?.length || 0,
        color: path.color,
        width: path.width,
        videoTime: path.videoTime,
        timeOffset: (path as any).timeOffset, // For debug only
        isReplay: !!path.videoTime // If videoTime is set, it's likely a replay
      });
      
      try {
        // Use the handleManualAnnotation method exposed by the AnnotationCanvas
        annotationCanvasComponentRef.current.handleManualAnnotation(path);
      } catch (error) {
        console.error('Error drawing annotation:', error);
      }
    } else {
      console.warn('Could not draw annotation: annotation canvas ref is not available');
    }
  }, []);
  
  // Clear annotations
  const clearAnnotations = useCallback(() => {
    if (annotationCanvasComponentRef.current) {
      console.log('Clearing annotations via clearCanvasDrawings');
      
      try {
        // Use the clearCanvasDrawings method exposed by the AnnotationCanvas
        if (annotationCanvasComponentRef.current.clearCanvasDrawings) {
          annotationCanvasComponentRef.current.clearCanvasDrawings();
        } else {
          console.warn('clearCanvasDrawings method not found on annotationCanvas');
        }
      } catch (error) {
        console.error('Error clearing annotations:', error);
      }
    } else {
      console.warn('Could not clear annotations: annotation canvas ref is not available');
    }
  }, []);
  
  // Download session data as JSON
  const downloadSessionData = useCallback(async () => {
    if (!currentSession) {
      alert('No recorded session to download.');
      return;
    }
    
    try {
      // Create a deep copy to avoid modifying the original state
      const sessionCopy = JSON.parse(JSON.stringify(currentSession));
      
      // Ensure categories are included in the download
      console.log('Current session categories before download:', sessionCopy.categories);
      
      // Deep copy the categories to ensure they're not references
      const categoriesCopy = JSON.parse(JSON.stringify(categories));
      
      // Log which categories are currently selected
      const selectedCategories = Object.entries(categoriesCopy)
        .filter(([_, value]) => value)
        .map(([key]) => key);
      console.log('Selected categories for download:', selectedCategories);
      
      // Make sure we have the latest categories
      sessionCopy.categories = categoriesCopy;
      console.log('Updated session categories for download:', sessionCopy.categories);
      
      // Prepare audio chunks for serialization
      if (sessionCopy.audioTrack && sessionCopy.audioTrack.chunks.length > 0) {
        try {
          console.log(`Preparing ${sessionCopy.audioTrack.chunks.length} audio chunks for save...`);
          sessionCopy.audioTrack.chunks = await prepareAudioChunksForSave(sessionCopy.audioTrack.chunks);
          console.log('Audio chunks prepared successfully:', sessionCopy.audioTrack.chunks.length);
        } catch (error) {
          console.error('Failed to prepare audio chunks for saving:', error);
          alert('There was an issue preparing audio data for download. Some audio content may be missing.');
        }
      }
      
      const dataStr = JSON.stringify(sessionCopy, null, 2);
      const dataUri = 'data:application/json;charset=utf-8,'+ encodeURIComponent(dataStr);
      
      const linkElement = document.createElement('a');
      linkElement.setAttribute('href', dataUri);
      linkElement.setAttribute('download', `feedback-session-${currentSession.id}.json`);
      document.body.appendChild(linkElement);
      linkElement.click();
      document.body.removeChild(linkElement);
    } catch (error) {
      console.error('Error during download process:', error);
      alert('Failed to download session data. See console for details.');
    }
  }, [currentSession, categories]);
  
  // Handle file upload for previously saved session data
  const handleFileUpload = useCallback((e: React.ChangeEvent<HTMLInputElement>) => {
    if (!e.target.files || e.target.files.length === 0) return;
    
    const file = e.target.files[0];
    const reader = new FileReader();
    
    reader.onload = (event) => {
      try {
        // Parse the JSON data
        const jsonData = JSON.parse(event.target?.result as string);
        
        // Check if it's the new format or legacy format
        if (jsonData.events && jsonData.audioTrack) {
          // It's the new FeedbackSession format
          const loadedSession = jsonData as FeedbackSession;
          
          // Log if we have categories
          console.log('Loaded session categories:', loadedSession.categories);
          
          // Restore audio chunks with proper Blob objects if they exist
          if (loadedSession.audioTrack && loadedSession.audioTrack.chunks) {
            loadedSession.audioTrack.chunks = restoreAudioChunks(loadedSession.audioTrack.chunks);
          }
          
          setCurrentSession(loadedSession);
          // Also update legacy format for compatibility
          setFeedbackData(convertSessionToLegacyData(loadedSession));
          
          console.log('Loaded feedback session:', loadedSession);
        } else {
          // It's the legacy FeedbackData format
          const legacyData = jsonData as FeedbackData;
          
          // Restore audio chunks
          if (legacyData.audioChunks) {
            legacyData.audioChunks = restoreAudioChunks(legacyData.audioChunks);
          }
          
          setFeedbackData(legacyData);
          // Convert to new format
          const newSession = convertLegacyDataToSession(legacyData);
          setCurrentSession(newSession);
          
          console.log('Loaded legacy feedback data and converted to session:', legacyData);
        }
      } catch (error) {
        console.error("Failed to parse uploaded file:", error);
        alert("Invalid feedback data file. Please upload a valid JSON file.");
      }
    };
    
    reader.onerror = (error) => {
      console.error('Error reading file:', error);
      alert('Error reading the file. Please try again.');
    };
    
    reader.readAsText(file);
  }, []);
  
  // Get an orchestrator reference
  const getOrchestratorRef = useCallback((orchestratorInstance: any) => {
    orchestratorRef.current = orchestratorInstance;
  }, []);
  
  // Method to record a category change
  const recordCategoryChange = useCallback((category: string, rating: number) => {
    if (orchestratorRef.current) {
      console.log(`Recording category change in orchestrator: ${category} = ${rating}`);
      
      // Check if recording is active - if not, initiate a minimal session if needed
      if (!isActive && mode === 'record') {
        console.log('Not actively recording, but will still save category rating');
        // If we don't have a current session, create a simple one for storing categories
        if (!currentSession) {
          console.log('Creating minimal session for categories');
          const newSession = {
            id: `temp-${Date.now()}`,
            videoId: videoId || 'unknown',
            startTime: Date.now(),
            events: [],
            audioTrack: { chunks: [], totalDuration: 0 },
            categories: { [category]: rating }
          };
          setCurrentSession(newSession);
        }
      }
      
      // Always store category changes, even if not actively recording
      orchestratorRef.current.handleCategoryEvent(category, rating);
    } else {
      console.warn('Unable to record category change - orchestrator not available');
    }
  }, [mode, isActive, currentSession, videoId]);
  
  // Get a reference to the annotation canvas via the video player
  const getVideoPlayerRef = useCallback((videoPlayerInstance: any) => {
    // Store the video player reference
    annotationCanvasComponentRef.current = videoPlayerInstance?.annotationCanvas;
    
    // Log the reference to ensure we have it
    console.log('Got video player ref with annotation canvas:', {
      videoPlayer: !!videoPlayerInstance,
      annotationCanvas: !!videoPlayerInstance?.annotationCanvas,
      canvasMethods: videoPlayerInstance?.annotationCanvas ? 
        Object.keys(videoPlayerInstance.annotationCanvas) : []
    });
  }, []);
  
  // Listen for replay progress to detect completion
  useEffect(() => {
    if (orchestratorRef.current && mode === 'replay' && isActive) {
      // Check if orchestrator has a replayProgress property
      const progress = orchestratorRef.current.replayProgress;
      if (progress === 100) {
        // Replay has completed, reset the UI state
        console.log('Detected replay completion via progress = 100%, resetting UI state');
        setIsActive(false);
        
        // Reset video to beginning
        if (videoRef.current) {
          console.log('Auto-resetting video position to start after replay completion');
          videoRef.current.currentTime = 0;
          
          // If it's playing, pause it
          if (!videoRef.current.paused) {
            videoRef.current.pause();
          }
        }
        
        // Clear annotations
        if (annotationCanvasComponentRef.current) {
          console.log('Auto-clearing annotations after replay completion');
          if (annotationCanvasComponentRef.current.clearCanvasDrawings) {
            annotationCanvasComponentRef.current.clearCanvasDrawings();
          }
        }
      }
    }
  }, [mode, isActive, orchestratorRef.current?.replayProgress]);

  // Clean up resources when component unmounts
  useEffect(() => {
    return () => {
      if (orchestratorRef.current && isActive) {
        if (mode === 'record') {
          orchestratorRef.current.endRecordingSession();
        } else {
          orchestratorRef.current.stopReplay();
        }
      }
    };
  }, [isActive, mode]);

  // Define window type with our custom property
  declare global {
    interface Window {
      __videoPlayerWrapper?: {
        recordCategoryChange: (category: string, rating: number) => void;
        isRecording: boolean;
      };
      __globalTimePosition?: number; // Global timeline position in milliseconds
      __lastClearTime?: number; // When the canvas was last cleared (global timeline time)
      __hasRecordedSession?: boolean;
    }
  }
  
  // Expose methods to the parent component and notify about mode changes
  useEffect(() => {
    // This runs once when the component mounts and when dependencies change
    if (typeof window !== 'undefined') {
      // Set global reference available to parent component
      window.__videoPlayerWrapper = {
        recordCategoryChange,
        isRecording: mode === 'record' && isActive
      };
      
      // Update session availability flag
      window.__hasRecordedSession = currentSession !== null;
    }
    
    // Notify parent component about replay mode changes
    if (onReplayModeChange) {
      const isReplay = mode === 'replay';
      console.log(`Notifying parent about replay mode: ${isReplay}`);
      onReplayModeChange(isReplay);
    }
    
    return () => {
      // Clean up on unmount
      if (typeof window !== 'undefined' && window.__videoPlayerWrapper) {
        delete window.__videoPlayerWrapper;
      }
    };
  }, [recordCategoryChange, mode, isActive, onReplayModeChange, currentSession]);
  
  // Load session but only start replay if not a completed video
  useEffect(() => {
    if (initialSession && !isActive && mode === 'record') {
      console.log("Initial session provided, preparing for replay");
      
      // Set a small delay to ensure everything is properly initialized
      setTimeout(() => {
        if (orchestratorRef.current) {
          // Set the current session
          setCurrentSession(initialSession);
          
          // Load the session without auto-starting replay or switching mode yet
          orchestratorRef.current.loadSession(initialSession);
          
          // Check if this is a new session or a completed video
          if (typeof window !== 'undefined' && !window.__isCompletedVideo) {
            console.log("Auto-starting replay for new session");
            // For new reviews, we auto-start and switch to replay mode
            setMode('replay');
            orchestratorRef.current.startReplay();
            setIsActive(true);
          } else {
            console.log("Completed video review - replay is ready but not auto-started");
            // For completed videos, just set ready flag but don't change mode yet
            if (typeof window !== 'undefined') {
              window.__sessionReady = true;
              // Dispatch event to notify UI
              window.dispatchEvent(new Event('session-ready'));
            }
          }
        }
      }, 1500);
    }
  }, [initialSession]);
  
  // Get the effective URL from the context with fallback
  let contextVideoUrl;
  try {
    const videoSource = useVideoSource();
    contextVideoUrl = videoSource?.effectiveUrl;
  } catch (error) {
    console.warn('VideoSource not available:', error);
    contextVideoUrl = videoUrl; // Fallback to prop
  }
  
  // Log when the effective URL changes
  useEffect(() => {
    console.log('VideoPlayerWrapper: Context effective URL:', contextVideoUrl);
  }, [contextVideoUrl]);
  
  return (
    <div className="w-full">
        {/* Hidden buttons that will be triggered by parent */}
        <div className="hidden">
          <button
            id="startRecordingButton"
            onClick={startRecording}
          >
            Start Recording
          </button>
          
          <button
            id="startReplayButton"
            onClick={startReplay}
            disabled={!currentSession}
          >
            Replay Session
          </button>
          
          <button
            id="downloadDataButton"
            onClick={downloadSessionData}
            disabled={!currentSession}
          >
            Download Data
          </button>
          
          <label>
            <input 
              id="fileUploadInput"
              type="file" 
              accept=".json" 
              onChange={handleFileUpload} 
            />
            Load Data
          </label>
          
          <button
            id="stopButton"
            onClick={mode === 'record' ? stopRecording : stopReplay}
          >
            Stop
          </button>
        </div>
        
        
        {/* Feedback Orchestrator handles all coordination */}
        <div className="relative">
          <VideoPlayer 
            ref={getVideoPlayerRef}
            isRecording={mode === 'record' && isActive}
            isReplaying={mode === 'replay' && isActive}
            setVideoRef={setVideoElementRef}
            replayAnnotations={currentSession?.events
              ?.filter(e => e.type === 'annotation' && e.payload?.action === 'draw' && e.payload?.path)
              ?.map(e => {
                // Make sure the tool property is preserved during replay
                const tool = e.payload.path.tool || 'freehand';
                console.log(`Preparing annotation for replay: tool=${tool}, points=${e.payload.path.points?.length}`);
                
                return {
                  ...e.payload.path,
                  timeOffset: e.timeOffset,
                  globalTimeOffset: e.timeOffset,
                  videoTime: e.timeOffset,
                  tool: tool  // Explicitly set the tool to ensure it's included
                };
              }) || feedbackData.annotations || []}
            videoUrl={videoUrl}
            onRecordAction={(action) => {
              // Forward video actions to the orchestrator
              if (orchestratorRef.current && mode === 'record' && isActive) {
                switch(action.type) {
                  case 'play':
                  case 'pause':
                  case 'seek':
                  case 'playbackRate':
                  case 'keyboardShortcut':
                    orchestratorRef.current.handleVideoEvent(action.type, action.details);
                    break;
                  case 'annotation':
                    if (action.details?.clear) {
                      orchestratorRef.current.handleAnnotationEvent('clear');
                    } else if (action.details?.path) {
                      orchestratorRef.current.handleAnnotationEvent('draw', action.details.path);
                    }
                    break;
                }
              }
            }}
            onAnnotationAdded={(annotation) => {
              // Forward annotation events to the orchestrator
              if (orchestratorRef.current && mode === 'record' && isActive) {
                orchestratorRef.current.handleAnnotationEvent('draw', annotation);
              }
            }}
          />
          
          {/* Initialize the Orchestrator */}
          <FeedbackOrchestrator
          videoElementRef={videoRef}
          canvasRef={canvasRef}
          drawAnnotation={drawAnnotation}
          clearAnnotations={clearAnnotations}
          onAudioRecorded={handleAudioRecorded}
          onSessionComplete={handleSessionComplete}
          initialSession={currentSession}
          mode={mode}
          onCategoriesLoaded={(loadedCategories) => {
            // When a session is loaded with categories, we need to notify the parent component
            if (loadedCategories) {
              console.log('WRAPPER: Received loaded categories from orchestrator:', loadedCategories);
              
              // First clear existing categories
              if (onCategoriesCleared) {
                console.log('WRAPPER: Clearing existing categories before loading new ones');
                onCategoriesCleared();
              }
              
              // Check if we have any true categories
              const hasCheckedCategories = Object.values(loadedCategories).some(isChecked => isChecked);
              console.log(`WRAPPER: Has checked categories: ${hasCheckedCategories}`);
              
              // Then load the saved categories using the callback if available
              if (hasCheckedCategories && onCategoriesLoaded) {
                console.log('WRAPPER: Passing categories to parent component');
                
                // Delay slightly to ensure UI state is updated properly after clearing
                setTimeout(() => {
                  onCategoriesLoaded(loadedCategories);
                }, 100);
              } else {
                console.log('WRAPPER: No checked categories or no callback available');
              }
            } else {
              console.warn('WRAPPER: No categories data received from orchestrator');
            }
          }}
          ref={getOrchestratorRef}
        />
      </div>
      
      {currentSession && (
        <div className="mt-6 p-4 bg-gray-100 rounded-lg">
          <h3 className="text-lg font-semibold mb-2">Recorded Session</h3>
          <div className="text-sm">
            <p><strong>Session ID:</strong> {currentSession.id}</p>
            <p><strong>Start Time:</strong> {new Date(currentSession.startTime).toLocaleString()}</p>
            <p><strong>Events:</strong> {currentSession.events.length} recorded actions</p>
            <p><strong>Audio Duration:</strong> {(currentSession.audioTrack.totalDuration / 1000).toFixed(2)}s</p>
            
            {currentSession.categories && Object.keys(currentSession.categories).length > 0 && (
              <div className="mt-2">
                <p><strong>Category Ratings:</strong></p>
                <ul className="list-none space-y-2">
                  {Object.entries(currentSession.categories)
                    .filter(([_, rating]) => rating !== null && rating > 0)
                    .map(([category, rating]) => (
                      <li key={category}>
                        <div>
                          {category.replace(/([A-Z])/g, ' $1').trim().replace(/^./, str => str.toUpperCase())}
                        </div>
                        <div className="text-yellow-500 flex text-sm">
                          {[1, 2, 3, 4, 5].map((star) => (
                            <span key={star} className={star <= (rating as number) ? 'text-yellow-500' : 'text-gray-300'}>
                              ★
                            </span>
                          ))}
                        </div>
                      </li>
                    ))}
                </ul>
              </div>
            )}
          </div>
          
          <div className="max-h-48 overflow-y-auto mt-4">
            <h4 className="font-medium text-sm mb-2">Timeline Events:</h4>
            <ul className="text-xs bg-white rounded p-2">
              {currentSession.events.map((event, index) => (
                <li key={index} className="mb-1 p-2 border-b">
                  <span className="font-semibold">{event.type}</span> at{' '}
                  <span className="font-mono">{(event.timeOffset / 1000).toFixed(2)}s</span>
                  {event.type === 'video' && (
                    <span className="block text-gray-600">
                      Action: {event.payload.action}
                      {event.payload.to !== undefined && ` (to: ${event.payload.to})`}
                    </span>
                  )}
                  {event.type === 'annotation' && (
                    <span className="block text-gray-600">
                      {event.payload.action === 'clear' 
                        ? "Cleared annotations" 
                        : `Drew annotation with ${event.payload.path?.points?.length || 0} points`}
                    </span>
                  )}
                  {event.type === 'marker' && (
                    <span className="block text-gray-600">
                      Marker: {event.payload.text}
                    </span>
                  )}
                  {event.type === 'category' && (
                    <span className="block text-gray-600">
                      Category: {getCategoryLabel(event.payload.category)} {event.payload.rating > 0 ? `(rated ${event.payload.rating}★)` : '(cleared)'}
                    </span>
                  )}
                </li>
              ))}
            </ul>
          </div>
        </div>
      )}
    </div>
  );
}
|| END ||


|| START ./src/components/AnnotationCanvas.tsx ||

'use client';

import React, { useRef, useState, useEffect, useMemo, forwardRef, useImperativeHandle, useCallback } from 'react';
import { useTimeline, useLastClearTime } from '../contexts/TimelineContext';

export interface Point {
  x: number;
  y: number;
}

export type DrawingTool = 'freehand' | 'line';

export interface DrawingPath {
  points: Point[];
  color: string;
  width: number;
  timestamp: number;
  videoTime?: number; // Time in the video when this annotation was created (in ms)
  tool?: DrawingTool; // The tool used to create this drawing
}

interface AnnotationCanvasProps {
  width: number;
  height: number;
  isEnabled: boolean;
  currentTime: number;
  isRecording?: boolean;
  isReplaying?: boolean;
  onAnnotationAdded?: (path: DrawingPath) => void;
  replayAnnotations?: DrawingPath[];
  toolColor?: string;
  toolWidth?: number;
  toolType?: DrawingTool;
  clearCanvas?: boolean;
  onClearComplete?: () => void;
}

const AnnotationCanvas = forwardRef<any, AnnotationCanvasProps>(({
  width,
  height,
  isEnabled,
  currentTime,
  isRecording = false,
  isReplaying = false,
  onAnnotationAdded,
  replayAnnotations = [],
  toolColor = '#ff0000',
  toolWidth = 4,
  toolType = 'freehand',
  clearCanvas = false,
  onClearComplete
}, ref) => {
  const canvasRef = useRef<HTMLCanvasElement>(null);
  const [isDrawing, setIsDrawing] = useState(false);
  const [currentPath, setCurrentPath] = useState<Point[]>([]);
  const [allDrawings, setAllDrawings] = useState<DrawingPath[]>([]);
  const [startPoint, setStartPoint] = useState<Point | null>(null);
  
  // Get timeline context
  const { state: { currentPosition: globalTimePosition } } = useTimeline();
  const { lastClearTime } = useLastClearTime();
  
  // Get canvas context in a memoized way
  const getContext = useCallback(() => {
    const canvas = canvasRef.current;
    if (!canvas) return null;
    const ctx = canvas.getContext('2d');
    if (!ctx) return null;
    return ctx;
  }, [canvasRef]);

  // Clear the canvas
  const clearCanvasDrawings = useCallback(() => {
    console.log('AnnotationCanvas: Starting canvas clear operation');
    const ctx = getContext();
    if (ctx) {
      ctx.clearRect(0, 0, width, height);
      console.log('AnnotationCanvas: Canvas context cleared');
    }
    setAllDrawings([]);
    
    // Return a promise that resolves when the state update is likely complete
    return new Promise<void>(resolve => {
      // Use requestAnimationFrame to wait for the next frame after state update
      requestAnimationFrame(() => {
        console.log('AnnotationCanvas: Canvas clear state update processed');
        resolve();
      });
    });
  }, [getContext, width, height]);

  // Listen for external clear command
  useEffect(() => {
    if (clearCanvas) {
      // Use requestAnimationFrame to ensure we're in a proper animation frame
      requestAnimationFrame(() => {
        // Clear the canvas and wait for completion
        clearCanvasDrawings()
          .then(() => {
            // Use another requestAnimationFrame to ensure the clearing has been rendered
            requestAnimationFrame(() => {
              // Now that the canvas is definitely cleared, notify the parent
              if (onClearComplete) {
                console.log('AnnotationCanvas: Canvas clearing complete, notifying parent');
                onClearComplete();
              }
            });
          })
          .catch(err => {
            console.error('Error during canvas clearing:', err);
            // Still call the completion handler even if there was an error
            if (onClearComplete) {
              onClearComplete();
            }
          });
      });
    }
  }, [clearCanvas, onClearComplete]);

  // Initialize canvas
  useEffect(() => {
    const canvas = canvasRef.current;
    if (!canvas) return;
    
    canvas.width = width;
    canvas.height = height;

    const ctx = canvas.getContext('2d');
    if (!ctx) return;
    
    ctx.lineCap = 'round';
    ctx.lineJoin = 'round';
  }, [width, height]);

  // Handle drawing a path
  const drawPath = useCallback((ctx: CanvasRenderingContext2D, path: DrawingPath) => {
    if (!path || !path.points || path.points.length < 2) return;

    ctx.beginPath();
    ctx.strokeStyle = path.color;
    ctx.lineWidth = path.width;
    
    // For line tool with just two points, draw a straight line
    if (path.tool === 'line' && path.points.length === 2) {
      const [start, end] = path.points;
      ctx.moveTo(start.x, start.y);
      ctx.lineTo(end.x, end.y);
    } else {
      // For freehand or legacy paths, draw all points
      ctx.moveTo(path.points[0].x, path.points[0].y);
      
      for (let i = 1; i < path.points.length; i++) {
        ctx.lineTo(path.points[i].x, path.points[i].y);
      }
    }
    
    ctx.stroke();
  }, []);

  // Draw annotations during replay
  useEffect(() => {
    // Only run this effect when replaying
    if (!isReplaying) return;
    
    console.log("REPLAY EFFECT RUNNING", {
      isReplaying,
      annotationsCount: replayAnnotations?.length || 0,
      globalTimePosition
    });
    
    const ctx = getContext();
    if (!ctx) {
      console.error("Failed to get canvas context for annotation replay");
      return;
    }
    
    // Clear canvas before drawing
    ctx.clearRect(0, 0, width, height);
    
    // Exit early if no annotations to draw
    if (!replayAnnotations || replayAnnotations.length === 0) {
      console.log("No annotations to replay");
      return;
    }
    
    // Always log the annotation data once to diagnose issues
    if (Math.random() < 0.1) {
      console.log("Annotation replay data:", 
        replayAnnotations.slice(0, 2).map(a => ({
          hasPoints: Boolean(a?.points),
          pointsLength: a?.points?.length,
          globalTimeOffset: (a as any).globalTimeOffset,
          timeOffset: (a as any).timeOffset,
          videoTime: a.videoTime,
          timestamp: a.timestamp
        }))
      );
    }
    
    // Also track video time for debugging
    const videoTimeMs = currentTime * 1000; // Convert to milliseconds
    
    // Log global timeline information periodically (every second)
    if (Math.floor(globalTimePosition / 1000) !== Math.floor((globalTimePosition - 100) / 1000)) {
      console.log(`Annotation replay: Global time: ${globalTimePosition}ms, Last clear: ${lastClearTime}ms, Annotations: ${replayAnnotations.length}`);
    }
    
    // First, filter annotations to only include those created after the last clear
    // This ensures "clear" actions are properly respected during replay
    const visibleAnnotations = replayAnnotations.filter(annotation => {
      if (!annotation || !annotation.points || annotation.points.length < 2) {
        return false; // Skip invalid annotations
      }
      
      // First, check if this annotation has globalTimeOffset and if it came after the last clear
      if ((annotation as any).globalTimeOffset !== undefined) {
        const globalTimeOffset = (annotation as any).globalTimeOffset;
        
        // Skip annotations that were drawn before the last clear
        if (globalTimeOffset <= lastClearTime) {
          return false;
        }
        
        // Now check if this annotation should be visible at the current timeline position
        const isVisible = globalTimeOffset <= globalTimePosition;
        
        if (Math.random() < 0.01) { // Reduce logging frequency
          console.log(`Annotation with globalTimeOffset: ${globalTimeOffset}ms at time: ${globalTimePosition}ms, lastClear: ${lastClearTime}ms`, {
            visible: isVisible,
            afterClear: globalTimeOffset > lastClearTime,
            videoTime: videoTimeMs
          });
        }
        
        return isVisible;
      }
      
      // Next check for explicit timeOffset (added by FeedbackOrchestrator)
      if ((annotation as any).timeOffset !== undefined) {
        const timeOffset = (annotation as any).timeOffset;
        
        // Skip annotations drawn before the last clear
        if (timeOffset <= lastClearTime) {
          return false;
        }
        
        const isVisible = timeOffset <= globalTimePosition;
        return isVisible;
      }
      
      // For legacy annotations without proper global timing, use video time
      // This is just a fallback for backward compatibility
      if (annotation.videoTime !== undefined) {
        return annotation.videoTime <= videoTimeMs;
      }
      
      // Last fallback to timestamp (original recording time)
      return annotation.timestamp <= videoTimeMs;
    });
    
    // Always log when we have visible annotations
    if (visibleAnnotations.length > 0) {
      console.log(`Showing ${visibleAnnotations.length} of ${replayAnnotations.length} annotations at global time ${globalTimePosition}ms`);
      
      // Draw each visible annotation
      visibleAnnotations.forEach(path => {
        console.log("Drawing path:", {
          points: path.points?.length,
          color: path.color,
          width: path.width,
          tool: path.tool || 'freehand'
        });
        
        // Ensure tool type is set for backwards compatibility
        if (!path.tool) {
          path.tool = 'freehand';
        }
        
        drawPath(ctx, path);
      });
    } else if (Math.random() < 0.1) {
      console.log(`No visible annotations at time ${globalTimePosition}ms (of ${replayAnnotations.length} total)`);
    }
  }, [isReplaying, replayAnnotations, currentTime, width, height, globalTimePosition, lastClearTime, getContext, drawPath]);

  // Draw all stored paths
  useEffect(() => {
    if (isReplaying) return; // Don't draw local paths during replay
    
    const ctx = getContext();
    if (!ctx) return;
    
    // Clear canvas before drawing
    ctx.clearRect(0, 0, width, height);
    
    // Draw all stored paths
    allDrawings.forEach(path => {
      drawPath(ctx, path);
    });
  }, [allDrawings, isReplaying, width, height]);

  // Method to handle an annotation that was generated programmatically
  const handleManualAnnotation = (path: DrawingPath) => {
    // Log additional timing information for debugging
    console.log('Handling manual annotation:', {
      path: path,
      points: path.points?.length || 0,
      videoTime: path.videoTime || 'not set',
      timestamp: path.timestamp || 'not set',
      timeOffset: (path as any).timeOffset || 'not set',
      currentVideoTime: currentTime * 1000,
      tool: path.tool || 'freehand' // Default to freehand if not specified
    });
    
    // Make sure tool type is set
    const completePath = {
      ...path,
      tool: path.tool || 'freehand'
    };
    
    // Add to local drawings - preserve the original path with all timing information
    setAllDrawings(prev => [...prev, completePath]);
    
    // Report the annotation if we're recording
    if (isRecording && onAnnotationAdded) {
      onAnnotationAdded(completePath);
    }
  };

  // Get mouse/touch position in canvas coordinates
  const getPointerPosition = (e: React.MouseEvent<HTMLCanvasElement> | React.TouchEvent<HTMLCanvasElement>) => {
    const canvas = canvasRef.current;
    if (!canvas) return null;

    const rect = canvas.getBoundingClientRect();
    let clientX: number, clientY: number;

    if ('touches' in e) {
      // Touch event
      clientX = e.touches[0].clientX;
      clientY = e.touches[0].clientY;
    } else {
      // Mouse event
      clientX = e.clientX;
      clientY = e.clientY;
    }

    return {
      x: clientX - rect.left,
      y: clientY - rect.top
    };
  };

  // Draw temporary straight line during line tool usage
  const drawTemporaryLine = (start: Point, end: Point) => {
    const ctx = getContext();
    if (!ctx) return;

    // Clear canvas before redrawing
    ctx.clearRect(0, 0, width, height);
    
    // Redraw all existing paths
    allDrawings.forEach(path => {
      drawPath(ctx, path);
    });
    
    // Draw the temporary line
    ctx.beginPath();
    ctx.strokeStyle = toolColor;
    ctx.lineWidth = toolWidth;
    ctx.moveTo(start.x, start.y);
    ctx.lineTo(end.x, end.y);
    ctx.stroke();
  };

  // Event handlers for drawing
  const startDrawing = (e: React.MouseEvent<HTMLCanvasElement> | React.TouchEvent<HTMLCanvasElement>) => {
    if (isReplaying) return; // Drawing is always enabled, only check for replay mode
    
    setIsDrawing(true);
    setCurrentPath([]);

    const position = getPointerPosition(e);
    if (!position) return;
    
    // For line tool, just save the start point
    if (toolType === 'line') {
      setStartPoint(position);
      setCurrentPath([position]);
    } else {
      // For freehand, start the path
      setCurrentPath([position]);
    }
  };

  const draw = (e: React.MouseEvent<HTMLCanvasElement> | React.TouchEvent<HTMLCanvasElement>) => {
    if (!isDrawing || isReplaying) return; // Drawing is always enabled
    
    if ('touches' in e) {
      // Prevent scrolling while drawing
      e.preventDefault();
    }

    const position = getPointerPosition(e);
    if (!position) return;
    
    if (toolType === 'line' && startPoint) {
      // For line tool, continuously update the preview without adding points
      drawTemporaryLine(startPoint, position);
      // Update current path to track the current end position
      setCurrentPath([startPoint, position]);
    } else {
      // For freehand, add point to path and draw incremental line segment
      setCurrentPath(prev => [...prev, position]);
      
      const ctx = getContext();
      if (ctx && currentPath.length > 0) {
        ctx.beginPath();
        ctx.strokeStyle = toolColor;
        ctx.lineWidth = toolWidth;
        
        const lastPoint = currentPath[currentPath.length - 1];
        ctx.moveTo(lastPoint.x, lastPoint.y);
        ctx.lineTo(position.x, position.y);
        ctx.stroke();
      }
    }
  };

  const endDrawing = () => {
    if (!isDrawing || isReplaying) return; // Drawing is always enabled
    
    setIsDrawing(false);
    
    if (toolType === 'line' && startPoint) {
      // For line tool, create a path with just the start and end points
      if (currentPath.length === 2) {
        const endPosition = currentPath[1];
        
        // Only create a line if the end position is different from the start
        if (endPosition.x !== startPoint.x || endPosition.y !== startPoint.y) {
          const newPath: DrawingPath = {
            points: [startPoint, endPosition],
            color: toolColor,
            width: toolWidth,
            timestamp: Date.now(),
            tool: 'line'
          };
          
          // Draw the final line
          const ctx = getContext();
          if (ctx) {
            ctx.beginPath();
            ctx.strokeStyle = newPath.color;
            ctx.lineWidth = newPath.width;
            ctx.moveTo(startPoint.x, startPoint.y);
            ctx.lineTo(endPosition.x, endPosition.y);
            ctx.stroke();
          }
          
          setAllDrawings(prev => [...prev, newPath]);
          
          // Report the annotation if we're recording
          if (isRecording && onAnnotationAdded) {
            onAnnotationAdded(newPath);
          }
          
          console.log('Created line from', startPoint, 'to', endPosition);
        }
      }
      setStartPoint(null);
    } else if (currentPath.length > 1) {
      // For freehand, create path with all points
      const newPath: DrawingPath = {
        points: [...currentPath],
        color: toolColor,
        width: toolWidth,
        timestamp: Date.now(),
        tool: 'freehand'
      };
      
      setAllDrawings(prev => [...prev, newPath]);
      
      // Report the annotation if we're recording
      if (isRecording && onAnnotationAdded) {
        onAnnotationAdded(newPath);
      }
    }
    
    setCurrentPath([]);
  };

  // Expose methods to parent component
  useImperativeHandle(ref, () => ({
    // Core canvas ref and state
    canvasRef,
    allDrawings,
    
    // Canvas utility methods
    getContext: () => getContext(),
    
    // Drawing manipulation methods
    clearCanvasDrawings: () => {
      console.log('AnnotationCanvas: Clearing all drawings');
      clearCanvasDrawings();
    },
    
    // New state-based reset method that bypasses the timeline completely
    resetCanvas: () => {
      console.log('AnnotationCanvas: Complete state-based canvas reset');
      
      // Use double requestAnimationFrame for reliable clearing
      requestAnimationFrame(() => {
        // Clear the physical canvas
        const ctx = getContext();
        if (ctx) {
          ctx.clearRect(0, 0, width, height);
        }
        
        // Reset all internal state
        setAllDrawings([]);
        setCurrentPath([]);
        setIsDrawing(false);
        
        // Force a second redraw in the next animation frame to ensure rendering
        requestAnimationFrame(() => {
          const ctx = getContext();
          if (ctx) {
            ctx.clearRect(0, 0, width, height);
            console.log('AnnotationCanvas: Second clearing pass complete');
          }
        });
      });
    },
    
    handleManualAnnotation: (path: DrawingPath) => {
      console.log('AnnotationCanvas: Handling manual annotation:', {
        pointsCount: path.points?.length || 0,
        color: path.color,
        width: path.width,
        videoTime: path.videoTime
      });
      handleManualAnnotation(path);
    }
  }));

  return (
    <canvas
      ref={canvasRef}
      className="absolute top-0 left-0 z-10 cursor-crosshair"
      width={width}
      height={height}
      onMouseDown={startDrawing}
      onMouseMove={draw}
      onMouseUp={endDrawing}
      onMouseLeave={endDrawing}
      onTouchStart={startDrawing}
      onTouchMove={draw}
      onTouchEnd={endDrawing}
    />
  );
});

AnnotationCanvas.displayName = 'AnnotationCanvas';

export default AnnotationCanvas;
|| END ||


|| START ./src/components/VideoPlayer.tsx ||

'use client';

import React, { useState, useRef, useEffect, useMemo } from 'react';
import { useTimeline, useLastClearTime } from '../contexts/TimelineContext';
import { useVideoSource, useVideoControls } from '../contexts/VideoContext';

// Add a utility function to check if a URL is a cross-origin URL
const isCrossOriginUrl = (url: string): boolean => {
  try {
    if (typeof window === 'undefined') return false;
    const urlObj = new URL(url, window.location.href);
    return urlObj.origin !== window.location.origin;
  } catch (e) {
    return false;
  }
};

// Define the types of events we want to record
export type ActionType = 'play' | 'pause' | 'seek' | 'playbackRate' | 'keyboardShortcut' | 'annotation' | 'audio';

// Define the structure of a recorded action
export interface RecordedAction {
  type: ActionType;
  timestamp: number; // Time in milliseconds since recording started
  videoTime: number; // Current time in the video
  details?: {
    [key: string]: any; // Additional details specific to the action
  };
}

import AnnotationCanvas, { DrawingPath, DrawingTool } from './AnnotationCanvas';

import { AudioChunk } from './AudioRecorder';

export interface FeedbackData {
  sessionId: string;
  videoId: string;
  actions: RecordedAction[];
  startTime: number;
  endTime?: number;
  annotations?: DrawingPath[];
  audioChunks?: AudioChunk[];
}

interface VideoPlayerProps {
  isRecording?: boolean;
  isReplaying?: boolean;
  onRecordAction?: (action: RecordedAction) => void;
  setVideoRef?: (ref: HTMLVideoElement | null) => void;
  replayAnnotations?: DrawingPath[];
  onAnnotationAdded?: (annotation: DrawingPath) => void;
  videoUrl?: string;
}

interface VideoPlayerImperativeHandle {
  video: HTMLVideoElement | null;
  annotationCanvas: any;
}

// Define the VideoPlayer component with proper memoization
const VideoPlayer = React.memo(React.forwardRef<VideoPlayerImperativeHandle, VideoPlayerProps>(({ 
  isRecording = false, 
  isReplaying = false,
  onRecordAction,
  setVideoRef,
  replayAnnotations = [],
  onAnnotationAdded,
  videoUrl = "https://commondatastorage.googleapis.com/gtv-videos-bucket/sample/BigBuckBunny.mp4"
}: VideoPlayerProps, ref) => {
  const [playing, setPlaying] = useState(false);
  const [currentTime, setCurrentTime] = useState(0);
  const [duration, setDuration] = useState(0);
  const [playbackRate, setPlaybackRate] = useState(1);
  const [isAnnotationEnabled, setIsAnnotationEnabled] = useState(true);
  const [annotationColor, setAnnotationColor] = useState('#ff0000'); // Default red
  const [annotationWidth, setAnnotationWidth] = useState(3);
  const [annotationTool, setAnnotationTool] = useState<DrawingTool>('freehand');
  const [videoDimensions, setVideoDimensions] = useState({ width: 0, height: 0 });
  const [shouldClearCanvas, setShouldClearCanvas] = useState(false);
  const [isLoading, setIsLoading] = useState(true);
  const [isVideoCached, setIsVideoCached] = useState(false);
  const [cachedVideoSrc, setCachedVideoSrc] = useState<string | null>(null);
  const [isLoadStarted, setIsLoadStarted] = useState(false);
  
  // Use contexts
  const { updatePosition } = useTimeline();
  const { updateClearTime } = useLastClearTime();
  
  const videoRef = useRef<HTMLVideoElement>(null);
  const videoContainerRef = useRef<HTMLDivElement>(null);
  const recordingStartTimeRef = useRef<number | null>(null);
  const annotationCanvasRef = useRef<any>(null);

  // Initialize recording start time when recording begins
  useEffect(() => {
    if (isRecording && !recordingStartTimeRef.current) {
      recordingStartTimeRef.current = Date.now();
    } else if (!isRecording) {
      recordingStartTimeRef.current = null;
    }
  }, [isRecording]);
  
  // Use the video context hooks with try/catch to handle case where context might not be available
  let effectiveUrl, isContextLoading, videoControls;
  try {
    const videoSource = useVideoSource();
    effectiveUrl = videoSource?.effectiveUrl;
    isContextLoading = videoSource?.isLoading;
    
    videoControls = useVideoControls();
  } catch (error) {
    console.warn('Video context not available:', error);
    effectiveUrl = null;
    isContextLoading = false;
    videoControls = null;
  }
  
  // Create a local video blob cache if needed (fallback if context isn't working)
  const localBlobCache = useRef<Record<string, string>>({});
  
  // Debug info about video URLs
  useEffect(() => {
    console.log('VideoPlayer URLs:', { 
      propUrl: videoUrl,
      effectiveUrl,
      cachedVideoSrc
    });
  }, [videoUrl, effectiveUrl, cachedVideoSrc]);
  
  // Directly handle loading videos, especially for cross-origin URLs
  useEffect(() => {
    // Skip if no URL or already started loading
    if (!videoUrl || isLoadStarted) return;
    
    console.log('Starting video loading process for:', videoUrl);
    setIsLoading(true);
    setIsLoadStarted(true);
    
    // Check if this is a cross-origin URL
    const crossOrigin = isCrossOriginUrl(videoUrl);
    
    if (crossOrigin) {
      console.log('Cross-origin video detected, using direct URL:', videoUrl);
      // For cross-origin videos, we don't attempt caching, just use directly
      setCachedVideoSrc(videoUrl);
      setIsLoading(false);
      setIsVideoCached(true);
    } else if (effectiveUrl) {
      // Use the effective URL from context if available
      console.log('Using effective URL from context:', effectiveUrl);
      setCachedVideoSrc(effectiveUrl);
      setIsLoading(false);
      setIsVideoCached(true);
    } else {
      // Default to original URL
      console.log('Using original video URL:', videoUrl);
      setCachedVideoSrc(videoUrl);
      setIsLoading(false);
    }
  }, [videoUrl, effectiveUrl, isLoadStarted]);
  
  // Consolidated video loading event handling
  useEffect(() => {
    if (!videoRef.current) return;
    
    // Create a single handler for all loading-related events
    const handleVideoLoad = () => {
      if (videoRef.current) {
        const readyState = videoRef.current.readyState;
        console.log('Video load event triggered - readyState:', readyState);
        
        // Set duration if available
        if (readyState >= 1 && videoRef.current.duration) {
          setDuration(videoRef.current.duration);
        }
        
        // Update loading state based on readiness
        if (readyState >= 2 && isLoading) {
          setIsLoading(false);
        }
        
        // Mark as cached once canplaythrough fires
        if (readyState >= 4 && !isVideoCached) {
          setIsVideoCached(true);
          
          // Store in local cache if using direct URL mode
          if (videoUrl && !cachedVideoSrc) {
            console.log('Video ready for playthrough, marking as cached:', videoUrl);
            // Use our local cache ref to store this information
            localBlobCache.current[videoUrl] = videoUrl;
          }
        }
      }
    };
    
    // Add listeners for all relevant events
    videoRef.current.addEventListener('loadedmetadata', handleVideoLoad);
    videoRef.current.addEventListener('loadeddata', handleVideoLoad);
    videoRef.current.addEventListener('canplay', handleVideoLoad);
    videoRef.current.addEventListener('canplaythrough', handleVideoLoad);
    videoRef.current.addEventListener('durationchange', handleVideoLoad);
    
    // Initial check in case events already fired
    if (videoRef.current.readyState >= 2) {
      handleVideoLoad();
    }
    
    return () => {
      if (videoRef.current) {
        videoRef.current.removeEventListener('loadedmetadata', handleVideoLoad);
        videoRef.current.removeEventListener('loadeddata', handleVideoLoad);
        videoRef.current.removeEventListener('canplay', handleVideoLoad);
        videoRef.current.removeEventListener('canplaythrough', handleVideoLoad);
        videoRef.current.removeEventListener('durationchange', handleVideoLoad);
      }
    };
  }, [videoRef.current, isLoading, isVideoCached, videoUrl, cachedVideoSrc]);
  
  // Pass video element reference to parent component
  useEffect(() => {
    if (setVideoRef && videoRef.current) {
      setVideoRef(videoRef.current);
    }
    
    return () => {
      if (setVideoRef) {
        setVideoRef(null);
      }
    };
  }, [setVideoRef, videoRef.current]);
  
  // Update video dimensions when video metadata is loaded
  useEffect(() => {
    const updateVideoDimensions = () => {
      if (videoRef.current && videoContainerRef.current) {
        const containerRect = videoContainerRef.current.getBoundingClientRect();
        setVideoDimensions({
          width: containerRect.width,
          height: containerRect.height
        });
      }
    };
    
    // Initial update
    if (videoRef.current) {
      if (videoRef.current.readyState >= 1) {
        updateVideoDimensions();
      } else {
        videoRef.current.addEventListener('loadedmetadata', updateVideoDimensions);
      }
    }
    
    // Update dimensions on window resize
    window.addEventListener('resize', updateVideoDimensions);
    
    return () => {
      window.removeEventListener('resize', updateVideoDimensions);
      if (videoRef.current) {
        videoRef.current.removeEventListener('loadedmetadata', updateVideoDimensions);
      }
    };
  }, [videoRef.current]);
  
  // Handle annotation being added
  const handleAnnotationAdded = (path: DrawingPath) => {
    // Calculate the global timeline offset
    const globalTimeOffset = isRecording && recordingStartTimeRef.current ? 
      Date.now() - recordingStartTimeRef.current : 0;
    
    // Update the annotation with both global time and video time
    const annotationWithTiming = {
      ...path,
      // Store both the original timestamp (relative to the recording start)
      videoTime: currentTime * 1000,
      // Add global timeline offset for proper replay synchronization
      globalTimeOffset: globalTimeOffset,
      // Ensure tool type is always included
      tool: path.tool || 'freehand'
    };
    
    // If recording, pass the annotation to the parent
    if (isRecording && onAnnotationAdded) {
      onAnnotationAdded(annotationWithTiming);
    }
    
    // Record the annotation action
    if (isRecording && recordingStartTimeRef.current && onRecordAction) {
      const action: RecordedAction = {
        type: 'annotation',
        timestamp: globalTimeOffset,
        videoTime: currentTime,
        details: { path: annotationWithTiming }
      };
      onRecordAction(action);
    }
  };
  
  // Toggle annotation mode - removed as drawing is always enabled
  
  // Clear annotations
  const clearAnnotations = () => {
    setShouldClearCanvas(true);
    
    // Record the clear action if recording
    if (isRecording && recordingStartTimeRef.current && onRecordAction) {
      // Calculate global timeline offset
      const globalTimeOffset = Date.now() - recordingStartTimeRef.current;
      
      console.log(`Recording canvas clear at global time ${globalTimeOffset}ms, video time ${currentTime}s`);
      
      // Update last clear time using context
      updateClearTime(globalTimeOffset);
      console.log(`Updated lastClearTime via context to ${globalTimeOffset}ms`);
      
      const action: RecordedAction = {
        type: 'annotation',
        timestamp: globalTimeOffset,
        videoTime: currentTime,
        details: { 
          action: 'clear', // Ensure consistent action name
          clear: true,
          globalTimeOffset: globalTimeOffset // Add global timeline information
        }
      };
      onRecordAction(action);
    }
  };
  
  // Handle canvas clear completion
  const handleClearComplete = () => {
    setShouldClearCanvas(false);
  };

  // Function to record an action if recording is enabled
  const recordAction = (type: ActionType, details?: {[key: string]: any}) => {
    if (isRecording && recordingStartTimeRef.current && onRecordAction) {
      // Calculate the global timeline offset
      const globalTimeOffset = Date.now() - recordingStartTimeRef.current;
      
      const action: RecordedAction = {
        type,
        timestamp: globalTimeOffset,
        videoTime: currentTime,
        // Add global timeline information to all actions
        details: {
          ...details,
          globalTimeOffset: globalTimeOffset
        }
      };
      
      console.log(`Recording ${type} action at global time ${globalTimeOffset}ms, video time ${currentTime}s`);
      onRecordAction(action);
    }
  };

  const togglePlay = () => {
    if (videoRef.current) {
      if (playing) {
        videoRef.current.pause();
        videoControls?.pause?.(); // Update context if method exists
        recordAction('pause');
      } else {
        videoRef.current.play();
        videoControls?.play?.(); // Update context if method exists
        recordAction('play');
      }
      setPlaying(!playing);
    }
  };

  const handleTimeUpdate = () => {
    if (videoRef.current) {
      // Update both component state and context to reflect the current video time
      const time = videoRef.current.currentTime;
      setCurrentTime(time);
      // Only call updateTime if it exists (using optional chaining)
      videoControls?.updateTime?.(time);
    }
  };

  const handleLoadedMetadata = () => {
    if (videoRef.current) {
      console.log('Video metadata loaded, duration:', videoRef.current.duration);
      const duration = videoRef.current.duration;
      setDuration(duration);
      // Only call updateDuration if it exists (using optional chaining)
      videoControls?.updateDuration?.(duration);
    }
  };
  
  // Add additional event handler for duration change
  const handleDurationChange = () => {
    if (videoRef.current) {
      console.log('Video duration changed:', videoRef.current.duration);
      const duration = videoRef.current.duration;
      setDuration(duration);
      // Only call updateDuration if it exists (using optional chaining)
      videoControls?.updateDuration?.(duration);
    }
  };

  const handleSeek = (e: React.ChangeEvent<HTMLInputElement>) => {
    const time = parseFloat(e.target.value);
    const result = seekToTime(time);
    if (result) {
      recordAction('seek', { from: result.previousTime, to: result.newTime });
    }
  };
  
  // Add a separate function for direct seeking when clicking on the slider
  const handleSliderClick = (e: React.MouseEvent<HTMLInputElement>) => {
    const element = e.target as HTMLInputElement;
    const rect = element.getBoundingClientRect();
    const offsetX = e.clientX - rect.left;
    const percentage = offsetX / rect.width;
    const time = percentage * duration;
    
    const result = seekToTime(time);
    if (result) {
      recordAction('seek', { from: result.previousTime, to: result.newTime });
    }
  };


  const handlePlaybackRateChange = (rate: number) => {
    if (videoRef.current) {
      const previousRate = videoRef.current.playbackRate;
      videoRef.current.playbackRate = rate;
      setPlaybackRate(rate);
      // Update context if the method exists
      videoControls?.setPlaybackRate?.(rate);
      recordAction('playbackRate', { from: previousRate, to: rate });
    }
  };

  const formatTime = (time: number) => {
    if (!time || isNaN(time) || time < 0) {
      return '0:00';
    }
    const minutes = Math.floor(time / 60);
    const seconds = Math.floor(time % 60);
    return `${minutes}:${seconds < 10 ? '0' : ''}${seconds}`;
  };

  // Helper function to seek to a specific time
  const seekToTime = (time: number) => {
    if (videoRef.current) {
      const previousTime = videoRef.current.currentTime;
      const newTime = Math.max(0, Math.min(duration, time));
      videoRef.current.currentTime = newTime;
      setCurrentTime(newTime);
      // Update the video context, only if the method exists
      videoControls?.seek?.(newTime);
      return { previousTime, newTime };
    }
    return null;
  };

  // Add keyboard shortcuts
  useEffect(() => {
    const handleKeyDown = (e: KeyboardEvent) => {
      if (e.key === ' ' || e.key === 'k') {
        togglePlay();
        recordAction('keyboardShortcut', { key: e.key, action: playing ? 'pause' : 'play' });
      } else if (e.key === 'ArrowLeft') {
        if (videoRef.current) {
          const result = seekToTime(videoRef.current.currentTime - 5);
          if (result) {
            recordAction('keyboardShortcut', { 
              key: e.key, 
              action: 'rewind',
              from: result.previousTime,
              to: result.newTime 
            });
          }
        }
      } else if (e.key === 'ArrowRight') {
        if (videoRef.current) {
          const result = seekToTime(videoRef.current.currentTime + 5);
          if (result) {
            recordAction('keyboardShortcut', { 
              key: e.key, 
              action: 'forward',
              from: result.previousTime,
              to: result.newTime
            });
          }
        }
      // 'm' shortcut removed
      }
    };

    window.addEventListener('keydown', handleKeyDown);
    return () => {
      window.removeEventListener('keydown', handleKeyDown);
    };
  }, [duration, playing, isRecording, currentTime]);

  // First useImperativeHandle is removed as it's duplicated below

  // Expose handlers for AnnotationCanvas 
  const handleManualAnnotation = (path: DrawingPath) => {
    if (annotationCanvasRef.current) {
      annotationCanvasRef.current.handleManualAnnotation(path);
    }
  };

  const clearAllAnnotations = () => {
    if (annotationCanvasRef.current) {
      annotationCanvasRef.current.clearCanvasDrawings();
    }
  };

  // Add methods to imperativeHandle 
  React.useImperativeHandle(ref, () => ({
    // Expose the video element
    video: videoRef.current,
    
    // Expose the annotation canvas and its methods
    annotationCanvas: annotationCanvasRef.current,
    
    // Expose annotation methods directly at the top level for easier access
    handleManualAnnotation: (path: DrawingPath) => {
      if (annotationCanvasRef.current) {
        console.log('VideoPlayer: Forwarding manual annotation to canvas');
        annotationCanvasRef.current.handleManualAnnotation(path);
        
        // If recording is active, also record this event
        if (isRecording && onRecordAction) {
          const action: RecordedAction = {
            type: 'annotation',
            timestamp: Date.now() - (recordingStartTimeRef.current || 0),
            videoTime: currentTime,
            details: { path }
          };
          onRecordAction(action);
        }
      } else {
        console.warn('VideoPlayer: Cannot forward annotation - canvas ref not available');
      }
    },
    
    clearAllAnnotations: () => {
      if (annotationCanvasRef.current) {
        console.log('VideoPlayer: Forwarding clear annotation to canvas');
        annotationCanvasRef.current.clearCanvasDrawings();
        
        // If recording is active, also record this event
        if (isRecording && onRecordAction) {
          // Calculate global timeline offset
          const globalTimeOffset = Date.now() - (recordingStartTimeRef.current || 0);
          
          console.log(`Recording canvas clear via clearAllAnnotations at global time ${globalTimeOffset}ms`);
          
          // Update last clear time using context
          updateClearTime(globalTimeOffset);
          console.log(`Updated lastClearTime via context to ${globalTimeOffset}ms`);
          
          const action: RecordedAction = {
            type: 'annotation',
            timestamp: globalTimeOffset,
            videoTime: currentTime,
            details: { 
              action: 'clear', // Ensure consistent action name
              clear: true,
              globalTimeOffset: globalTimeOffset
            }
          };
          onRecordAction(action);
        }
      } else {
        console.warn('VideoPlayer: Cannot clear annotations - canvas ref not available');
      }
    }
  }));

  return (
    <div className="flex flex-col w-full max-w-3xl bg-gray-100 rounded-lg shadow-md overflow-hidden">
      <div className="relative" ref={videoContainerRef}>
        {isRecording && (
          <div className="absolute top-2 right-2 z-20 flex items-center px-2 py-1 bg-red-500 text-white rounded-md text-sm">
            <span className="animate-pulse mr-1">●</span> Recording
          </div>
        )}
        {isLoading && (
          <div className="absolute inset-0 bg-black bg-opacity-60 flex flex-col items-center justify-center z-30">
            <div className="animate-spin rounded-full h-12 w-12 border-t-2 border-b-2 border-white mb-2"></div>
            <p className="text-white font-medium">Loading video...</p>
            <p className="text-white text-sm mt-1">{isVideoCached ? 'Video will be cached for future viewing' : 'Please wait while the video loads'}</p>
          </div>
        )}
        <video
          key={cachedVideoSrc || videoUrl} // Key forces remount when source changes
          ref={videoRef}
          className="w-full aspect-video"
          onTimeUpdate={handleTimeUpdate}
          onLoadedMetadata={handleLoadedMetadata}
          onDurationChange={handleDurationChange}
          src={cachedVideoSrc || videoUrl}
          playsInline
          preload="auto"
          muted
          // Use the local cachedVideoSrc which is set from context or direct URL
        />
        
        {videoDimensions.width > 0 && videoDimensions.height > 0 && (
          <AnnotationCanvas
            ref={annotationCanvasRef}
            width={videoDimensions.width}
            height={videoDimensions.height}
            isEnabled={isAnnotationEnabled && !isReplaying}
            currentTime={currentTime}
            isRecording={isRecording}
            isReplaying={isReplaying}
            onAnnotationAdded={handleAnnotationAdded}
            replayAnnotations={replayAnnotations}
            toolColor={annotationColor}
            toolWidth={annotationWidth}
            toolType={annotationTool}
            clearCanvas={shouldClearCanvas}
            onClearComplete={handleClearComplete}
          />
        )}
      </div>
      
      <div className="p-4 bg-white">
        <div className="flex items-center mb-2 relative">
          <input
            type="range"
            min="0"
            max={duration || 0}
            value={currentTime}
            onChange={handleSeek}
            onClick={handleSliderClick}
            className="w-full h-3 bg-gray-200 rounded-lg appearance-none cursor-pointer relative z-10
                      focus:outline-none focus:ring-2 focus:ring-blue-300
                      [&::-webkit-slider-thumb]:appearance-none
                      [&::-webkit-slider-thumb]:bg-blue-500
                      [&::-webkit-slider-thumb]:h-4
                      [&::-webkit-slider-thumb]:w-4
                      [&::-webkit-slider-thumb]:rounded-full
                      [&::-webkit-slider-thumb]:border-0
                      [&::-webkit-slider-thumb]:shadow
                      [&::-webkit-slider-thumb]:cursor-pointer"
            style={{
              background: `linear-gradient(to right, #3b82f6 0%, #3b82f6 ${(currentTime / (duration || 1)) * 100}%, #e5e7eb ${(currentTime / (duration || 1)) * 100}%, #e5e7eb 100%)`
            }}
          />
        </div>
        
        <div className="flex justify-between items-center mb-2">
          <div className="flex items-center space-x-2">
            <button
              onClick={togglePlay}
              className="p-2 rounded-full bg-gray-200 hover:bg-gray-300"
            >
              {playing ? 
                <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="currentColor" className="w-6 h-6">
                  <path fillRule="evenodd" d="M6.75 5.25a.75.75 0 0 1 .75-.75H9a.75.75 0 0 1 .75.75v13.5a.75.75 0 0 1-.75.75H7.5a.75.75 0 0 1-.75-.75V5.25Zm7.5 0A.75.75 0 0 1 15 4.5h1.5a.75.75 0 0 1 .75.75v13.5a.75.75 0 0 1-.75.75H15a.75.75 0 0 1-.75-.75V5.25Z" clipRule="evenodd" />
                </svg>
                :
                <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="currentColor" className="w-6 h-6">
                  <path fillRule="evenodd" d="M4.5 5.653c0-1.427 1.529-2.33 2.779-1.643l11.54 6.347c1.295.712 1.295 2.573 0 3.286l-11.54 6.347c-1.25.687-2.779-.217-2.779-1.643V5.653Z" clipRule="evenodd" />
                </svg>
              }
            </button>
            
            
            <span className="text-sm text-gray-600">
              {formatTime(currentTime)} / {duration ? formatTime(duration) : '0:00'}
            </span>
          </div>
          
          <div className="flex items-center space-x-2">
            <select 
              value={playbackRate}
              onChange={(e) => handlePlaybackRateChange(parseFloat(e.target.value))}
              className="bg-gray-200 text-sm rounded px-2 py-1"
            >
              <option value="0.5">0.5x</option>
              <option value="1">1x</option>
              <option value="1.5">1.5x</option>
              <option value="2">2x</option>
            </select>
          </div>
        </div>
        
        {/* Annotation controls */}
        <div className="flex flex-wrap items-center justify-between pt-2 border-t border-gray-200">
          <div className="flex items-center space-x-2">
            <div className="flex items-center space-x-1">
              <label className="text-xs text-gray-600">Tool:</label>
              <div className="flex bg-gray-100 rounded overflow-hidden border border-gray-300">
                <button
                  onClick={() => setAnnotationTool('freehand')}
                  className={`py-1 px-2 text-xs ${annotationTool === 'freehand' ? 'bg-blue-500 text-white' : 'bg-gray-100 text-gray-700'}`}
                >
                  Pen
                </button>
                <button
                  onClick={() => setAnnotationTool('line')}
                  className={`py-1 px-2 text-xs ${annotationTool === 'line' ? 'bg-blue-500 text-white' : 'bg-gray-100 text-gray-700'}`}
                >
                  Line
                </button>
              </div>
            </div>
            
            <div className="flex items-center space-x-1">
              <label className="text-xs text-gray-600">Color:</label>
              <select
                value={annotationColor}
                onChange={(e) => setAnnotationColor(e.target.value)}
                className="bg-gray-100 text-xs rounded p-1 border border-gray-300"
              >
                <option value="#ff0000">Red</option>
                <option value="#0000ff">Blue</option>
                <option value="#00ff00">Green</option>
                <option value="#ffff00">Yellow</option>
                <option value="#000000">Black</option>
                <option value="#ffffff">White</option>
              </select>
            </div>
            
            <div className="flex items-center space-x-1">
              <label className="text-xs text-gray-600">Width:</label>
              <select
                value={annotationWidth}
                onChange={(e) => setAnnotationWidth(parseInt(e.target.value))}
                className="bg-gray-100 text-xs rounded p-1 border border-gray-300"
              >
                <option value="1">Thin</option>
                <option value="3">Medium</option>
                <option value="5">Thick</option>
                <option value="8">Very Thick</option>
              </select>
            </div>
            
            <button
              onClick={clearAnnotations}
              className="py-1 px-3 text-xs bg-red-100 hover:bg-red-200 text-red-700 rounded-md transition-colors"
              disabled={isReplaying}
            >
              Clear
            </button>
          </div>
        </div>
      </div>
    </div>
  );
}));

// Add displayName for better debugging in React DevTools
VideoPlayer.displayName = 'VideoPlayer';

// Custom comparison function for memoization
const arePropsEqual = (prevProps: VideoPlayerProps, nextProps: VideoPlayerProps) => {
  // Only re-render if these props changed
  return (
    prevProps.videoUrl === nextProps.videoUrl &&
    prevProps.isRecording === nextProps.isRecording &&
    prevProps.isReplaying === nextProps.isReplaying &&
    // Simplistic comparison of replay annotations (optimally would do deep compare)
    prevProps.replayAnnotations === nextProps.replayAnnotations
  );
};

// Export with proper memo implementation
export default React.memo(VideoPlayer, arePropsEqual);
|| END ||


|| START ./src/components/AudioRecorder.tsx ||

'use client';

import React, { useState, useRef, useEffect, useCallback } from 'react';

export interface AudioChunk {
  blob: Blob | string;      // The audio data as Blob or string (for serialization)
  startTime: number;        // Relative to recording start
  duration: number;         // Length of audio chunk in ms
  videoTime: number;        // Video timestamp when this audio was recorded
  url?: string;             // URL for playback (created during replay)
  mimeType?: string;        // MIME type for proper playback
  blobUrl?: string;         // URL for the Azure Storage blob 
}

interface AudioRecorderProps {
  isRecording: boolean;
  isReplaying: boolean;
  currentVideoTime: number;
  onAudioChunk?: (chunk: AudioChunk) => void;
  replayAudioChunks?: AudioChunk[];
}

export default function AudioRecorder({
  isRecording,
  isReplaying,
  currentVideoTime,
  onAudioChunk,
  replayAudioChunks = []
}: AudioRecorderProps) {
  const [isRecordingAudio, setIsRecordingAudio] = useState(false);
  const [audioPermissionGranted, setAudioPermissionGranted] = useState(false);
  const [error, setError] = useState<string | null>(null);
  const [recordingFormat, setRecordingFormat] = useState('');
  const [elapsedTime, setElapsedTime] = useState(0);
  
  // Refs for managing audio recording state
  const mediaRecorderRef = useRef<MediaRecorder | null>(null);
  const streamRef = useRef<MediaStream | null>(null);
  const recordingStartTimeRef = useRef<number | null>(null);
  const chunksRef = useRef<Blob[]>([]);
  const timerRef = useRef<NodeJS.Timeout | null>(null);
  
  // Refs for managing audio playback during replay
  const audioPlayersRef = useRef<Map<number, HTMLAudioElement>>(new Map());
  const playingChunksRef = useRef<Set<number>>(new Set());
  
  // Check for audio permission when component mounts
  useEffect(() => {
    const checkAudioPermission = async () => {
      try {
        const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
        setAudioPermissionGranted(true);
        
        // Stop the tracks after permission check
        stream.getTracks().forEach(track => track.stop());
      } catch (err) {
        console.error('Error accessing microphone:', err);
        setError('Microphone access denied. Please enable microphone permissions in your browser.');
        setAudioPermissionGranted(false);
      }
    };
    
    checkAudioPermission();
  }, []);
  
  // Start or stop recording based on props
  useEffect(() => {
    if (isRecording && audioPermissionGranted && !isRecordingAudio) {
      // We intentionally set recordingStartTimeRef.current to null here
      // to ensure a clean start for each new recording session.
      // It will be properly set in startAudioRecording() before the recorder starts.
      recordingStartTimeRef.current = null;
      
      console.log('Starting audio recording because isRecording=true');
      startAudioRecording();
    } else if (!isRecording && isRecordingAudio) {
      console.log('Stopping audio recording because isRecording=false');
      stopAudioRecording();
    }
    
    // Clean up on unmount
    return () => {
      if (isRecordingAudio) {
        stopAudioRecording();
      }
      cleanupAudioPlayers();
    };
  }, [isRecording, audioPermissionGranted, isRecordingAudio]);
  
  // Start recording audio
  const startAudioRecording = async () => {
    try {
      // Reset state
      chunksRef.current = [];
      setElapsedTime(0);
      setError(null);
      
      // Request microphone access with quality settings
      const stream = await navigator.mediaDevices.getUserMedia({
        audio: {
          echoCancellation: true,
          noiseSuppression: true,
          autoGainControl: true,
          channelCount: 1, // Mono for voice clarity
          sampleRate: 48000 // Higher sample rate for better quality
        }
      });
      
      streamRef.current = stream;
      
      // Find the best supported audio format
      let mimeType = '';
      const formats = [
        'audio/webm;codecs=opus',
        'audio/webm',
        'audio/mp4;codecs=opus',
        'audio/mp4',
        'audio/ogg;codecs=opus',
        'audio/ogg',
        'audio/wav'
      ];
      
      for (const format of formats) {
        if (MediaRecorder.isTypeSupported(format)) {
          mimeType = format;
          break;
        }
      }
      
      setRecordingFormat(mimeType || 'default format');
      console.log('Using audio format:', mimeType || 'default');
      
      // Create recorder with quality settings
      const recorderOptions = {
        mimeType: mimeType || undefined,
        audioBitsPerSecond: 128000
      };
      
      const recorder = new MediaRecorder(stream, recorderOptions);
      mediaRecorderRef.current = recorder;
      
      // Handle data available event
      recorder.ondataavailable = (e) => {
        if (e.data && e.data.size > 0) {
          chunksRef.current.push(e.data);
        }
      };
      
      // Handle recording stop
      recorder.onstop = async () => {
        if (chunksRef.current.length === 0) {
          console.warn('No audio chunks recorded');
          return;
        }
        
        const audioBlob = new Blob(chunksRef.current, { type: mimeType || 'audio/webm' });
        
        // Check if recordingStartTimeRef is set, if not use current time as fallback
        if (!recordingStartTimeRef.current) {
          console.warn('Recording start time not set, using fallback current time');
          recordingStartTimeRef.current = Date.now() - (elapsedTime * 1000);
        }
        
        // Calculate the actual duration of the recording based on the current time
        const now = Date.now();
        const calculatedDuration = recordingStartTimeRef.current ? now - recordingStartTimeRef.current : 0;
        
        // Compare the timer-based duration with the calculated duration
        console.log('Duration calculation:', {
          timerBasedDuration: elapsedTime * 1000,
          calculatedDuration: calculatedDuration,
          usingCalculated: calculatedDuration > 0
        });
        
        // Use the calculated duration if it's greater than 0, otherwise fall back to the timer-based duration
        const finalDuration = calculatedDuration > 0 ? calculatedDuration : elapsedTime * 1000;
        
        // Only create and report the audio chunk if we have valid data
        if (audioBlob.size > 0 && recordingStartTimeRef.current && onAudioChunk) {
          try {
            // Create the audio chunk with all required properties
            const audioChunk: AudioChunk = {
              blob: audioBlob,
              startTime: recordingStartTimeRef.current,
              duration: finalDuration, // Use the calculated duration in ms
              videoTime: currentVideoTime * 1000, // Convert to ms
              mimeType: mimeType || 'audio/webm', // Store the MIME type explicitly
            };
            
            // Pass the chunk to the parent component
            onAudioChunk(audioChunk);
            
            console.log('Audio chunk recorded successfully:', {
              duration: `${(finalDuration/1000).toFixed(2)}s`,
              size: `${Math.round(audioBlob.size / 1024)} KB`,
              format: mimeType || 'audio/webm',
              videoPosition: `${currentVideoTime.toFixed(2)}s`,
              audioStartTime: new Date(recordingStartTimeRef.current).toLocaleTimeString(),
              calculatedDuration: `${(calculatedDuration/1000).toFixed(2)}s`,
              timerDuration: `${elapsedTime}s`
            });
          } catch (error) {
            console.error('Error processing audio chunk:', error);
            setError(`Failed to process audio recording: ${error instanceof Error ? error.message : String(error)}`);
          }
        } else {
          console.warn('No valid audio data to save', {
            chunks: chunksRef.current.length,
            blobSize: audioBlob.size,
            hasStartTime: !!recordingStartTimeRef.current
          });
        }
        
        // Stop all tracks to release the microphone
        if (streamRef.current) {
          streamRef.current.getTracks().forEach(track => track.stop());
          streamRef.current = null;
        }
        
        // Clear the timer
        if (timerRef.current) {
          clearInterval(timerRef.current);
          timerRef.current = null;
        }
        
        setIsRecordingAudio(false);
      };
      
      // Start the recorder and explicitly set the recording start time
      // CRITICAL: This is where we set the recordingStartTimeRef that will be used
      // to calculate audio chunk duration and is essential for proper audio recording
      const startTime = Date.now();
      recordingStartTimeRef.current = startTime;
      
      console.log('Setting recording start time:', startTime, 'at', new Date(startTime).toLocaleTimeString());
      
      // Start the recorder AFTER setting the start time reference to ensure proper timing
      recorder.start();
      setIsRecordingAudio(true);
      
      // Double-check that the start time was set correctly
      if (!recordingStartTimeRef.current) {
        console.warn('Warning: recordingStartTimeRef.current is null after setting it!');
        // Emergency fallback - set it again
        recordingStartTimeRef.current = Date.now();
        console.log('Emergency reset of recordingStartTimeRef to:', recordingStartTimeRef.current);
      }
      
      // Update elapsed time every second
      timerRef.current = setInterval(() => {
        setElapsedTime(prev => prev + 1);
      }, 1000);
      
    } catch (error) {
      console.error('Error starting recording:', error);
      setError(`Could not start recording: ${error instanceof Error ? error.message : String(error)}`);
    }
  };
  
  // Stop recording
  const stopAudioRecording = () => {
    // First, capture the final duration info before stopping
    // This helps ensure we have valid timing information for the recording
    const recordingEndTime = Date.now();
    const finalElapsedTimeMs = elapsedTime * 1000; // Convert seconds to ms
    
    // If recordingStartTimeRef is missing, try to reconstruct it from elapsed time
    if (!recordingStartTimeRef.current) {
      console.warn('stopAudioRecording: recordingStartTimeRef.current is null, calculating from current time');
      recordingStartTimeRef.current = recordingEndTime - finalElapsedTimeMs;
      console.log('Reconstructed recordingStartTimeRef.current:', recordingStartTimeRef.current);
    }
    
    // Calculate duration for logging
    const calculatedDuration = recordingStartTimeRef.current 
      ? recordingEndTime - recordingStartTimeRef.current 
      : 0;
      
    console.log('Stopping audio recording with duration info:', {
      timerDuration: `${finalElapsedTimeMs}ms`,
      calculatedDuration: `${calculatedDuration}ms`,
      startTime: recordingStartTimeRef.current 
        ? new Date(recordingStartTimeRef.current).toLocaleTimeString() 
        : 'unknown',
      endTime: new Date(recordingEndTime).toLocaleTimeString()
    });
    
    // Now stop the media recorder
    if (mediaRecorderRef.current && mediaRecorderRef.current.state !== 'inactive') {
      try {
        mediaRecorderRef.current.stop();
      } catch (e) {
        console.error('Error stopping media recorder:', e);
      }
    }
    
    // Ensure stream is stopped
    if (streamRef.current) {
      streamRef.current.getTracks().forEach(track => track.stop());
      streamRef.current = null;
    }
    
    // Clear timer
    if (timerRef.current) {
      clearInterval(timerRef.current);
      timerRef.current = null;
    }
    
    setIsRecordingAudio(false);
    // We've already handled any null recordingStartTimeRef, now we can clear it
    recordingStartTimeRef.current = null;
  };
  
  // Format time as MM:SS
  const formatTime = (seconds: number) => {
    const mins = Math.floor(seconds / 60);
    const secs = seconds % 60;
    return `${mins}:${secs < 10 ? '0' : ''}${secs}`;
  };
  
  // Enhanced helper function to convert data URL to Blob
  const dataURLToBlob = (dataURL: string): Blob => {
    try {
      // More comprehensive validation of data URL format
      if (!dataURL || typeof dataURL !== 'string') {
        console.error('Invalid data URL: not a string or empty', typeof dataURL);
        throw new Error('Invalid data URL: not a string or empty');
      }
      
      if (!dataURL.startsWith('data:')) {
        console.error('Invalid data URL format - missing data: prefix');
        console.debug('URL starts with:', dataURL.substring(0, Math.min(20, dataURL.length)));
        throw new Error('Invalid data URL format - missing data: prefix');
      }
      
      // Split the data URL into parts - header and payload
      const parts = dataURL.split(',');
      if (parts.length !== 2) {
        console.error('Invalid data URL format - wrong number of parts:', parts.length);
        throw new Error('Invalid data URL format - wrong number of parts');
      }
      
      // Extract the MIME type with better validation
      const headerPart = parts[0];
      let mime = 'audio/webm'; // Default fallback
      
      // More robust MIME type extraction
      const mimeMatch = headerPart.match(/^data:(.*?)(;base64)?$/);
      if (mimeMatch && mimeMatch[1]) {
        mime = mimeMatch[1];
      } else {
        console.warn('Could not extract MIME type from data URL, using default:', mime);
      }
      
      // Verify that we have a base64 encoded payload
      if (!headerPart.includes(';base64')) {
        console.warn('Data URL does not specify base64 encoding, may cause issues');
      }
      
      // Get base64 data
      const base64Data = parts[1];
      if (!base64Data) {
        console.error('Empty base64 data in data URL');
        throw new Error('Empty base64 data in data URL');
      }
      
      try {
        // Convert base64 to binary with error handling
        const binary = atob(base64Data);
        
        // Create array buffer with proper size validation
        const arrayBuffer = new ArrayBuffer(binary.length);
        const uint8Array = new Uint8Array(arrayBuffer);
        
        // Fill array buffer with binary data
        for (let i = 0; i < binary.length; i++) {
          uint8Array[i] = binary.charCodeAt(i);
        }
        
        // Create and return the blob
        const blob = new Blob([uint8Array], { type: mime });
        
        // Validate the created blob
        if (blob.size === 0) {
          console.warn('Created an empty blob from data URL, possible data corruption');
        } else {
          console.log(`Successfully converted data URL to Blob: size=${blob.size}, type=${blob.type}`);
        }
        
        return blob;
      } catch (binaryError) {
        console.error('Error processing binary data:', binaryError);
        throw new Error(`Failed to process binary data: ${binaryError instanceof Error ? binaryError.message : String(binaryError)}`);
      }
    } catch (error) {
      console.error('Error converting data URL to Blob:', error);
      // Return an empty blob instead of throwing to prevent UI failures
      return new Blob([], { type: 'audio/webm' });
    }
  };

  // Clean up audio players
  const cleanupAudioPlayers = () => {
    audioPlayersRef.current.forEach((player, key) => {
      try {
        player.pause();
        if (player.src && player.src.startsWith('blob:')) {
          URL.revokeObjectURL(player.src);
        }
      } catch (e) {
        console.warn('Error cleaning up audio player:', e);
      }
    });
    
    audioPlayersRef.current.clear();
    playingChunksRef.current.clear();
  };
  
  // Handle audio playback during replay
  useEffect(() => {
    if (!isReplaying) {
      // Clean up audio players when not replaying
      cleanupAudioPlayers();
      return;
    }
    
    // Current video time in milliseconds
    const videoTimeMs = currentVideoTime * 1000;
    
    // Process each chunk to determine if it should play
    replayAudioChunks.forEach((chunk, index) => {
      const chunkId = chunk.startTime; // Use startTime as unique ID
      
      // Log more details about the chunk the first time we see it during replay
      if (replayAudioChunks.length > 0 && index === 0) {
        console.log(`Audio replay details (${replayAudioChunks.length} chunks):`, {
          currentVideoTimeMs: videoTimeMs,
          firstChunk: {
            startTime: new Date(chunk.startTime).toLocaleTimeString(),
            durationMs: chunk.duration,
            videoTimeMs: chunk.videoTime,
            blobType: chunk.blob instanceof Blob ? 'Blob' : typeof chunk.blob,
            blobLength: typeof chunk.blob === 'string' ? chunk.blob.length : (chunk.blob instanceof Blob ? chunk.blob.size : 'unknown'),
            mimeType: chunk.mimeType || 'not specified'
          }
        });
      }
      
      // Check if this chunk should be playing based on video time
      const shouldPlay = 
        videoTimeMs >= chunk.videoTime && 
        videoTimeMs <= chunk.videoTime + chunk.duration + 500; // Add 500ms buffer
      
      // Is this chunk already playing?
      const isPlaying = playingChunksRef.current.has(chunkId);
      
      // Log playback state changes for debugging
      if (shouldPlay !== isPlaying) {
        console.log(`Chunk ${index} playback state changing:`, {
          chunkId,
          shouldPlay,
          isPlaying,
          videoTimeMs,
          chunkVideoTime: chunk.videoTime,
          chunkEndTime: chunk.videoTime + chunk.duration,
          duration: chunk.duration,
          diff: videoTimeMs - chunk.videoTime
        });
      }
      
      // If should play but not playing yet
      if (shouldPlay && !isPlaying) {
        // Create audio element if one doesn't exist for this chunk
        if (!audioPlayersRef.current.has(chunkId)) {
          try {
            // Handle blob data in different formats
            let audioUrl: string;
            
            try {
              // Check for Azure Storage blob URL first
              if (chunk.blobUrl) {
                // Use the Azure Storage blob URL if available
                audioUrl = chunk.blobUrl;
                console.log(`Chunk ${chunkId}: Using Azure Storage blob URL for playback: ${chunk.blobUrl}`);
                
                // We'll handle the CORS issue by using Audio element directly
                // instead of trying to fetch the blob first
              }
              // Fall back to local URLs if no Azure Storage blob URL is available
              else if (chunk.url) {
                // Use provided URL if available
                audioUrl = chunk.url;
                console.log(`Chunk ${chunkId}: Using provided URL for playback`);
              } else if (chunk.blob instanceof Blob) {
                // Create URL from Blob
                audioUrl = URL.createObjectURL(chunk.blob);
                console.log(`Chunk ${chunkId}: Created URL from Blob object for audio playback:`, {
                  blobSize: chunk.blob.size,
                  blobType: chunk.blob.type
                });
              } else if (typeof chunk.blob === 'string' && chunk.blob.startsWith('data:')) {
                // Data URL - can either use directly or convert to blob first
                console.log(`Chunk ${chunkId}: Processing data URL for playback, length: ${chunk.blob.length}`);
                
                // Option 1: Convert data URL to blob first (more reliable across browsers)
                const convertedBlob = dataURLToBlob(chunk.blob);
                audioUrl = URL.createObjectURL(convertedBlob);
                console.log(`Chunk ${chunkId}: Converted data URL to Blob URL for playback:`, {
                  blobSize: convertedBlob.size,
                  blobType: convertedBlob.type
                });
                
                // Option 2 (alternative): Use data URL directly
                // audioUrl = chunk.blob;
                // console.log(`Chunk ${chunkId}: Using data URL directly for playback`);
              } else {
                console.error(`Chunk ${chunkId}: Invalid audio blob format:`, typeof chunk.blob);
                if (typeof chunk.blob === 'string') {
                  console.error(`Chunk ${chunkId}: String blob does not start with 'data:' - first 30 chars:`, 
                    chunk.blob.substring(0, 30));
                }
                return;
              }
            } catch (formatError) {
              console.error(`Chunk ${chunkId}: Error processing audio format:`, formatError);
              return;
            }
            
            // Create and configure audio element
            const audio = new Audio(audioUrl);
            
            // Enhanced error and event handling for debugging
            audio.onloadedmetadata = () => {
              console.log(`Chunk ${chunkId}: Audio metadata loaded:`, {
                duration: audio.duration,
                readyState: audio.readyState,
                src: audio.src.substring(0, 50) + '...' // Log truncated src for debugging
              });
            };
            
            audio.oncanplay = () => {
              console.log(`Chunk ${chunkId}: Audio can play now, ready state:`, audio.readyState);
            };
            
            audio.onplay = () => {
              console.log(`Chunk ${chunkId}: Audio playback started`);
            };
            
            audio.onended = () => {
              console.log(`Chunk ${chunkId}: Audio playback ended normally, duration:`, audio.duration);
              playingChunksRef.current.delete(chunkId);
              // Only revoke if we created the URL (not for Azure Storage URLs, data URLs, or provided URLs)
              if (!chunk.url && !chunk.blobUrl && chunk.blob instanceof Blob) {
                console.log(`Chunk ${chunkId}: Revoking local blob URL:`, audioUrl);
                URL.revokeObjectURL(audioUrl);
              }
            };
            
            audio.onerror = () => {
              const errorDetails = {
                errorCode: audio.error?.code,
                errorMessage: audio.error?.message,
                audioSrc: audio.src.substring(0, 50) + '...',
                audioReadyState: audio.readyState,
                chunkDetails: {
                  blobType: chunk.blob instanceof Blob ? 'Blob' : typeof chunk.blob,
                  mimeType: chunk.mimeType,
                  duration: chunk.duration,
                  hasBlobUrl: !!chunk.blobUrl,
                  blobUrlType: chunk.blobUrl ? (chunk.blobUrl.startsWith('/api') ? 'proxy' : 'direct') : 'none'
                }
              };
              
              console.error(`Chunk ${chunkId}: Error playing audio:`, errorDetails);
              setError(`Audio playback error: ${audio.error?.message || 'Unknown error'}`);
              playingChunksRef.current.delete(chunkId);
              // Only revoke if we created the URL (not for Azure Storage URLs, data URLs, or provided URLs)
              if (!chunk.url && !chunk.blobUrl && chunk.blob instanceof Blob) {
                console.log(`Chunk ${chunkId}: Revoking local blob URL on error:`, audioUrl);
                URL.revokeObjectURL(audioUrl);
              }
            };
            
            // Store the audio element
            audioPlayersRef.current.set(chunkId, audio);
          } catch (e) {
            console.error('Error creating audio player:', e);
            return;
          }
        }
        
        // Get the audio element
        const audio = audioPlayersRef.current.get(chunkId);
        if (!audio) return;
        
        // Calculate offset in the audio if needed
        const audioOffset = Math.max(0, (videoTimeMs - chunk.videoTime) / 1000);
        if (audioOffset > 0 && audioOffset < chunk.duration / 1000) {
          audio.currentTime = audioOffset;
        }
        
        // Play the audio
        const playPromise = audio.play();
        if (playPromise) {
          playPromise.catch(error => {
            if (error.name === 'NotAllowedError') {
              setError('Audio playback requires user interaction. Click anywhere to enable audio.');
              
              // Set up a one-time click handler to try again
              const clickHandler = () => {
                audio.play().catch(e => console.warn('Still could not play audio after interaction:', e));
                document.removeEventListener('click', clickHandler);
                setError(null);
              };
              document.addEventListener('click', clickHandler, { once: true });
            } else {
              console.error('Error playing audio:', error);
            }
          });
        }
        
        // Mark as playing
        playingChunksRef.current.add(chunkId);
      } 
      // If should not play but is currently playing
      else if (!shouldPlay && isPlaying) {
        const audio = audioPlayersRef.current.get(chunkId);
        if (audio) {
          audio.pause();
          playingChunksRef.current.delete(chunkId);
        }
      }
    });
    
    // Clean up on unmount or when replay status changes
    return () => {
      cleanupAudioPlayers();
    };
  }, [isReplaying, currentVideoTime, replayAudioChunks]);
  
  return (
    <div className="mb-4">
      {error && (
        <div className="text-red-500 text-sm mb-2 p-2 bg-red-50 border border-red-200 rounded">
          {error}
        </div>
      )}
      
      <div className={`flex items-center ${isRecordingAudio ? 'text-red-500' : 'text-gray-500'}`}>
        {isRecordingAudio && (
          <span className="inline-block h-3 w-3 bg-red-500 rounded-full animate-pulse mr-2"></span>
        )}
        <span className="text-sm font-medium">
          {isRecordingAudio 
            ? `Recording audio: ${formatTime(elapsedTime)}` 
            : isRecording 
              ? 'Microphone ready' 
              : 'Audio recording ready'}
        </span>
        {isRecordingAudio && (
          <span className="ml-2 text-xs bg-red-100 text-red-800 px-2 py-1 rounded-full">
            Live
          </span>
        )}
      </div>
      
      {!audioPermissionGranted && (
        <div className="mt-2 text-sm text-amber-600 bg-amber-50 p-2 rounded flex items-center">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="currentColor" className="w-5 h-5 mr-1">
            <path fillRule="evenodd" d="M9.401 3.003c1.155-2 4.043-2 5.197 0l7.355 12.748c1.154 2-.29 4.5-2.599 4.5H4.645c-2.309 0-3.752-2.5-2.598-4.5L9.4 3.003ZM12 8.25a.75.75 0 0 1 .75.75v3.75a.75.75 0 0 1-1.5 0V9a.75.75 0 0 1 .75-.75Zm0 8.25a.75.75 0 1 0 0-1.5.75.75 0 0 0 0 1.5Z" clipRule="evenodd" />
          </svg>
          <span>Microphone access is required for audio recording. Please allow microphone permissions.</span>
        </div>
      )}
      
      {isReplaying && replayAudioChunks && replayAudioChunks.length > 0 && (
        <div className="mt-2 text-xs text-blue-700 bg-blue-50 p-2 rounded">
          <div className="flex items-center">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="currentColor" className="w-5 h-5 mr-1">
              <path d="M13.5 4.06c0-1.336-1.616-2.005-2.56-1.06l-4.5 4.5H4.508c-1.141 0-2.318.664-2.66 1.905A9.76 9.76 0 0 0 1 15c0 1.614.332 3.151.927 4.55.35 1.256 1.518 1.95 2.661 1.95h1.93l4.5 4.5c.945.945 2.561.276 2.561-1.06V4.06ZM18.584 5.106a.75.75 0 0 1 1.06 0c3.808 3.807 3.808 9.98 0 13.788a.75.75 0 0 1-1.06-1.06 8.25 8.25 0 0 0 0-11.668.75.75 0 0 1 0-1.06Z" />
            </svg>
            <span>
              Replaying {replayAudioChunks.length} audio segment{replayAudioChunks.length !== 1 ? 's' : ''}.
              Audio will play automatically at the correct timestamps.
            </span>
          </div>
        </div>
      )}
      
      {isReplaying && (!replayAudioChunks || replayAudioChunks.length === 0) && (
        <div className="mt-2 text-xs text-gray-600 bg-gray-100 p-2 rounded flex items-center">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="currentColor" className="w-5 h-5 mr-1 text-gray-500">
            <path fillRule="evenodd" d="M1.5 4.5a3 3 0 0 1 3-3h1.372c.86 0 1.61.586 1.819 1.42l1.105 4.423a1.875 1.875 0 0 1-.694 1.955l-1.293.97c-.135.101-.164.249-.126.352a11.285 11.285 0 0 0 6.697 6.697c.103.038.25.009.352-.126l.97-1.293a1.875 1.875 0 0 1 1.955-.694l4.423 1.105c.834.209 1.42.959 1.42 1.82V19.5a3 3 0 0 1-3 3h-2.25C8.552 22.5 1.5 15.448 1.5 6.75V4.5Z" clipRule="evenodd" />
          </svg>
          <span>No audio recordings to replay.</span>
        </div>
      )}
      
      <div className="mt-3 flex flex-wrap gap-2">
        {isRecordingAudio && (
          <div className="px-3 py-1 bg-red-100 text-red-800 text-xs rounded-md flex items-center">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="currentColor" className="w-4 h-4 mr-1">
              <path d="M8.25 4.5a3.75 3.75 0 1 1 7.5 0v8.25a3.75 3.75 0 1 1-7.5 0V4.5Z" />
              <path d="M6 10.5a.75.75 0 0 1 .75.75v1.5a5.25 5.25 0 1 0 10.5 0v-1.5a.75.75 0 0 1 1.5 0v1.5a6.751 6.751 0 0 1-6 6.709v2.291h3a.75.75 0 0 1 0 1.5h-7.5a.75.75 0 0 1 0-1.5h3v-2.291a6.751 6.751 0 0 1-6-6.709v-1.5A.75.75 0 0 1 6 10.5Z" />
            </svg>
            <span>Speak to add verbal feedback</span>
          </div>
        )}
        
        {isRecordingAudio && (
          <div className="text-xs text-gray-500 mt-1">
            Recording format: {recordingFormat}
          </div>
        )}
      </div>
    </div>
  );
}
|| END ||


|| START ./src/components/FeedbackOrchestrator.tsx ||

'use client';

import { useState, useRef, useEffect, useCallback, forwardRef, useImperativeHandle } from 'react';
import { useTimeline, useLastClearTime } from '../contexts/TimelineContext';
import type { AudioChunk } from './AudioRecorder';
import type { DrawingPath } from './AnnotationCanvas';
import type { RecordedAction } from './VideoPlayer';

/**
 * Main feedback session structure
 */
export interface FeedbackSession {
  id: string;
  videoId: string;
  startTime: number;
  endTime?: number;
  audioTrack: AudioTrack;
  events: TimelineEvent[];
  categories?: Record<string, boolean>;
}

/**
 * Audio track containing all audio recording data
 */
export interface AudioTrack {
  chunks: AudioChunk[];
  totalDuration: number;
}

/**
 * Timeline event - all synchronized to audio timeline
 */
export interface TimelineEvent {
  id: string;
  type: 'video' | 'annotation' | 'marker' | 'category';
  timeOffset: number; // milliseconds from audio start
  duration?: number; // for events with duration
  payload: any; // specific data based on type
  priority?: number; // priority level for sorting when timestamps match
}

/**
 * Props for the FeedbackOrchestrator component
 */
interface FeedbackOrchestratorProps {
  // Video component ref
  videoElementRef: React.RefObject<HTMLVideoElement>;
  // Annotation canvas component ref and methods
  canvasRef: React.RefObject<any>;
  drawAnnotation: (path: DrawingPath) => void;
  clearAnnotations: () => void;
  // Audio recording methods and callbacks
  onAudioRecorded: (audioTrack: AudioTrack) => void;
  // Session management callbacks
  onSessionComplete: (session: FeedbackSession) => void;
  // Optional initial session for replay
  initialSession?: FeedbackSession | null;
  // Operation mode
  mode: 'record' | 'replay';
  // Callback for when categories are loaded during replay
  onCategoriesLoaded?: (categories: Record<string, boolean>) => void;
}

/**
 * Feedback Orchestrator Component
 * Coordinates all aspects of recording and playback for a feedback session
 */
const FeedbackOrchestrator = forwardRef<any, FeedbackOrchestratorProps>(({
  videoElementRef,
  canvasRef,
  drawAnnotation,
  clearAnnotations,
  onAudioRecorded,
  onSessionComplete,
  initialSession,
  mode,
  onCategoriesLoaded
}, ref) => {
  // State for tracking active session
  const [isActive, setIsActive] = useState(false);
  const [currentSession, setCurrentSession] = useState<FeedbackSession | null>(initialSession || null);
  const [replayProgress, setReplayProgress] = useState(0);
  const [audioPlayer, setAudioPlayer] = useState<HTMLAudioElement | null>(null);
  
  // Use timeline context
  const { updatePosition, resetTimelinePosition } = useTimeline();
  const { updateClearTime } = useLastClearTime();
  
  // Refs for tracking internal state
  const recordingStartTimeRef = useRef<number | null>(null);
  const audioChunksRef = useRef<AudioChunk[]>([]);
  const eventsRef = useRef<TimelineEvent[]>([]);
  const audioRecorderRef = useRef<MediaRecorder | null>(null);
  const streamRef = useRef<MediaStream | null>(null);
  const pendingEventsRef = useRef<TimelineEvent[]>([]);
  const replayTimeoutIdsRef = useRef<number[]>([]);
  
  // Create a stable event execution function using useState first
  // This ensures it's defined before other functions that depend on it
  const [executeEvent] = useState(() => (event: TimelineEvent) => {
    console.log(`Executing ${event.type} event:`, event.payload);
    
    switch (event.type) {
      case 'video':
        if (videoElementRef.current) {
          const video = videoElementRef.current;
          const payload = event.payload;
          
          // Log the global timeline position for this video event
          console.log(`Executing video ${payload.action} at global time ${event.timeOffset}ms`, {
            videoCurrentTime: video.currentTime,
            eventDetails: payload
          });
          
          switch (payload.action) {
            case 'play':
              video.play().catch(err => console.warn('Failed to play video:', err));
              break;
            case 'pause':
              video.pause();
              break;
            case 'seek':
              if (payload.to !== undefined) {
                // Seek to the target time in the video
                const prevTime = video.currentTime;
                video.currentTime = payload.to;
                console.log(`Replayed seek: ${prevTime.toFixed(2)}s → ${payload.to.toFixed(2)}s (at global time ${event.timeOffset}ms)`);
              }
              break;
            case 'playbackRate':
              if (payload.to !== undefined) {
                const prevRate = video.playbackRate;
                video.playbackRate = payload.to;
                console.log(`Replayed rate change: ${prevRate}x → ${payload.to}x (at global time ${event.timeOffset}ms)`);
              }
              break;
            // Handle keyboard shortcuts too
            case 'keyboardShortcut':
              if (payload.action === 'forward' && payload.to !== undefined) {
                video.currentTime = payload.to;
                console.log(`Replayed forward: to ${payload.to.toFixed(2)}s (at global time ${event.timeOffset}ms)`);
              } else if (payload.action === 'rewind' && payload.to !== undefined) {
                video.currentTime = payload.to;
                console.log(`Replayed rewind: to ${payload.to.toFixed(2)}s (at global time ${event.timeOffset}ms)`);
              } else if (payload.action === 'play') {
                video.play().catch(err => console.warn('Failed to play video:', err));
              } else if (payload.action === 'pause') {
                video.pause();
              }
              break;
          }
        }
        break;
      case 'annotation':
        if (drawAnnotation && clearAnnotations) {
          const payload = event.payload;
          
          switch (payload.action) {
            case 'draw':
              if (payload.path) {
                try {
                  // Create a copy of the path to preserve original properties
                  const pathWithTiming = { 
                    ...payload.path,
                    // Ensure the timeOffset from the event is used for replay timing
                    // This ensures the annotation appears at the correct time during replay
                    timeOffset: event.timeOffset,
                    // Always prefer the global timeline (event.timeOffset) for synchronization
                    // This ensures annotations are synchronized to the global timeline, not video time
                    globalTimeOffset: event.timeOffset,
                    // Keep videoTime for backward compatibility
                    videoTime: event.timeOffset
                  };
                  
                  console.log(`Executing drawing at global time ${event.timeOffset}ms`);
                  drawAnnotation(pathWithTiming);
                } catch (error) {
                  console.error('Error during annotation drawing:', error);
                }
              }
              break;
            case 'clear':
              try {
                console.log(`Executing canvas clear at global time ${event.timeOffset}ms`);
                // Record the global time when the clear happened in context
                updateClearTime(event.timeOffset);
                clearAnnotations();
              } catch (error) {
                console.error('Error during annotation clearing:', error);
              }
              break;
          }
        }
        break;
      case 'marker':
        // Could display a marker UI
        console.log('Marker:', event.payload.text);
        break;
      case 'category':
        // We're now handling all categories at the start of replay instead of during timeline events
        console.log(`Processing category event during replay: ${event.payload?.category} = ${event.payload?.rating}`);
        break;
    }
  });
  
  /**
   * Assign priorities to events based on their type
   */
  const assignEventPriority = useCallback((event: TimelineEvent): number => {
    if (event.priority !== undefined) return event.priority;
    
    // Default priority order: video (highest) -> annotation -> marker -> category (lowest)
    switch (event.type) {
      case 'video': return 1;
      case 'annotation': return 2;
      case 'marker': return 3;
      case 'category': return 4;
      default: return 10; // Fallback for any new event types
    }
  }, []);

  /**
   * Process pending events based on current timeline position
   */
  const processPendingEvents = useCallback((currentTimeMs: number) => {
    // Nothing to process
    if (pendingEventsRef.current.length === 0) return;
    
    // Find all events that should be executed by now
    const eventsToExecute: TimelineEvent[] = [];
    const remainingEvents: TimelineEvent[] = [];
    
    // Log current timeline position periodically (every second)
    if (Math.floor(currentTimeMs / 1000) !== Math.floor((currentTimeMs - 100) / 1000)) {
      console.log(`Global timeline position: ${(currentTimeMs / 1000).toFixed(1)}s`);
      
      // Also log how many events are still pending
      if (pendingEventsRef.current.length > 0) {
        const nextEvent = pendingEventsRef.current[0];
        console.log(`Next event: ${nextEvent.type} at ${(nextEvent.timeOffset / 1000).toFixed(1)}s (in ${((nextEvent.timeOffset - currentTimeMs) / 1000).toFixed(1)}s)`);
      }
    }
    
    pendingEventsRef.current.forEach(event => {
      if (event.timeOffset <= currentTimeMs) {
        eventsToExecute.push(event);
      } else {
        remainingEvents.push(event);
      }
    });
    
    // Update the pending events
    pendingEventsRef.current = remainingEvents;
    
    // If no events to execute, return early
    if (eventsToExecute.length === 0) return;
    
    // Sort events by timestamp first, then by priority
    eventsToExecute.sort((a, b) => {
      // First sort by timeOffset
      if (a.timeOffset !== b.timeOffset) {
        return a.timeOffset - b.timeOffset;
      }
      
      // If same timeOffset, sort by priority
      return assignEventPriority(a) - assignEventPriority(b);
    });
    
    // Log processing information
    console.log(`Processing ${eventsToExecute.length} events at global timeline ${currentTimeMs}ms`, {
      eventsToExecute: eventsToExecute.map(e => ({
        id: e.id, 
        type: e.type, 
        timeOffset: e.timeOffset,
        priority: assignEventPriority(e),
        action: e.type === 'annotation' ? e.payload.action : 
              (e.type === 'video' ? e.payload.action : 'none'),
        // Add more detailed information about video events
        details: e.type === 'video' ? {
          action: e.payload.action,
          from: e.payload.from,
          to: e.payload.to,
          globalTimeOffset: e.payload.globalTimeOffset
        } : undefined
      })),
      remainingCount: pendingEventsRef.current.length
    });
    
    // Use requestAnimationFrame for smooth event execution
    // This ensures all events are executed within a single frame
    requestAnimationFrame(() => {
      // Execute all events in proper order
      // Use Promise.resolve().then to ensure position updates complete before event processing
      Promise.resolve().then(() => {
        eventsToExecute.forEach(event => {
          // For category events, use requestAnimationFrame to ensure UI updates properly
          if (event.type === 'category') {
            requestAnimationFrame(() => {
              executeEvent(event);
            });
          } else {
            // Execute other events immediately
            executeEvent(event);
          }
        });
      });
    });
  }, [assignEventPriority, executeEvent]);
  
  /**
   * Generate a unique ID
   */
  const generateId = useCallback(() => {
    return Date.now().toString(36) + Math.random().toString(36).substring(2);
  }, []);
  
  /**
   * Initialize a new recording session
   */
  const startRecordingSession = useCallback(async () => {
    if (isActive) return;
    
    try {
      // Start audio recording
      const stream = await navigator.mediaDevices.getUserMedia({
        audio: {
          echoCancellation: true,
          noiseSuppression: true,
          autoGainControl: true,
          channelCount: 1,
          sampleRate: 48000
        }
      });
      
      streamRef.current = stream;
      
      // Determine best audio format
      let mimeType = '';
      const formats = [
        'audio/webm;codecs=opus',
        'audio/webm',
        'audio/mp4;codecs=opus',
        'audio/mp4',
        'audio/ogg;codecs=opus',
        'audio/ogg',
        'audio/wav'
      ];
      
      for (const format of formats) {
        if (MediaRecorder.isTypeSupported(format)) {
          mimeType = format;
          break;
        }
      }
      
      console.log(`Using audio format: ${mimeType || 'default'}`);
      
      // Configure recorder
      const recorder = new MediaRecorder(stream, {
        mimeType: mimeType || undefined,
        audioBitsPerSecond: 128000
      });
      
      audioRecorderRef.current = recorder;
      
      // Set up data handling
      const chunks: Blob[] = [];
      audioChunksRef.current = [];
      eventsRef.current = [];
      
      recorder.ondataavailable = (e) => {
        if (e.data && e.data.size > 0) {
          chunks.push(e.data);
        }
      };
      
      // Handle recording stop
      recorder.onstop = async () => {
        if (chunks.length === 0) return;
        
        // Create the main audio blob
        const audioBlob = new Blob(chunks, { type: mimeType || 'audio/webm' });
        
        // Calculate duration
        const recordingEndTime = Date.now();
        const startTime = recordingStartTimeRef.current || 0;
        const duration = recordingEndTime - startTime;
        
        // Generate a session ID if we don't have one yet
        const sessionId = generateId();
        
        // Try to upload the audio blob to Azure Storage
        let blobUrl: string | undefined;
        try {
          // Import the audio storage utility
          const { uploadAudioToStorage } = await import('../utils/audioStorage');
          
          // Make sure we have a valid blob
          if (audioBlob && audioBlob.size > 0) {
            // Upload the blob to Azure Storage
            blobUrl = await uploadAudioToStorage(audioBlob, sessionId);
            console.log('Audio blob uploaded to Azure Storage:', blobUrl);
          } else {
            console.warn('No valid audio blob to upload to Azure Storage');
          }
        } catch (error) {
          console.error('Failed to upload audio to Azure Storage:', error);
          // Continue with local blob if upload fails
        }
        
        // Create audio chunk (with blobUrl if available)
        const audioChunk: AudioChunk = {
          blob: audioBlob,
          startTime: startTime,
          duration: duration,
          videoTime: 0,
          mimeType: mimeType || 'audio/webm',
          blobUrl: blobUrl
        };
        
        // Create and finalize session
        audioChunksRef.current = [audioChunk];
        
        const audioTrack: AudioTrack = {
          chunks: audioChunksRef.current,
          totalDuration: duration
        };
        
        // Get current session to preserve categories
        const currentSessionData = currentSession || { categories: {} };
        
        // Make sure we have the most up-to-date categories
        console.log('Building final session with categories:', currentSessionData.categories);
        
        // Create a properly-typed categories object
        const categories: Record<string, number> = {};
        
        // Ensure all category values are numbers, not null
        if (currentSessionData.categories) {
          Object.keys(currentSessionData.categories).forEach(key => {
            const value = currentSessionData.categories[key];
            // Only include ratings that have a positive numeric value
            if (typeof value === 'number' && value > 0) {
              categories[key] = value;
            }
          });
        }
        
        console.log('Final clean categories object for saving:', categories);
        
        const session: FeedbackSession = {
          id: sessionId,
          videoId: 'video-' + generateId(),
          startTime: startTime,
          endTime: recordingEndTime,
          audioTrack: audioTrack,
          events: eventsRef.current,
          categories: categories
        };
        
        setCurrentSession(session);
        onAudioRecorded(audioTrack);
        onSessionComplete(session);
        
        // Clean up
        stream.getTracks().forEach(track => track.stop());
        streamRef.current = null;
      };
      
      // Start recording
      const startTime = Date.now();
      recordingStartTimeRef.current = startTime;
      
      console.log(`Starting recording session at ${new Date(startTime).toISOString()}`);
      recorder.start();
      
      // Create the new session object
      const newSession: FeedbackSession = {
        id: generateId(),
        videoId: 'video-' + generateId(),
        startTime: startTime,
        audioTrack: { chunks: [], totalDuration: 0 },
        events: [],
        categories: {} // Initialize empty categories object
      };
      
      setCurrentSession(newSession);
      setIsActive(true);
      
    } catch (error) {
      console.error('Failed to start recording session:', error);
      alert(`Could not start recording: ${error instanceof Error ? error.message : String(error)}`);
    }
  }, [isActive, generateId, onAudioRecorded, onSessionComplete]);
  
  /**
   * End the current recording session
   */
  const endRecordingSession = useCallback(() => {
    if (!isActive) return;
    
    console.log('Ending recording session');
    
    // Stop audio recording
    if (audioRecorderRef.current && audioRecorderRef.current.state !== 'inactive') {
      audioRecorderRef.current.stop();
    }
    
    if (streamRef.current) {
      streamRef.current.getTracks().forEach(track => track.stop());
      streamRef.current = null;
    }
    
    // Reset recording state
    setIsActive(false);
    recordingStartTimeRef.current = null;
    
    // Note: Video reset and annotation clearing are now handled by VideoPlayerWrapper
  }, [isActive]);
  
  /**
   * Record a timeline event during recording
   */
  const recordEvent = useCallback((type: 'video' | 'annotation' | 'marker', payload: any, duration?: number) => {
    if (!isActive || !recordingStartTimeRef.current) return;
    
    const now = Date.now();
    const timeOffset = now - recordingStartTimeRef.current;
    
    // Assign default priority based on event type
    let priority: number;
    switch (type) {
      case 'video': priority = 1; break;
      case 'annotation': priority = 2; break;
      case 'marker': priority = 3; break;
      default: priority = 10; break;
    }
    
    const event: TimelineEvent = {
      id: generateId(),
      type,
      timeOffset,
      duration,
      payload,
      priority
    };
    
    eventsRef.current.push(event);
    console.log(`Recorded ${type} event at ${timeOffset}ms:`, payload);
    
    return event;
  }, [isActive, generateId]);
  
  /**
   * Handle video events (play, pause, seek, etc.)
   */
  const handleVideoEvent = useCallback((action: string, details?: any) => {
    return recordEvent('video', { action, ...details });
  }, [recordEvent]);
  
  /**
   * Handle annotation events (drawing, clearing)
   */
  const handleAnnotationEvent = useCallback((action: string, path?: DrawingPath) => {
    console.log(`Recording annotation event: ${action}`, {
      hasPath: !!path,
      pointsCount: path?.points?.length || 0,
      color: path?.color,
      width: path?.width
    });
    
    // Record the event in the timeline
    const event = recordEvent('annotation', { action, path });
    
    // For debugging during development
    if (event) {
      console.log(`Annotation event recorded with ID: ${event.id}`, {
        timeOffset: event.timeOffset,
        eventCount: eventsRef.current.length
      });
    } else {
      console.warn('Failed to record annotation event - recording may not be active');
    }
    
    return event;
  }, [recordEvent]);
  
  /**
   * Add a marker at the current time
   */
  const addMarker = useCallback((text: string) => {
    return recordEvent('marker', { text });
  }, [recordEvent]);
  
  /**
   * Record a category change - store only in categories object, not as events
   */
  const handleCategoryEvent = useCallback((category: string, rating: number) => {
    console.log(`Recording category change: ${category} = ${rating}`);
    
    // Allow category updates even when not recording
    // This ensures category changes are saved regardless of recording state
    
    // Update the session's categories directly
    setCurrentSession(prevSession => {
      if (!prevSession) {
        // Create initial session if none exists
        return {
          id: generateId(),
          videoId: 'unknown', 
          startTime: recordingStartTimeRef.current || Date.now(),
          events: [],
          audioTrack: { chunks: [], totalDuration: 0 },
          categories: { [category]: rating }
        };
      }
      
      // Get current categories or initialize empty object
      const currentCategories = prevSession.categories || {};
      
      // Create new categories object with the updated rating
      const updatedCategories = {
        ...currentCategories,
        [category]: rating
      };
      
      console.log('Updated categories object:', updatedCategories);
      
      // Update existing session - only update categories, don't add to events
      return {
        ...prevSession,
        categories: updatedCategories
      };
    });
    
    // Log the current session after update (for debugging)
    setTimeout(() => {
      console.log('Current session after category update:', currentSession);
    }, 50);
    
    console.log(`Updated session categories with ${category}: ${rating}`);
  }, [generateId, currentSession]);
  
  /**
   * Complete the replay process
   */
  const completeReplay = useCallback(() => {
    // Reset global timeline position and last clear time when replay completes
    resetTimelinePosition();
    console.log('Timeline position and lastClearTime reset to 0ms via context at completion');
    
    // Clean up audio player
    if (audioPlayer) {
      audioPlayer.pause();
      
      // Only revoke object URL if it's a local blob (not an Azure Storage URL)
      const isLocalBlob = audioPlayer.dataset.isLocalBlob === 'true';
      if (isLocalBlob && audioPlayer.src.startsWith('blob:')) {
        console.log('Revoking local blob URL:', audioPlayer.src);
        URL.revokeObjectURL(audioPlayer.src);
      }
      
      setAudioPlayer(null);
    }
    
    // Clear any timeouts
    replayTimeoutIdsRef.current.forEach(id => window.clearTimeout(id));
    replayTimeoutIdsRef.current = [];
    
    // Reset video position to the beginning
    if (videoElementRef.current) {
      console.log('Replay complete: resetting video position to start');
      videoElementRef.current.currentTime = 0;
      
      // If it's playing, pause it
      if (!videoElementRef.current.paused) {
        videoElementRef.current.pause();
      }
    }
    
    // Clear all annotations when replay is done using the new state-based reset
    try {
      console.log('Replay complete: performing state-based canvas reset');
      // Try the new resetCanvas method first
      if (canvasRef.current && canvasRef.current.resetCanvas) {
        canvasRef.current.resetCanvas();
      } else {
        // Fall back to the original clear method if resetCanvas isn't available
        console.log('Replay complete: falling back to standard clear annotations');
        clearAnnotations();
      }
    } catch (error) {
      console.error('Error clearing annotations on replay completion:', error);
    }
    
    // Ensure any pending events are cleared
    pendingEventsRef.current = [];
    
    // Update state to indicate replay is complete but maintain the 100% progress
    setIsActive(false);
    setReplayProgress(100);
    
    // Use setTimeout to reset progress to 0 after a brief delay
    // This gives users visual feedback that replay completed successfully
    setTimeout(() => {
      setReplayProgress(0);
      console.log('Replay progress reset to 0');
    }, 1500);
    
  }, [audioPlayer, videoElementRef, clearAnnotations, resetTimelinePosition]);
  
  /**
   * Helper function to simulate timeline without audio
   */
  const simulateTimelineWithoutAudio = useCallback((session: FeedbackSession) => {
    console.log('Using simulated timeline instead of audio playback');
    
    // Calculate the total duration from events or use a default
    const totalDuration = session.events.length > 0 
      ? Math.max(...session.events.map(e => e.timeOffset)) + 5000
      : 30000; // Default 30s if no events
    
    console.log(`Simulating timeline for ${totalDuration}ms`);
    
    let elapsed = 0;
    const interval = 100; // 100ms updates
    
    const timelineInterval = window.setInterval(() => {
      elapsed += interval;
      setReplayProgress((elapsed / totalDuration) * 100);
      
      // Use requestAnimationFrame for smoother updates
      requestAnimationFrame(() => {
        // Update global timeline position via context
        updatePosition(elapsed);
        
        // Log global time position every second to avoid flooding logs
        if (Math.floor(elapsed / 1000) !== Math.floor((elapsed - interval) / 1000)) {
          console.log(`Timeline position updated via context (simulated): ${elapsed}ms`);
        }
        
        // Use Promise.resolve().then to ensure position updates complete before event processing
        Promise.resolve().then(() => {
          processPendingEvents(elapsed);
        });
        
        if (elapsed >= totalDuration) {
          clearInterval(timelineInterval);
          console.log('Simulated timeline complete, cleaning up and resetting...');
          completeReplay();
          // Ensure the active state is updated
          setIsActive(false);
        }
      });
    }, interval);
    
    // Store the interval ID for cleanup
    replayTimeoutIdsRef.current.push(timelineInterval as unknown as number);
  }, [updatePosition, processPendingEvents, completeReplay]);
  
  /**
   * Start replay of a feedback session
   */
  const startReplay = useCallback(() => {
    if (!currentSession || isActive) return;
    
    // Reset global timeline position and last clear time at the start of replay
    resetTimelinePosition();
    console.log('Timeline position and lastClearTime reset to 0ms via context');
    
    setIsActive(true);
    setReplayProgress(0);
    
    // Clear any previous timeouts
    replayTimeoutIdsRef.current.forEach(id => window.clearTimeout(id));
    replayTimeoutIdsRef.current = [];
    
    // Clear any pending events
    pendingEventsRef.current = [];
    
    // Create a copy of events to process and sort them by timestamp first, then by type priority
    pendingEventsRef.current = [...currentSession.events].sort((a, b) => {
      // First sort by timeOffset
      if (a.timeOffset !== b.timeOffset) {
        return a.timeOffset - b.timeOffset;
      }
      
      // If same timeOffset, sort by priority (add priorities if they don't exist)
      const priorityA = a.priority ?? (a.type === 'video' ? 1 : 
                        a.type === 'annotation' ? 2 : 
                        a.type === 'marker' ? 3 : 
                        a.type === 'category' ? 4 : 10);
                        
      const priorityB = b.priority ?? (b.type === 'video' ? 1 : 
                        b.type === 'annotation' ? 2 : 
                        b.type === 'marker' ? 3 : 
                        b.type === 'category' ? 4 : 10);
      
      return priorityA - priorityB;
    });
    
    // Create audio player for the main timeline
    if (currentSession.audioTrack.chunks.length > 0) {
      const mainAudioChunk = currentSession.audioTrack.chunks[0];
      
      try {
        let audioUrl: string;
        let isLocalBlob = false;
        
        // First check if we have a blob URL from Azure Storage
        if (mainAudioChunk.blobUrl) {
          console.log('Using Azure Storage blob URL for audio playback:', mainAudioChunk.blobUrl);
          // Use the direct URL from Azure Storage
          audioUrl = mainAudioChunk.blobUrl;
        }
        // Fall back to local blob or data URL if no Azure URL is available
        else if (mainAudioChunk.blob instanceof Blob) {
          console.log('Using local Blob object for audio playback');
          audioUrl = URL.createObjectURL(mainAudioChunk.blob);
          isLocalBlob = true;
        } else if (typeof mainAudioChunk.blob === 'string' && mainAudioChunk.blob.startsWith('data:')) {
          // Handle data URL
          console.log('Converting data URL to Blob for audio playback');
          const parts = mainAudioChunk.blob.split(',');
          if (parts.length !== 2) {
            throw new Error('Invalid data URL format');
          }
          
          const mimeMatch = parts[0].match(/:(.*?);/);
          const mime = mimeMatch ? mimeMatch[1] : mainAudioChunk.mimeType || 'audio/webm';
          
          const binary = atob(parts[1]);
          const arrayBuffer = new ArrayBuffer(binary.length);
          const uint8Array = new Uint8Array(arrayBuffer);
          
          for (let i = 0; i < binary.length; i++) {
            uint8Array[i] = binary.charCodeAt(i);
          }
          
          const blob = new Blob([uint8Array], { type: mime });
          audioUrl = URL.createObjectURL(blob);
          isLocalBlob = true;
        } else if (!mainAudioChunk.blob) {
          console.error('No valid audio blob or blob URL in the session');
          return;
        } else {
          throw new Error('Unsupported audio format');
        }
        
        console.log(`Creating audio player with URL: ${audioUrl.substring(0, 50)}...`);
        
        const audio = new Audio();
        
        // Add event listeners before setting src to catch any loading errors
        audio.preload = 'auto';
        
        // Add a load listener to detect when audio is ready
        audio.onloadeddata = () => {
          console.log('Audio data loaded successfully:', {
            duration: audio.duration,
            readyState: audio.readyState
          });
        };
        
        // Set up audio events
        audio.onplay = () => {
          console.log('Audio playback started');
        };
        
        // Set src after adding listeners
        audio.src = audioUrl;
        
        audio.ontimeupdate = () => {
          const currentTime = audio.currentTime * 1000; // Convert to ms
          const totalDuration = currentSession.audioTrack.totalDuration;
          setReplayProgress((currentTime / totalDuration) * 100);
          
          // Use requestAnimationFrame for smoother updates
          requestAnimationFrame(() => {
            // Update global timeline position via context
            updatePosition(currentTime);
            
            // Log global time position every 250ms (to avoid flooding logs)
            if (Math.floor(currentTime / 250) !== Math.floor((currentTime - 16) / 250)) {
              console.log(`Timeline position updated via context: ${currentTime}ms`);
            }
            
            // Use Promise.resolve().then to ensure position updates complete before event processing
            Promise.resolve().then(() => {
              // Process any pending events that should occur by this time
              processPendingEvents(currentTime);
            });
          });
        };
        
        audio.onended = () => {
          console.log('Audio playback complete, cleaning up and resetting UI...');
          completeReplay();
          // Ensure the active state in parent component is also updated
          setIsActive(false);
        };
        
        audio.onerror = (e) => {
          const errorInfo = {
            code: audio.error ? audio.error.code : 'unknown',
            message: audio.error ? audio.error.message : 'No error details available',
            src: audioUrl.substring(0, 100) + '...',
            audioType: isLocalBlob ? 'local-blob' : (audioUrl.startsWith('/api') ? 'proxy-url' : 'azure-url'),
            readyState: audio.readyState,
            networkState: audio.networkState,
            error: audio.error
          };
          console.error('Audio playback error:', errorInfo);
          
          // Try to recover by stopping and restarting
          try {
            audio.pause();
            setTimeout(() => {
              console.log('Attempting to restart audio playback after error...');
              audio.load();
              audio.play().catch(err => {
                console.error('Failed to restart audio playback:', err);
                completeReplay(); // Stop the replay if we can't recover
              });
            }, 1000);
          } catch (recoverError) {
            console.error('Error attempting to recover from audio playback error:', recoverError);
            completeReplay(); // Stop the replay if we can't recover
          }
        };
        
        // Store isLocalBlob status with the audio player for cleanup
        audio.dataset.isLocalBlob = isLocalBlob.toString();
        
        setAudioPlayer(audio);
        
        // Wait a moment to ensure the audio is loaded before playing
        setTimeout(() => {
          console.log('Attempting to start audio playback...');
          
          // First check if audio is ready
          if (audio.readyState >= 2) { // HAVE_CURRENT_DATA or better
            console.log('Audio is ready to play, starting playback');
            audio.play().catch(error => {
              console.error('Failed to start audio playback despite ready state:', error);
              
              // Show a user-friendly message for autoplay policy errors
              if (error.name === 'NotAllowedError') {
                alert('Audio playback requires user interaction. Please click anywhere on the page and try again.');
              } else {
                // Fall back to simulated playback if audio fails completely
                console.log('Switching to simulated timeline mode due to audio error');
                simulateTimelineWithoutAudio(currentSession);
              }
            });
          } else {
            console.warn(`Audio not ready yet, readyState: ${audio.readyState}. Retrying in 1 second...`);
            
            // Retry after a delay
            setTimeout(() => {
              console.log('Retrying audio playback...');
              audio.play().catch(error => {
                console.error('Failed to start audio playback on retry:', error);
                
                // Fall back to simulated playback if audio fails completely
                console.log('Switching to simulated timeline mode due to audio error');
                simulateTimelineWithoutAudio(currentSession);
              });
            }, 1000);
          }
        }, 500);
      } catch (error) {
        console.error('Error creating audio player for replay:', error);
      }
    } else {
      console.warn('No audio track found for replay. Using simulated timeline.');
      
      // Use the dedicated helper function for simulated timeline
      simulateTimelineWithoutAudio(currentSession);
    }
  }, [currentSession, isActive, resetTimelinePosition, updatePosition, processPendingEvents, completeReplay, simulateTimelineWithoutAudio]);
  
  /**
   * Stop the current replay
   */
  const stopReplay = useCallback(() => {
    if (!isActive) return;
    
    console.log('Stopping replay session');
    
    // Reset global timeline position and last clear time when stopping replay
    resetTimelinePosition();
    console.log('Timeline position and lastClearTime reset to 0ms via context');
    
    // Clean up audio player
    if (audioPlayer) {
      audioPlayer.pause();
      
      // Only revoke object URL if it's a local blob (not an Azure Storage URL)
      const isLocalBlob = audioPlayer.dataset.isLocalBlob === 'true';
      if (isLocalBlob && audioPlayer.src.startsWith('blob:')) {
        console.log('Revoking local blob URL:', audioPlayer.src);
        URL.revokeObjectURL(audioPlayer.src);
      }
      
      setAudioPlayer(null);
    }
    
    // Clear any timeouts
    replayTimeoutIdsRef.current.forEach(id => window.clearTimeout(id));
    replayTimeoutIdsRef.current = [];
    
    // Reset video position to the beginning
    if (videoElementRef.current) {
      console.log('Replay stopped: resetting video position to start');
      videoElementRef.current.currentTime = 0;
      
      // If it's playing, pause it
      if (!videoElementRef.current.paused) {
        videoElementRef.current.pause();
      }
    }
    
    // Clear all annotations when replay is stopped using the new state-based reset
    try {
      console.log('Replay stopped: performing state-based canvas reset');
      // Try the new resetCanvas method first
      if (canvasRef.current && canvasRef.current.resetCanvas) {
        canvasRef.current.resetCanvas();
      } else {
        // Fall back to the original clear method if resetCanvas isn't available
        console.log('Replay stopped: falling back to standard clear annotations');
        clearAnnotations();
      }
    } catch (error) {
      console.error('Error clearing annotations when stopping replay:', error);
    }
    
    // Ensure any pending events are cleared
    pendingEventsRef.current = [];
    
    // Reset replay state
    setIsActive(false);
    setReplayProgress(0);
  }, [isActive, audioPlayer, resetTimelinePosition, videoElementRef, clearAnnotations]);
  
  /**
   * Load a session for replay
   */
  const loadSession = useCallback(async (session: FeedbackSession) => {
    console.log('Loading session for replay, session ID:', session.id);
    console.log('Total events in session:', session.events.length);
    console.log('All event types:', session.events.map(e => e.type));
    
    // If we have audio chunks with blobUrl from Azure Storage, preload them and convert to proxy URLs
    if (session.audioTrack && session.audioTrack.chunks && session.audioTrack.chunks.length > 0) {
      // Set flag that session is loading
      if (typeof window !== 'undefined') {
        window.__sessionReady = false;
      }
      
      for (let i = 0; i < session.audioTrack.chunks.length; i++) {
        const chunk = session.audioTrack.chunks[i];
        if (chunk.blobUrl) {
          try {
            console.log('Processing Azure Storage blob URL:', chunk.blobUrl);
            
            // Convert Azure Storage URL to our proxy URL to avoid CORS issues
            try {
              const url = new URL(chunk.blobUrl);
              const blobPath = url.pathname.split('/').slice(2).join('/'); // Extract path after container name
              
              // Create a proxy URL that goes through our Next.js API
              const proxyUrl = `/api/audio/${blobPath}`;
              console.log(`Converted Azure URL to proxy URL: ${proxyUrl}`);
              
              // Update the chunk's URL to use our proxy instead
              session.audioTrack.chunks[i] = {
                ...chunk,
                blobUrl: proxyUrl
              };
            } catch (urlError) {
              console.warn('Failed to convert Azure URL to proxy URL:', urlError);
              // Keep original URL if conversion fails
            }
            
            if (typeof window !== 'undefined') {
              console.log('Using proxy URL to avoid CORS issues:', session.audioTrack.chunks[i].blobUrl);
            }
          } catch (error) {
            console.error('Error processing audio blob URL:', error);
            // Continue with next chunk
          }
        }
      }
      
      // Mark session as ready after processing all chunks
      if (typeof window !== 'undefined') {
        window.__sessionReady = true;
        window.dispatchEvent(new Event('session-ready'));
      }
    }
    
    setCurrentSession(session);
    
    // Detailed log of all events to debug
    session.events.forEach((event, index) => {
      console.log(`Event ${index}: type=${event.type}, timeOffset=${event.timeOffset}, payload=`, event.payload);
    });
    
    // Combine both approaches for most reliable category data
    const categoriesState: Record<string, number> = {};
    
    // First collect categories from direct property if available (primary source)
    if (session.categories && Object.keys(session.categories).length > 0) {
      console.log('Session has categories property directly:', session.categories);
      
      // Add all categories from session.categories
      Object.entries(session.categories).forEach(([category, rating]) => {
        if (typeof rating === 'number' && rating > 0) {
          categoriesState[category] = rating;
        }
      });
      
      console.log('Added categories from direct property:', Object.keys(categoriesState));
    }
    
    // Then also check category events as a fallback or supplementary source
    const allCategoryEvents = session.events.filter(event => event.type === 'category');
    console.log(`Found ${allCategoryEvents.length} total category events in session`);
    
    // Add or update categories from events
    allCategoryEvents.forEach(event => {
      if (event.payload?.category && typeof event.payload.rating === 'number' && event.payload.rating > 0) {
        // Only add if not already present (prefer session.categories) or if it's zero in categories
        if (!categoriesState[event.payload.category] || categoriesState[event.payload.category] === 0) {
          categoriesState[event.payload.category] = event.payload.rating;
          console.log(`Added category from event: ${event.payload.category} = ${event.payload.rating}`);
        }
      }
    });
    
    // Find which categories have ratings in the final state
    const ratedCategories = Object.entries(categoriesState)
      .filter(([_, rating]) => rating > 0)
      .map(([category]) => category);
    
    console.log(`Final state has ${ratedCategories.length} rated categories:`, ratedCategories);
    
    // Even if we have no ratings, send the empty state to ensure UI is updated
    if (onCategoriesLoaded) {
      console.log('Final categories state to send:', categoriesState);
      
      // Update session's categories property for future reference
      if (Object.keys(categoriesState).length > 0) {
        session.categories = { ...categoriesState };
        setCurrentSession({ ...session });
      }
      
      // Notify parent right away
      onCategoriesLoaded(categoriesState);
      
      // Also schedule a delayed notification to ensure component has mounted
      setTimeout(() => {
        if (onCategoriesLoaded) {
          console.log('Delayed call to onCategoriesLoaded with:', categoriesState);
          onCategoriesLoaded(categoriesState);
        }
      }, 100);
    } else {
      console.warn('No callback available for notifying categories', {
        categoriesState: Object.keys(categoriesState).length
      });
    }
  }, [onCategoriesLoaded]);
  
  /**
   * Clean up resources when component unmounts
   */
  useEffect(() => {
    return () => {
      // Stop recording if active
      if (audioRecorderRef.current && audioRecorderRef.current.state !== 'inactive') {
        audioRecorderRef.current.stop();
      }
      
      if (streamRef.current) {
        streamRef.current.getTracks().forEach(track => track.stop());
      }
      
      // Clean up audio player
      if (audioPlayer) {
        audioPlayer.pause();
        
        // Only revoke object URL if it's a local blob (not an Azure Storage URL)
        const isLocalBlob = audioPlayer.dataset.isLocalBlob === 'true';
        if (isLocalBlob && audioPlayer.src.startsWith('blob:')) {
          console.log('Revoking local blob URL:', audioPlayer.src);
          URL.revokeObjectURL(audioPlayer.src);
        }
      }
      
      // Clear any timeouts
      replayTimeoutIdsRef.current.forEach(id => window.clearTimeout(id));
    };
  }, [audioPlayer]);
  
  // Expose imperative methods to parent component using the ref
  useImperativeHandle(ref, () => ({
    // Status
    isActive,
    currentSession,
    replayProgress,
    
    // Recording methods
    startRecordingSession,
    endRecordingSession,
    handleVideoEvent,
    handleAnnotationEvent,
    addMarker,
    handleCategoryEvent,
    
    // Replay methods
    startReplay,
    stopReplay,
    completeReplay,
    loadSession,
  }));
  
  // Return null as this is a controller component without UI
  return null;
});

FeedbackOrchestrator.displayName = 'FeedbackOrchestrator';

export default FeedbackOrchestrator;
|| END ||


|| START ./src/lib/auth.ts ||

import { hash, compare } from "bcrypt";
import { CosmosClient } from "@azure/cosmos";

// Validate required environment variables
if (!process.env.COSMOS_ENDPOINT) {
  throw new Error('COSMOS_ENDPOINT environment variable is required');
}

if (!process.env.COSMOS_KEY) {
  throw new Error('COSMOS_KEY environment variable is required');
}

if (!process.env.COSMOS_DATABASE_ID) {
  throw new Error('COSMOS_DATABASE_ID environment variable is required');
}

if (!process.env.COSMOS_USERS_CONTAINER_ID) {
  throw new Error('COSMOS_USERS_CONTAINER_ID environment variable is required');
}

// Connect to your Cosmos DB
const client = new CosmosClient({
  endpoint: process.env.COSMOS_ENDPOINT,
  key: process.env.COSMOS_KEY,
});

const database = client.database(process.env.COSMOS_DATABASE_ID);
const container = database.container(process.env.COSMOS_USERS_CONTAINER_ID);

export async function findUserByEmail(email: string) {
  const querySpec = {
    query: "SELECT * FROM c WHERE c.email = @email",
    parameters: [
      {
        name: "@email",
        value: email
      }
    ]
  };
  
  const { resources } = await container.items.query(querySpec).fetchAll();
  return resources[0] || null;
}

export async function createUser(email: string, name: string, password: string) {
  const hashedPassword = await hash(password, 12);
  
  const newUser = {
    id: Date.now().toString(),
    email,
    name,
    hashedPassword,
    createdAt: new Date().toISOString()
  };
  
  const { resource } = await container.items.create(newUser);
  return resource;
}

export async function validatePassword(password: string, hashedPassword: string) {
  return compare(password, hashedPassword);
}

|| END ||


